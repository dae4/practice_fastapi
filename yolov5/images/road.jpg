<!doctype html><html lang="en"><head><script defer src="https://cdn.optimizely.com/js/16180790160.js"></script><title data-rh="true">Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3 | by Jonathan Hui | Medium</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="article:published_time" content="2019-08-27T14:09:10.959Z"/><meta data-rh="true" name="title" content="Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3 | by Jonathan Hui | Medium"/><meta data-rh="true" property="og:title" content="Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3"/><meta data-rh="true" property="twitter:title" content="Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3"/><meta data-rh="true" name="twitter:site" content="@Medium"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/28b1b93e2088"/><meta data-rh="true" property="al:android:url" content="medium://p/28b1b93e2088"/><meta data-rh="true" property="al:ios:url" content="medium://p/28b1b93e2088"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="You only look once (YOLO) is an object detection system targeted for real-time processing. We will introduce YOLO, YOLOv2 and YOLO9000 in this article. For those only interested in YOLOv3, please…"/><meta data-rh="true" property="og:description" content="You only look once (YOLO) is an object detection system targeted for real-time processing. We will introduce YOLO, YOLOv2 and YOLO9000 in…"/><meta data-rh="true" property="twitter:description" content="You only look once (YOLO) is an object detection system targeted for real-time processing. We will introduce YOLO, YOLOv2 and YOLO9000 in…"/><meta data-rh="true" property="og:url" content="https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088"/><meta data-rh="true" property="al:web:url" content="https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*j4PnWfxP3yoVPOFyI27tww.jpeg"/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*j4PnWfxP3yoVPOFyI27tww.jpeg"/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" property="article:author" content="https://jonathan-hui.medium.com"/><meta data-rh="true" name="twitter:creator" content="@jonathan_hui"/><meta data-rh="true" name="author" content="Jonathan Hui"/><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"/><meta data-rh="true" name="referrer" content="unsafe-url"/><meta data-rh="true" name="twitter:label1" content="Reading time"/><meta data-rh="true" name="twitter:data1" content="18 min read"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/fit/c/152/152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/fit/c/120/120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/fit/c/76/76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/fit/c/60/60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"/><link data-rh="true" rel="preconnect" href="https://glyph.medium.com" crossOrigin=""/><link data-rh="true" rel="preconnect" href="https://logx.optimizely.com"/><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" rel="author" href="https://jonathan-hui.medium.com"/><link data-rh="true" rel="canonical" href="https://jonathan-hui.medium.com/real-time-object-detection-with-yolo-yolov2-28b1b93e2088"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/28b1b93e2088"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*j4PnWfxP3yoVPOFyI27tww.jpeg"],"url":"https:\u002F\u002Fjonathan-hui.medium.com\u002Freal-time-object-detection-with-yolo-yolov2-28b1b93e2088","dateCreated":"2018-03-18T02:08:01.687Z","datePublished":"2018-03-18T02:08:01.687Z","dateModified":"2019-08-27T14:09:11.256Z","headline":"Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3","name":"Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3","description":"You only look once (YOLO) is an object detection system targeted for real-time processing. We will introduce YOLO, YOLOv2 and YOLO9000 in this article. For those only interested in YOLOv3, please…","identifier":"28b1b93e2088","author":{"@type":"Person","name":"Jonathan Hui","url":"https:\u002F\u002Fjonathan-hui.medium.com"},"creator":["Jonathan Hui"],"publisher":{"@type":"Organization","name":"Medium","url":"https:\u002F\u002Fjonathan-hui.medium.com\u002F","logo":{"@type":"ImageObject","width":308,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F616\u002F1*OMF3fSqH8t4xBJ9-6oZDZw.png"}},"mainEntityOfPage":"https:\u002F\u002Fjonathan-hui.medium.com\u002Freal-time-object-detection-with-yolo-yolov2-28b1b93e2088"}</script><link rel="preload" href="https://cdn.optimizely.com/js/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="588" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="KEYFRAME">@-webkit-keyframes k1{from{filter:hue-rotate(0deg)}to{filter:hue-rotate(360deg)}}@-moz-keyframes k1{from{filter:hue-rotate(0deg)}to{filter:hue-rotate(360deg)}}@keyframes k1{from{filter:hue-rotate(0deg)}to{filter:hue-rotate(360deg)}}@-webkit-keyframes k2{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-moz-keyframes k2{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@keyframes k2{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-webkit-keyframes k3{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-moz-keyframes k3{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@keyframes k3{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{height:25px}.r{fill:rgba(41, 41, 41, 1)}.s{display:block}.t{margin-bottom:36px}.v{padding-top:8px}.w{width:100%}.ab{flex:0 0 auto}.ac{justify-self:flex-end}.ae{z-index:500}.af{visibility:hidden}.ag{box-shadow:inset 0 -1px 0 rgba(230, 230, 230, 1)}.ah{min-height:115px}.ak{flex-direction:column}.al{display:none}.an{white-space:nowrap}.ao{border-bottom:1px solid rgba(230, 230, 230, 1)}.ap{position:relative}.av{max-width:1192px}.aw{min-width:0}.ax{height:62px}.ay{flex-direction:row}.az{flex:1 0 auto}.ba{margin-right:16px}.bb{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bc{font-size:14px}.bd{line-height:20px}.be{color:rgba(26, 137, 23, 1)}.bf{padding:7px 16px 9px}.bg{background:0}.bh{fill:rgba(26, 137, 23, 1)}.bi{border-color:rgba(26, 137, 23, 1)}.bn:disabled{cursor:inherit}.bo:disabled{opacity:0.3}.bp:disabled:hover{color:rgba(26, 137, 23, 1)}.bq:disabled:hover{fill:rgba(26, 137, 23, 1)}.br:disabled:hover{border-color:rgba(26, 137, 23, 1)}.bs{border-radius:99em}.bt{border-width:1px}.bu{border-style:solid}.bv{box-sizing:border-box}.bw{display:inline-block}.bx{text-decoration:none}.by{margin-left:0px}.bz{color:rgba(117, 117, 117, 1)}.ca{font-size:inherit}.cb{border:inherit}.cc{font-family:inherit}.cd{letter-spacing:inherit}.ce{font-weight:inherit}.cf{padding:0}.cg{margin:0}.ch:disabled{cursor:default}.ci:disabled{color:rgba(163, 208, 162, 0.5)}.cj:disabled{fill:rgba(163, 208, 162, 0.5)}.ck{justify-content:space-between}.cq{align-items:flex-start}.cr{margin-bottom:0px}.cs{margin-top:-32px}.ct{align-items:flex-end}.cu{flex-wrap:wrap}.cx{margin-top:32px}.cy{margin-right:24px}.da{font-weight:500}.db{font-size:27px}.dc{line-height:34px}.dd:before{margin-bottom:-14px}.de:before{content:""}.df:before{display:table}.dg:before{border-collapse:collapse}.dh:after{margin-top:-6px}.di:after{content:""}.dj:after{display:table}.dk:after{border-collapse:collapse}.dl{letter-spacing:0}.dm{color:rgba(25, 25, 25, 1)}.dn{word-break:break-word}.do{margin-bottom:-3px}.dp{margin-left:14px}.dq{margin-top:-3px}.dr{padding-top:1px}.ds{height:70px}.dt{font-size:16px}.du{line-height:24px}.dv:before{margin-bottom:-10px}.dw{margin-right:12px}.dx{display:inline-flex}.dy{color:inherit}.dz{fill:inherit}.ec:disabled{color:rgba(117, 117, 117, 1)}.ed:disabled{fill:rgba(117, 117, 117, 1)}.ee{margin-left:12px}.ef{position:absolute}.eg{right:24px}.eh{margin:0px}.ei{border:0px}.ej{padding:0px}.ek{cursor:pointer}.el{stroke:rgba(117, 117, 117, 1)}.eo{left:0}.ep{opacity:0}.eq{position:fixed}.er{right:0}.es{top:0}.eu{height:60px}.ex{height:100%}.fa{margin-left:16px}.fb{padding-left:24px}.fc{padding-right:24px}.fd{margin-left:auto}.fe{margin-right:auto}.ff{max-width:728px}.fg{background:rgba(255, 255, 255, 1)}.fh{border:1px solid rgba(230, 230, 230, 1)}.fi{border-radius:4px}.fj{box-shadow:0 1px 4px rgba(230, 230, 230, 1)}.fk{max-height:100vh}.fl{overflow-y:auto}.fm{top:calc(100vh + 100px)}.fn{bottom:calc(100vh + 100px)}.fo{width:10px}.fp{pointer-events:none}.fq{word-wrap:break-word}.fr:after{display:block}.fs:after{clear:both}.ft{max-width:680px}.fu{line-height:1.23}.fv{font-style:normal}.fw{font-weight:700}.gr{margin-bottom:-0.27em}.gs{color:rgba(41, 41, 41, 1)}.gw{border-radius:50%}.gx{height:28px}.gy{width:28px}.gz{margin-left:8px}.ha{margin:0 4px}.hb{margin:0 7px}.hk{margin:0 6px 0 7px}.hl{line-height:1.58}.hm{letter-spacing:-0.004em}.hn{font-family:charter, Georgia, Cambria, "Times New Roman", Times, serif}.id{margin-top:24px}.ie{margin-bottom:-0.46em}.if{text-decoration:underline}.ig{max-width:1091px}.im{clear:both}.io{cursor:zoom-in}.ip{z-index:auto}.ir{max-width:100%}.is{height:auto}.it{margin-top:10px}.iu{text-align:center}.jc{margin:auto}.jd{padding-bottom:56.206088992974244%}.je{height:0}.jf{max-width:872px}.jg{transition:opacity 100ms 400ms}.jh{overflow:hidden}.ji{will-change:transform}.jj{transform:translateZ(0)}.jk{background-color:rgba(242, 242, 242, 1)}.jl{padding-bottom:67.28571428571429%}.jm{filter:blur(20px)}.jn{transform:scale(1.1)}.jo{visibility:visible}.jp{font-weight:600}.jq{max-width:1139px}.jr{padding-bottom:58.85714285714286%}.js{max-width:667px}.jt{padding-bottom:100.29985007496252%}.ju{max-width:3036px}.jv{padding-bottom:61.14285714285714%}.jw{list-style-type:disc}.jx{margin-left:30px}.jy{padding-left:0px}.ke{max-width:2113px}.kf{padding-bottom:64.14285714285714%}.kg{font-style:italic}.kh{max-width:804px}.ki{padding-bottom:29.714285714285715%}.kj{max-width:1715px}.kk{padding-bottom:27.428571428571427%}.kl{max-width:1528px}.km{padding-bottom:6%}.kn{max-width:1624px}.ko{padding-bottom:34.85714285714286%}.kp{line-height:1.12}.kq{letter-spacing:-0.022em}.lj{margin-bottom:-0.28em}.ls{max-width:1920px}.lt{padding-bottom:5px}.lu{padding-top:5px}.ma{max-width:1454px}.mb{padding-bottom:29.857142857142854%}.mc{max-width:1598px}.md{padding-bottom:38.285714285714285%}.me{max-width:1658px}.mf{padding-bottom:28%}.mg{max-width:1526px}.mh{padding-bottom:36.142857142857146%}.mi{max-width:1372px}.mj{padding-bottom:46.71428571428571%}.mk{list-style-type:decimal}.ml{max-width:2140px}.mm{padding-bottom:35.714285714285715%}.mn{max-width:691px}.mo{padding-bottom:72.06946454413894%}.mp{max-width:2688px}.mq{padding-bottom:53.14285714285714%}.mr{max-width:504px}.ms{padding-bottom:45.03968253968254%}.mt{box-shadow:inset 3px 0 0 0 rgba(41, 41, 41, 1)}.mu{padding-left:23px}.mv{margin-left:-20px}.mw{max-width:1904px}.mx{padding-bottom:42.42857142857143%}.my{max-width:1047px}.mz{padding-bottom:45.57142857142858%}.na{max-width:1579px}.nb{padding-bottom:59.857142857142854%}.nc{max-width:822px}.nd{padding-bottom:46.14285714285714%}.ne{max-width:388px}.nf{padding-bottom:76.80412371134021%}.ng{max-width:1114px}.nh{padding-bottom:53%}.ni{max-width:1536px}.nj{padding-bottom:66.42857142857143%}.nk{max-width:1214px}.nl{padding-bottom:75.71428571428572%}.nm{max-width:1600px}.nn{padding-bottom:21.571428571428573%}.no{max-width:1568px}.np{padding-bottom:41.42857142857143%}.nq{max-width:462px}.nr{padding-bottom:70.34632034632034%}.ns{max-width:464px}.nt{padding-bottom:128.8793103448276%}.nu{max-width:1794px}.nv{max-width:1440px}.nw{padding-bottom:103%}.nx{max-width:1562px}.ny{padding-bottom:3.8571428571428568%}.nz{max-width:1554px}.oa{padding-bottom:5.857142857142857%}.ob{max-width:1576px}.oc{padding-bottom:4.571428571428571%}.od{max-width:386px}.oe{padding-bottom:85.23316062176166%}.of{max-width:1578px}.og{padding-bottom:15.142857142857142%}.oh{max-width:1040px}.oi{padding-bottom:17.42857142857143%}.oj{max-width:334px}.ok{padding-bottom:138.3233532934132%}.ol{max-width:1328px}.om{will-change:opacity}.on{width:188px}.oo{left:50%}.op{transform:translateX(406px)}.oq{top:calc(65px + 54px + 14px)}.ot{will-change:opacity, transform}.ou{transform:translateY(159px)}.ow{width:197px}.ox{margin-bottom:20px}.oy{padding-top:2px}.oz{padding-top:20px}.pa{color:rgba(255, 255, 255, 1)}.pb{fill:rgba(255, 255, 255, 1)}.pc{background:rgba(26, 137, 23, 1)}.pe:disabled:hover{background:rgba(26, 137, 23, 1)}.pf{stroke:rgba(242, 242, 242, 1)}.pg{height:36px}.ph{width:36px}.pi{color:rgba(242, 242, 242, 1)}.pj{fill:rgba(242, 242, 242, 1)}.pk{background:rgba(242, 242, 242, 1)}.pl{border-color:rgba(242, 242, 242, 1)}.pr{padding-top:32px}.ps{border-top:1px solid rgba(230, 230, 230, 1)}.pt{justify-content:space-evenly}.pu{margin-right:20px}.qa{outline:0}.qb{border:0}.qc{user-select:none}.qd> svg{pointer-events:none}.qf{-webkit-user-select:none}.qp button{text-align:left}.qq{margin-top:2px}.qr{fill:rgba(61, 61, 61, 1)}.qs{opacity:1}.qt{padding-left:6px}.qu{margin-top:1px}.qv{margin-top:40px}.qw{padding-bottom:25px}.qx{margin-top:25px}.qy{max-width:155px}.rf{top:1px}.ri{margin-left:24px}.rj{margin-top:4px}.rk{margin-left:4px}.rl{margin-top:5px}.rm{padding-bottom:40px}.rn{list-style-type:none}.ro{margin-right:8px}.rp{margin-bottom:8px}.rq{font-size:13px}.rr{line-height:22px}.rs{border-radius:3px}.rt{padding:5px 10px}.ru{padding-top:40px}.rv{padding-bottom:4px}.rw{background-color:rgba(250, 250, 250, 1)}.sm{text-overflow:ellipsis}.sn{display:-webkit-box}.so{-webkit-line-clamp:2}.sp{-webkit-box-orient:vertical}.sr{padding-top:25px}.su{margin-bottom:40px}.sv{padding-bottom:16px}.sw{margin-bottom:24px}.uq{flex-grow:0}.ur{padding-bottom:24px}.us{max-width:500px}.ut{flex:0 1 auto}.uv{padding-bottom:8px}.vg{padding-bottom:100%}.bj:hover{color:rgba(15, 115, 12, 1)}.bk:hover{fill:rgba(15, 115, 12, 1)}.bl:hover{border-color:rgba(15, 115, 12, 1)}.bm:hover{cursor:pointer}.ea:hover{color:rgba(25, 25, 25, 1)}.eb:hover{fill:rgba(25, 25, 25, 1)}.pd:hover{background:rgba(15, 115, 12, 1)}.pm:hover{background:rgba(242, 242, 242, 1)}.pn:hover{border-color:rgba(242, 242, 242, 1)}.po:hover{cursor:wait}.pp:hover{color:rgba(242, 242, 242, 1)}.pq:hover{fill:rgba(242, 242, 242, 1)}.qh:hover{fill:rgba(117, 117, 117, 1)}.vd:hover{text-decoration:underline}.iq:focus{transform:scale(1.01)}.qg:focus{fill:rgba(117, 117, 117, 1)}.qe:active{border-style:none}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.x{display:flex}.au{margin:0 64px}.gn{font-size:46px}.go{margin-top:0.6em}.gp{line-height:56px}.gq{letter-spacing:-0.011em}.hi{margin-left:30px}.ia{font-size:21px}.ib{line-height:32px}.ic{letter-spacing:-0.003em}.il{margin-top:56px}.jb{margin-top:2em}.kd{margin-top:1.05em}.lf{font-size:30px}.lg{margin-top:1.95em}.lh{line-height:36px}.li{letter-spacing:0}.lr{max-width:1192px}.lz{margin-top:0.86em}.pz{margin-right:5px}.qo{margin-top:0px}.re{margin-top:5px}.rh{display:inline-block}.sj{font-size:20px}.sk{line-height:24px}.sl{max-height:48px}.st{margin:0}.th{font-size:22px}.ti{line-height:28px}.tv{width:calc(100% + 32px)}.tw{margin-left:-16px}.tx{margin-right:-16px}.um{padding-left:16px}.un{padding-right:16px}.uo{flex-basis:25%}.up{max-width:25%}.va{font-size:16px}.vb{line-height:20px}.vp{min-width:70px}.vq{min-height:70px}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.hh{margin-left:30px}.iv{margin-left:auto}.iw{text-align:center}.py{margin-right:5px}.qn{margin-top:0px}.rd{margin-top:5px}.rg{display:inline-block}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.hg{margin-left:30px}.px{margin-right:5px}.qm{margin-top:0px}.rb{display:inline-block}.rc{margin-top:5px}.uu{margin-right:16px}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.u{margin-bottom:20px}.ai{box-shadow:inset 0 -1px 0 rgba(230, 230, 230, 1)}.aj{min-height:230px}.am{display:block}.cl{min-height:98px}.cm{display:flex}.cn{align-items:flex-start}.co{flex-direction:column}.cp{justify-content:flex-end}.cv{margin-bottom:28px}.cw{margin-top:0px}.cz{margin-top:28px}.em{border-top:1px solid rgba(230, 230, 230, 1)}.en{border-bottom:1px solid rgba(230, 230, 230, 1)}.ey{align-items:center}.ez{flex:1 0 auto}.gu{margin-top:32px}.gv{flex-direction:column-reverse}.he{margin-bottom:30px}.hf{margin-left:0px}.pw{margin-left:8px}.qk{margin-top:2px}.ql{margin-right:16px}.ra{display:inline-block}.sx{padding-bottom:12px}.sy{margin-top:16px}.ve{margin-left:16px}.vf{margin-right:0px}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.aq{margin:0 24px}.ev{display:block}.fx{font-size:32px}.fy{margin-top:0.64em}.fz{line-height:40px}.ga{letter-spacing:-0.016em}.gt{margin-top:32px}.hc{margin-bottom:30px}.hd{margin-left:0px}.ho{font-size:18px}.hp{line-height:28px}.hq{letter-spacing:-0.003em}.ih{margin-top:40px}.ix{margin-top:1.56em}.jz{margin-top:1.34em}.kr{font-size:22px}.ks{margin-top:1.2em}.kt{letter-spacing:0}.lk{margin:0}.ll{max-width:100%}.lv{margin-top:0.67em}.pv{margin-left:8px}.qi{margin-top:2px}.qj{margin-right:16px}.qz{display:inline-block}.rx{font-size:16px}.ry{line-height:20px}.rz{max-height:40px}.sz{font-size:20px}.ta{line-height:24px}.tj{width:calc(100% + 24px)}.tk{margin-left:-12px}.tl{margin-right:-12px}.ty{padding-left:12px}.tz{padding-right:12px}.ua{flex-basis:100%}.vc{margin-bottom:0px}.vh{min-width:48px}.vi{min-height:48px}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.y{display:flex}.at{margin:0 64px}.gj{font-size:46px}.gk{margin-top:0.6em}.gl{line-height:56px}.gm{letter-spacing:-0.011em}.hx{font-size:21px}.hy{line-height:32px}.hz{letter-spacing:-0.003em}.ik{margin-top:56px}.ja{margin-top:2em}.kc{margin-top:1.05em}.lb{font-size:30px}.lc{margin-top:1.95em}.ld{line-height:36px}.le{letter-spacing:0}.lq{max-width:1192px}.ly{margin-top:0.86em}.sg{font-size:20px}.sh{line-height:24px}.si{max-height:48px}.ss{margin:0}.tf{font-size:22px}.tg{line-height:28px}.ts{width:calc(100% + 32px)}.tt{margin-left:-16px}.tu{margin-right:-16px}.ui{padding-left:16px}.uj{padding-right:16px}.uk{flex-basis:25%}.ul{max-width:25%}.uy{font-size:16px}.uz{line-height:20px}.vn{min-width:70px}.vo{min-height:70px}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.z{display:flex}.as{margin:0 48px}.gf{font-size:46px}.gg{margin-top:0.6em}.gh{line-height:56px}.gi{letter-spacing:-0.011em}.hu{font-size:21px}.hv{line-height:32px}.hw{letter-spacing:-0.003em}.ij{margin-top:56px}.iz{margin-top:2em}.kb{margin-top:1.05em}.kx{font-size:30px}.ky{margin-top:1.95em}.kz{line-height:36px}.la{letter-spacing:0}.lo{margin:0}.lp{max-width:100%}.lx{margin-top:0.86em}.sd{font-size:20px}.se{line-height:24px}.sf{max-height:48px}.td{font-size:22px}.te{line-height:28px}.tp{width:calc(100% + 28px)}.tq{margin-left:-14px}.tr{margin-right:-14px}.ue{padding-left:14px}.uf{padding-right:14px}.ug{flex-basis:50%}.uh{max-width:50%}.uw{font-size:16px}.ux{line-height:20px}.vl{min-width:48px}.vm{min-height:48px}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ar{margin:0 24px}.ew{display:block}.gb{font-size:32px}.gc{margin-top:0.64em}.gd{line-height:40px}.ge{letter-spacing:-0.016em}.hr{font-size:18px}.hs{line-height:28px}.ht{letter-spacing:-0.003em}.ii{margin-top:40px}.iy{margin-top:1.56em}.ka{margin-top:1.34em}.ku{font-size:22px}.kv{margin-top:1.2em}.kw{letter-spacing:0}.lm{margin:0}.ln{max-width:100%}.lw{margin-top:0.67em}.sa{font-size:16px}.sb{line-height:20px}.sc{max-height:40px}.tb{font-size:20px}.tc{line-height:24px}.tm{width:calc(100% + 24px)}.tn{margin-left:-12px}.to{margin-right:-12px}.ub{padding-left:12px}.uc{padding-right:12px}.ud{flex-basis:100%}.vj{min-width:48px}.vk{min-height:48px}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="print">.hj{display:none}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.et{animation:k3 .2s ease-in-out both}.in{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.or{transition:opacity 200ms}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (max-width: 1230px)">.os{display:none}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (max-width: 1240px)">.ov{display:none}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.sq{max-height:none}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div class="s"><div class="t s u"><div class="ag ah s ai aj"><div class="n ak"><div class="al am"><div class="ao s ap ae"><div class="n p"><div class="aq ar as at au av aw w"><div class="ax n o"><div class="n o ay az"><div class="ba s"><span><button class="bb b bc bd be bf bg bh bi bj bk bl bm bn bo bp bq br bs bt bu bv bw bx">Get started</button></span></div><div class="an"><div class="by al am"><span class="bb b bc bd bz"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F28b1b93e2088&amp;~feature=LoOpenInAppButton&amp;~channel=ShowPostUnderUser&amp;~stage=mobileNavBar&amp;source=post_page-----28b1b93e2088--------------------------------" class="be bh ca cb cc cd ce cf cg bm bj bk ch ci cj" rel="noopener follow">Open in app</a></span></div></div></div><a href="https://medium.com/?source=post_page-----28b1b93e2088--------------------------------" rel="noopener follow" aria-label="Homepage"><svg viewBox="0 0 1043.63 592.71" class="q r"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div><div class="n p"><div class="aq ar as at au av aw w"><div class="ah n o ay ck cl cm cn co cp"><div class="w n cq ck"><div class="v n w"><div class="cr cs w n ct ay cu cv cw cm cn co"><div class="cx cy s cz"><a rel="noopener follow" aria-label="Author Homepage" href="/?source=post_page-----28b1b93e2088--------------------------------"><span class="bb da db dc dd de df dg dh di dj dk dl dm dn">Jonathan Hui</span></a></div></div></div><div class="x y z k h ab ac o ae af"><p class="bb b bc bd bz"><span><a href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Fjonathan-hui.medium.com%2Freal-time-object-detection-with-yolo-yolov2-28b1b93e2088&amp;source=post_page-----28b1b93e2088---------------------nav_reg-----------" class="be bh ca cb cc cd ce cf cg bm bj bk ch ci cj" rel="noopener follow">Sign in</a></span></p><div class="do dp dq cy s"><span><button class="bb b bc bd be bf bg bh bi bj bk bl bm bn bo bp bq br bs bt bu bv bw bx">Get started</button></span></div><a href="https://medium.com/?source=post_page-----28b1b93e2088--------------------------------" rel="noopener follow" aria-label="Homepage"><svg viewBox="0 0 1043.63 592.71" class="q r"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div></div><div class="s"><div class="n p"><div class="aq ar as at au av aw w"><div><div class="dr ds n o"><div class="s"><span class="bb b dt du dv de df dg dh di dj dk bz"><div class="n o"><div class="dw dx ak"><span class="bb b bc bd bz"><a class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed" rel="noopener follow" href="/followers?source=post_page-----28b1b93e2088--------------------------------">22K Followers</a></span></div><div class="s h"></div><div class="s h"></div><div class="ee n ak h"><a class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed" rel="noopener follow" href="/about?source=post_page-----28b1b93e2088--------------------------------">About</a></div></div></span></div><div class="al ef eg am"><button class="n o p eh ei ej ek" aria-label="Expand navbar"><svg width="14" height="14" class="el"><path d="M0 .5h14M0 7h14M0 13.5h14"></path></svg></button></div></div></div></div></div></div></div><div class="em en c eo ep eq er es af ae et"><div class="n p"><div class="aq ar as at au av aw w"><div class="eu w ev ew j i d es ae"><div class="ex n o"><div class="al cm ey ez"><span><button class="bb b bc bd be bf bg bh bi bj bk bl bm bn bo bp bq br bs bt bu bv bw bx">Get started</button></span><div class="fa al am"><span class="bb b bc bd bz"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F28b1b93e2088&amp;~feature=LoOpenInAppButton&amp;~channel=ShowPostUnderUser&amp;~stage=mobileNavBar&amp;source=post_page-----28b1b93e2088--------------------------------" class="be bh ca cb cc cd ce cf cg bm bj bk ch ci cj" rel="noopener follow">Open in app</a></span></div></div><a href="https://medium.com/?source=post_page-----28b1b93e2088--------------------------------" rel="noopener follow" aria-label="Homepage"><svg viewBox="0 0 1043.63 592.71" class="q r"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div></div><article><section class="fb fc fd fe w ff bv s"></section><span class="s"></span><div><div class="ef eo fm fn fo fp"></div><section class="dn fq fr di fs"><div class="n p"><div class="aq ar as at au ft aw w"><div class=""><h1 id="79c8" class="fu dl fv bb fw fx fy fz ga gb gc gd ge gf gg gh gi gj gk gl gm gn go gp gq gr gs">Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3</h1><div class="cx"><div class="n ck gt gu gv"><div class="o n"><div><a rel="noopener follow" href="/?source=post_page-----28b1b93e2088--------------------------------"><img alt="Jonathan Hui" class="s gw gx gy" src="https://miro.medium.com/fit/c/56/56/1*c3Z3aOPBooxEX4tx4RkzLw.jpeg" width="28" height="28"/></a></div><div class="gz w n cu"><div class="n"><div style="flex:1"><span class="bb b bc bd gs"><a class="" rel="noopener follow" href="/?source=post_page-----28b1b93e2088--------------------------------"><p class="bb b bc bd be">Jonathan Hui</p></a></span></div></div><span class="bb b bc bd bz"><a class="" rel="noopener follow" href="/real-time-object-detection-with-yolo-yolov2-28b1b93e2088?source=post_page-----28b1b93e2088--------------------------------"><p class="bb b bc bd bz"><span class="ha"></span>Mar 18, 2018<span class="hb">·</span>18 min read</p></a></span></div></div><div class="n ct hc hd he hf hg hh hi hj"><div class="n o"><div class="bw" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bw" role="tooltip" aria-hidden="false"><button class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div><div class="hk s"></div><div class="s az"></div></div></div></div></div></div><p id="1749" class="hl hm fv hn b ho hp hq hr hs ht hu hv hw hx hy hz ia ib ic id ie dn gs"><strong class="hn fw">You only look once (YOLO)</strong> is an object detection system targeted for real-time processing. We will introduce YOLO, YOLOv2 and YOLO9000 in this article. For those only interested in YOLOv3, please forward to the<a href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088#b8ad" class="dy if" rel="noopener"> bottom of the article</a>. Here is the accuracy and speed comparison provided by the YOLO web site.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe ig"><img alt="" class="w ir is" src="https://miro.medium.com/max/1400/1*ju1oaoIkVUkaIPAdpehlzA.png" width="700" height="616" role="presentation"/></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><a href="https://pjreddie.com/darknet/yolo/" class="dy if" target="_blank" rel="noopener ugc nofollow">Source</a></figcaption></figure><p id="e01c" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">A demonstration from the YOLOv2.</p><figure class="ih ii ij ik il im"><div class="jc s ap"><div class="jd je s"></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">Object detection in real-time</figcaption></figure><p id="ff6b" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Let’s start with our own testing image below.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe jf"><div class="jc s ap jk"><div class="jl je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*EYFejGUjvjPcc4PZTwoufw.jpeg?q=20" width="700" height="471" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="471" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*EYFejGUjvjPcc4PZTwoufw.jpeg" width="700" height="471" srcSet="https://miro.medium.com/max/552/1*EYFejGUjvjPcc4PZTwoufw.jpeg 276w, https://miro.medium.com/max/1104/1*EYFejGUjvjPcc4PZTwoufw.jpeg 552w, https://miro.medium.com/max/1280/1*EYFejGUjvjPcc4PZTwoufw.jpeg 640w, https://miro.medium.com/max/1400/1*EYFejGUjvjPcc4PZTwoufw.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><strong class="bb jp">Testing image</strong></figcaption></figure><p id="acb9" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">The objects detected by YOLO:</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe jf"><div class="jc s ap jk"><div class="jl je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*QOGcvHbrDZiCqTG6THIQ_w.png?q=20" width="700" height="471" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="471" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*QOGcvHbrDZiCqTG6THIQ_w.png" width="700" height="471" srcSet="https://miro.medium.com/max/552/1*QOGcvHbrDZiCqTG6THIQ_w.png 276w, https://miro.medium.com/max/1104/1*QOGcvHbrDZiCqTG6THIQ_w.png 552w, https://miro.medium.com/max/1280/1*QOGcvHbrDZiCqTG6THIQ_w.png 640w, https://miro.medium.com/max/1400/1*QOGcvHbrDZiCqTG6THIQ_w.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">Objects detected by YOLO.</figcaption></figure><p id="4bc0" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Grid cell</strong></p><p id="65e3" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">For our discussion, we crop o<span id="rmm"><span id="rmm">u</span></span>r original photo. YOLO divides the input image into an <strong class="hn fw">S</strong>×<strong class="hn fw">S</strong> grid. Each grid cell predicts only <strong class="hn fw">one</strong> object. For example, the yellow grid cell below tries to predict the “person” object whose center (the blue dot) falls inside the grid cell.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe jq"><div class="jc s ap jk"><div class="jr je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*6qZXYCDUkC5Bc8nRolT0Mw.jpeg?q=20" width="700" height="412" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="412" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*6qZXYCDUkC5Bc8nRolT0Mw.jpeg" width="700" height="412" srcSet="https://miro.medium.com/max/552/1*6qZXYCDUkC5Bc8nRolT0Mw.jpeg 276w, https://miro.medium.com/max/1104/1*6qZXYCDUkC5Bc8nRolT0Mw.jpeg 552w, https://miro.medium.com/max/1280/1*6qZXYCDUkC5Bc8nRolT0Mw.jpeg 640w, https://miro.medium.com/max/1400/1*6qZXYCDUkC5Bc8nRolT0Mw.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">Each grid cell detects only one object.</figcaption></figure><p id="2220" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Each grid cell predicts a fixed number of boundary boxes. In this example, the yellow grid cell makes two boundary box predictions (blue boxes) to locate where the person is.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div class="fd fe js"><div class="jc s ap jk"><div class="jt je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*4Y1PaY3ZgxKt5w84_0pNxw.jpeg?q=20" width="667" height="669" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="667" height="669" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1334/1*4Y1PaY3ZgxKt5w84_0pNxw.jpeg" width="667" height="669" srcSet="https://miro.medium.com/max/552/1*4Y1PaY3ZgxKt5w84_0pNxw.jpeg 276w, https://miro.medium.com/max/1104/1*4Y1PaY3ZgxKt5w84_0pNxw.jpeg 552w, https://miro.medium.com/max/1280/1*4Y1PaY3ZgxKt5w84_0pNxw.jpeg 640w, https://miro.medium.com/max/1334/1*4Y1PaY3ZgxKt5w84_0pNxw.jpeg 667w" sizes="667px" role="presentation"/></noscript></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">Each grid cell make a fixed number of boundary box guesses for the object.</figcaption></figure><p id="779b" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">However, the one-object rule limits how close detected objects can be. For that, YOLO does have some limitations on how close objects can be. For the picture below, there are 9 Santas in the lower left corner but YOLO can detect 5 only.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe ju"><div class="jc s ap jk"><div class="jv je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*j4PnWfxP3yoVPOFyI27tww.jpeg?q=20" width="700" height="428" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="428" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*j4PnWfxP3yoVPOFyI27tww.jpeg" width="700" height="428" srcSet="https://miro.medium.com/max/552/1*j4PnWfxP3yoVPOFyI27tww.jpeg 276w, https://miro.medium.com/max/1104/1*j4PnWfxP3yoVPOFyI27tww.jpeg 552w, https://miro.medium.com/max/1280/1*j4PnWfxP3yoVPOFyI27tww.jpeg 640w, https://miro.medium.com/max/1400/1*j4PnWfxP3yoVPOFyI27tww.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">YOLO may miss objects that are too close.</figcaption></figure><p id="d9cf" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">For each grid cell,</p><ul class=""><li id="f040" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie jw jx jy gs">it predicts <strong class="hn fw">B</strong> boundary boxes and each box has one <strong class="hn fw">box confidence score</strong>,</li><li id="23b9" class="hl hm fv hn b ho jz hp hq hr ka hs ht hu kb hv hw hx kc hy hz ia kd ib ic ie jw jx jy gs">it detects <strong class="hn fw">one</strong> object only regardless of the number of boxes B,</li><li id="4e9c" class="hl hm fv hn b ho jz hp hq hr ka hs ht hu kb hv hw hx kc hy hz ia kd ib ic ie jw jx jy gs">it predicts <strong class="hn fw">C</strong> <strong class="hn fw">conditional class probabilities</strong> (one per class for the likeliness of the object class).</li></ul><p id="be90" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">To evaluate PASCAL VOC, YOLO uses 7×7 grids (S×S), 2 boundary boxes (B) and 20 classes (C).</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe ke"><div class="jc s ap jk"><div class="kf je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*OuMJUWo2rXYA-GYU63NUGw.jpeg?q=20" width="700" height="449" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="449" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*OuMJUWo2rXYA-GYU63NUGw.jpeg" width="700" height="449" srcSet="https://miro.medium.com/max/552/1*OuMJUWo2rXYA-GYU63NUGw.jpeg 276w, https://miro.medium.com/max/1104/1*OuMJUWo2rXYA-GYU63NUGw.jpeg 552w, https://miro.medium.com/max/1280/1*OuMJUWo2rXYA-GYU63NUGw.jpeg 640w, https://miro.medium.com/max/1400/1*OuMJUWo2rXYA-GYU63NUGw.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">YOLO makes SxS predictions with B boundary boxes.</figcaption></figure><p id="5ef3" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Let’s get into more details. Each boundary box contains 5 elements: (<em class="kg">x, y, w, h</em>) and a <strong class="hn fw">box confidence score</strong>. The confidence score reflects how likely the box contains an object (<strong class="hn fw">objectness</strong>) and how accurate is the boundary box. We normalize the bounding box width <em class="kg">w</em> and height <em class="kg">h</em> by the image width and height. <em class="kg">x</em> and <em class="kg">y</em> are offsets to the corresponding cell. Hence, <em class="kg">x, y, w</em> and <em class="kg">h</em> are all between 0 and 1. Each cell has 20 conditional class probabilities. The <strong class="hn fw">conditional class probability</strong> is the probability that the detected object belongs to a particular class (one probability per category for each cell). So, YOLO’s prediction has a shape of (S, S, B×5 + C) = (7, 7, 2×5 + 20) = (7, 7, 30).</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe kh"><div class="jc s ap jk"><div class="ki je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*jtnrhMFNwGxiQmkY6LkdCQ.jpeg?q=20" width="700" height="208" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="208" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*jtnrhMFNwGxiQmkY6LkdCQ.jpeg" width="700" height="208" srcSet="https://miro.medium.com/max/552/1*jtnrhMFNwGxiQmkY6LkdCQ.jpeg 276w, https://miro.medium.com/max/1104/1*jtnrhMFNwGxiQmkY6LkdCQ.jpeg 552w, https://miro.medium.com/max/1280/1*jtnrhMFNwGxiQmkY6LkdCQ.jpeg 640w, https://miro.medium.com/max/1400/1*jtnrhMFNwGxiQmkY6LkdCQ.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><a href="https://arxiv.org/pdf/1506.02640.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">Source</a></figcaption></figure><p id="03d4" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">The major concept of YOLO is to build a CNN network to predict a (7, 7, 30) tensor. It uses a CNN network to reduce the spatial dimension to 7×7 with 1024 output channels at each location. YOLO performs a linear regression using two fully connected layers to make 7×7×2 boundary box predictions (the middle picture below). To make a final prediction, we keep those with high box confidence scores (greater than 0.25) as our final predictions (the right picture).</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe kj"><div class="jc s ap jk"><div class="kk je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*e0VY6U1_WMF2KBoKQNZvkQ.png?q=20" width="700" height="192" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="192" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*e0VY6U1_WMF2KBoKQNZvkQ.png" width="700" height="192" srcSet="https://miro.medium.com/max/552/1*e0VY6U1_WMF2KBoKQNZvkQ.png 276w, https://miro.medium.com/max/1104/1*e0VY6U1_WMF2KBoKQNZvkQ.png 552w, https://miro.medium.com/max/1280/1*e0VY6U1_WMF2KBoKQNZvkQ.png 640w, https://miro.medium.com/max/1400/1*e0VY6U1_WMF2KBoKQNZvkQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><a href="https://pjreddie.com/darknet/yolo/" class="dy if" target="_blank" rel="noopener ugc nofollow">Source</a></figcaption></figure><p id="4733" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">The <strong class="hn fw">class confidence score </strong>for each prediction box is computed as:</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe kl"><div class="jc s ap jk"><div class="km je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*qVL77IZyEnra4DvENayXUA.png?q=20" width="700" height="42" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="42" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*qVL77IZyEnra4DvENayXUA.png" width="700" height="42" srcSet="https://miro.medium.com/max/552/1*qVL77IZyEnra4DvENayXUA.png 276w, https://miro.medium.com/max/1104/1*qVL77IZyEnra4DvENayXUA.png 552w, https://miro.medium.com/max/1280/1*qVL77IZyEnra4DvENayXUA.png 640w, https://miro.medium.com/max/1400/1*qVL77IZyEnra4DvENayXUA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="7161" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">It measures the confidence on both the classification and the <strong class="hn fw">localization</strong> (where an object is located).</p><p id="8084" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">We may mix up those scoring and probability terms easily. Here are the mathematical definitions for your future reference.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe kn"><div class="jc s ap jk"><div class="ko je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*0IPktA65WxOBfP_ULQWcmw.png?q=20" width="700" height="244" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="244" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*0IPktA65WxOBfP_ULQWcmw.png" width="700" height="244" srcSet="https://miro.medium.com/max/552/1*0IPktA65WxOBfP_ULQWcmw.png 276w, https://miro.medium.com/max/1104/1*0IPktA65WxOBfP_ULQWcmw.png 552w, https://miro.medium.com/max/1280/1*0IPktA65WxOBfP_ULQWcmw.png 640w, https://miro.medium.com/max/1400/1*0IPktA65WxOBfP_ULQWcmw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><h1 id="5fe5" class="kp kq fv bb da kr ks hp kt ku kv hs kw kx ky kz la lb lc ld le lf lg lh li lj gs"><strong class="ce">Network design</strong></h1></div></div><div class="im"><div class="n p"><div class="lk ll lm ln lo lp at lq au lr aw w"><figure class="ih ii ij ik il im lt lu paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe ls"><img alt="" class="w ir is" src="https://miro.medium.com/max/2000/1*9ER4GVUtQGVA2Y0skC9OQQ.png" width="1000" height="420" role="presentation"/></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><a href="https://arxiv.org/pdf/1506.02640.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">Source</a></figcaption></figure></div></div></div><div class="n p"><div class="aq ar as at au ft aw w"><p id="91ea" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">YOLO has 24 convolutional layers followed by 2 fully connected layers (FC). Some convolution layers use 1 × 1 reduction layers alternatively to reduce the depth of the features maps. For the last convolution layer, it outputs a tensor with shape (7, 7, 1024). The tensor is then flattened. Using 2 fully connected layers as a form of linear regression, it outputs 7×7×30 parameters and then reshapes to (7, 7, 30), i.e. 2 boundary box predictions per location.</p><p id="6672" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">A faster but less accurate version of YOLO, called Fast YOLO, uses only 9 convolutional layers with shallower feature maps.</p><h1 id="7bde" class="kp kq fv bb da kr ks hp kt ku kv hs kw kx ky kz la lb lc ld le lf lg lh li lj gs"><strong class="ce">Loss function</strong></h1><p id="5db5" class="hl hm fv hn b ho lv hp hq hr lw hs ht hu lx hv hw hx ly hy hz ia lz ib ic ie dn gs">YOLO predicts multiple bounding boxes per grid cell. To compute the loss for the true positive, we only want one of them to be <strong class="hn fw">responsible</strong> for the object. For this purpose, we select the one with the highest IoU (intersection over union) with the ground truth. This strategy leads to specialization among the bounding box predictions. Each prediction gets better at predicting certain sizes and aspect ratios.</p><p id="8503" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">YOLO uses sum-squared error between the predictions and the ground truth to calculate loss. The loss function composes of:</p><ul class=""><li id="d975" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie jw jx jy gs">the <strong class="hn fw">classification loss</strong>.</li><li id="ef04" class="hl hm fv hn b ho jz hp hq hr ka hs ht hu kb hv hw hx kc hy hz ia kd ib ic ie jw jx jy gs">the <strong class="hn fw">localization loss</strong> (errors between the predicted boundary box and the ground truth).</li><li id="718c" class="hl hm fv hn b ho jz hp hq hr ka hs ht hu kb hv hw hx kc hy hz ia kd ib ic ie jw jx jy gs">the <strong class="hn fw">confidence loss</strong> (the objectness of the box).</li></ul><p id="b1d2" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Classification loss</strong></p><p id="4291" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">If <em class="kg">an object is detected</em>, the classification loss at each cell is the squared error of the class conditional probabilities for each class:</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe ma"><div class="jc s ap jk"><div class="mb je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*lF6SCAVj5jMwLxs39SCogw.png?q=20" width="700" height="209" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="209" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*lF6SCAVj5jMwLxs39SCogw.png" width="700" height="209" srcSet="https://miro.medium.com/max/552/1*lF6SCAVj5jMwLxs39SCogw.png 276w, https://miro.medium.com/max/1104/1*lF6SCAVj5jMwLxs39SCogw.png 552w, https://miro.medium.com/max/1280/1*lF6SCAVj5jMwLxs39SCogw.png 640w, https://miro.medium.com/max/1400/1*lF6SCAVj5jMwLxs39SCogw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="7476" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Localization loss</strong></p><p id="e7fa" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">The localization loss measures the errors in the predicted boundary box locations and sizes. We only count the box responsible for detecting the object.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe mc"><div class="jc s ap jk"><div class="md je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*BwhGMvffFfqtND9413oiwA.png?q=20" width="700" height="268" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="268" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*BwhGMvffFfqtND9413oiwA.png" width="700" height="268" srcSet="https://miro.medium.com/max/552/1*BwhGMvffFfqtND9413oiwA.png 276w, https://miro.medium.com/max/1104/1*BwhGMvffFfqtND9413oiwA.png 552w, https://miro.medium.com/max/1280/1*BwhGMvffFfqtND9413oiwA.png 640w, https://miro.medium.com/max/1400/1*BwhGMvffFfqtND9413oiwA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="890d" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">We do not want to weight absolute errors in large boxes and small boxes equally. i.e. a 2-pixel error in a large box is the same for a small box. To partially address this, YOLO predicts the square root of the bounding box width and height instead of the width and height. In addition, to put more emphasis on the boundary box accuracy, we multiply the loss by λ<em class="kg">coord</em> (default: 5).</p><p id="34aa" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Confidence loss</strong></p><p id="d347" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">If <em class="kg">an object is detected in the box</em>, the confidence loss (measuring the objectness of the box) is:</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe me"><div class="jc s ap jk"><div class="mf je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*QT7mwEbyLJYIxTYtOWClFQ.png?q=20" width="700" height="196" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="196" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*QT7mwEbyLJYIxTYtOWClFQ.png" width="700" height="196" srcSet="https://miro.medium.com/max/552/1*QT7mwEbyLJYIxTYtOWClFQ.png 276w, https://miro.medium.com/max/1104/1*QT7mwEbyLJYIxTYtOWClFQ.png 552w, https://miro.medium.com/max/1280/1*QT7mwEbyLJYIxTYtOWClFQ.png 640w, https://miro.medium.com/max/1400/1*QT7mwEbyLJYIxTYtOWClFQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="6918" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">If <em class="kg">an object is not detected in the box</em>, the confidence loss is:</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe mg"><div class="jc s ap jk"><div class="mh je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*Yc_OJIXOoV2WaGQ6PqhTXA.png?q=20" width="700" height="253" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="253" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*Yc_OJIXOoV2WaGQ6PqhTXA.png" width="700" height="253" srcSet="https://miro.medium.com/max/552/1*Yc_OJIXOoV2WaGQ6PqhTXA.png 276w, https://miro.medium.com/max/1104/1*Yc_OJIXOoV2WaGQ6PqhTXA.png 552w, https://miro.medium.com/max/1280/1*Yc_OJIXOoV2WaGQ6PqhTXA.png 640w, https://miro.medium.com/max/1400/1*Yc_OJIXOoV2WaGQ6PqhTXA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="ca3e" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Most boxes do not contain any objects. This causes a class imbalance problem, i.e. we train the model to detect background more frequently than detecting objects. To remedy this, we weight this loss down by a factor λ<em class="kg">noobj</em> (default: 0.5).</p><p id="e037" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Loss</strong></p><p id="a20c" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">The final loss adds localization, confidence and classification losses together.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe mi"><div class="jc s ap jk"><div class="mj je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*aW6htqx4Q7APLrSQg2eWDw.png?q=20" width="700" height="327" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="327" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*aW6htqx4Q7APLrSQg2eWDw.png" width="700" height="327" srcSet="https://miro.medium.com/max/552/1*aW6htqx4Q7APLrSQg2eWDw.png 276w, https://miro.medium.com/max/1104/1*aW6htqx4Q7APLrSQg2eWDw.png 552w, https://miro.medium.com/max/1280/1*aW6htqx4Q7APLrSQg2eWDw.png 640w, https://miro.medium.com/max/1400/1*aW6htqx4Q7APLrSQg2eWDw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><a href="https://arxiv.org/pdf/1506.02640.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">Source</a></figcaption></figure><h1 id="7519" class="kp kq fv bb da kr ks hp kt ku kv hs kw kx ky kz la lb lc ld le lf lg lh li lj gs">Inference: Non-maximal suppression</h1><p id="9c17" class="hl hm fv hn b ho lv hp hq hr lw hs ht hu lx hv hw hx ly hy hz ia lz ib ic ie dn gs">YOLO can make duplicate detections for the same object. To fix this, YOLO applies non-maximal suppression to remove duplications with lower confidence. Non-maximal suppression adds 2- 3% in mAP.</p><p id="a146" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Here is one of the possible non-maximal suppression implementation:</p><ol class=""><li id="9fab" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie mk jx jy gs">Sort the predictions by the confidence scores.</li><li id="7185" class="hl hm fv hn b ho jz hp hq hr ka hs ht hu kb hv hw hx kc hy hz ia kd ib ic ie mk jx jy gs">Start from the top scores, ignore any current prediction if we find any previous predictions that have the same class and IoU &gt; 0.5 with the current prediction.</li><li id="691a" class="hl hm fv hn b ho jz hp hq hr ka hs ht hu kb hv hw hx kc hy hz ia kd ib ic ie mk jx jy gs">Repeat step 2 until all predictions are checked.</li></ol><h1 id="73bf" class="kp kq fv bb da kr ks hp kt ku kv hs kw kx ky kz la lb lc ld le lf lg lh li lj gs"><strong class="ce">Benefits of YOLO</strong></h1><ul class=""><li id="49ce" class="hl hm fv hn b ho lv hp hq hr lw hs ht hu lx hv hw hx ly hy hz ia lz ib ic ie jw jx jy gs">Fast. Good for real-time processing.</li><li id="0c9b" class="hl hm fv hn b ho jz hp hq hr ka hs ht hu kb hv hw hx kc hy hz ia kd ib ic ie jw jx jy gs">Predictions (object locations and classes) are made from one single network. Can be trained end-to-end to improve accuracy.</li><li id="4722" class="hl hm fv hn b ho jz hp hq hr ka hs ht hu kb hv hw hx kc hy hz ia kd ib ic ie jw jx jy gs">YOLO is more generalized. It outperforms other methods when generalizing from natural images to other domains like artwork.</li></ul><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe ml"><div class="jc s ap jk"><div class="mm je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*XbOnbcZmc50hyhhTwhD5QA.png?q=20" width="700" height="250" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="250" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*XbOnbcZmc50hyhhTwhD5QA.png" width="700" height="250" srcSet="https://miro.medium.com/max/552/1*XbOnbcZmc50hyhhTwhD5QA.png 276w, https://miro.medium.com/max/1104/1*XbOnbcZmc50hyhhTwhD5QA.png 552w, https://miro.medium.com/max/1280/1*XbOnbcZmc50hyhhTwhD5QA.png 640w, https://miro.medium.com/max/1400/1*XbOnbcZmc50hyhhTwhD5QA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><ul class=""><li id="d34b" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie jw jx jy gs">Region proposal methods limit the classifier to the specific region. YOLO accesses to the whole image in predicting boundaries. With the additional context, YOLO demonstrates fewer false positives in background areas.</li><li id="fb3f" class="hl hm fv hn b ho jz hp hq hr ka hs ht hu kb hv hw hx kc hy hz ia kd ib ic ie jw jx jy gs">YOLO detects one object per grid cell. It enforces spatial diversity in making predictions.</li></ul><h1 id="77ea" class="kp kq fv bb da kr ks hp kt ku kv hs kw kx ky kz la lb lc ld le lf lg lh li lj gs">YOLOv2</h1><p id="2df6" class="hl hm fv hn b ho lv hp hq hr lw hs ht hu lx hv hw hx ly hy hz ia lz ib ic ie dn gs">SSD is a strong competitor for YOLO which at one point demonstrates higher accuracy for real-time processing. Comparing with region based detectors, YOLO has higher localization errors and the recall (measure how good to locate all objects) is lower. YOLOv2 is the second version of the YOLO with the objective of improving the accuracy significantly while making it faster.</p><h1 id="12a7" class="kp kq fv bb da kr ks hp kt ku kv hs kw kx ky kz la lb lc ld le lf lg lh li lj gs">Accuracy improvements</h1><p id="63fc" class="hl hm fv hn b ho lv hp hq hr lw hs ht hu lx hv hw hx ly hy hz ia lz ib ic ie dn gs"><strong class="hn fw">Batch normalization</strong></p><p id="34ba" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Add batch normalization in convolution layers. This removes the need for dropouts and pushes mAP up 2%.</p><p id="f021" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">High-resolution classifier</strong></p><p id="6a6d" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">The YOLO training composes of 2 phases. First, we train a classifier network like VGG16. Then we replace the fully connected layers with a convolution layer and retrain it end-to-end for the object detection. YOLO trains the classifier with 224 × 224 pictures followed by 448 × 448 pictures for the object detection. YOLOv2 starts with 224 × 224 pictures for the classifier training but then retune the classifier again with 448 × 448 pictures using much fewer epochs. This makes the detector training easier and moves mAP up by 4%.</p><p id="afa6" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Convolutional with Anchor Boxes</strong></p><p id="77f0" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">As indicated in the <a href="https://arxiv.org/pdf/1506.02640.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">YOLO paper</a>, the early training is susceptible to unstable gradients. Initially, YOLO makes arbitrary guesses on the boundary boxes. These guesses may work well for some objects but badly for others resulting in steep gradient changes. In early training, predictions are fighting with each other on what shapes to specialize on.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div class="fd fe mn"><div class="jc s ap jk"><div class="mo je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*7iwTsezrn-tSndx96twprA.jpeg?q=20" width="691" height="498" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="691" height="498" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1382/1*7iwTsezrn-tSndx96twprA.jpeg" width="691" height="498" srcSet="https://miro.medium.com/max/552/1*7iwTsezrn-tSndx96twprA.jpeg 276w, https://miro.medium.com/max/1104/1*7iwTsezrn-tSndx96twprA.jpeg 552w, https://miro.medium.com/max/1280/1*7iwTsezrn-tSndx96twprA.jpeg 640w, https://miro.medium.com/max/1382/1*7iwTsezrn-tSndx96twprA.jpeg 691w" sizes="691px" role="presentation"/></noscript></div></div></div></figure><p id="4704" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">In the real-life domain, the boundary boxes are not arbitrary. Cars have very similar shapes and pedestrians have an approximate aspect ratio of 0.41.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe mp"><div class="jc s ap jk"><div class="mq je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*krGqonOLMzSE_PWqH_LvQA.jpeg?q=20" width="700" height="372" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="372" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*krGqonOLMzSE_PWqH_LvQA.jpeg" width="700" height="372" srcSet="https://miro.medium.com/max/552/1*krGqonOLMzSE_PWqH_LvQA.jpeg 276w, https://miro.medium.com/max/1104/1*krGqonOLMzSE_PWqH_LvQA.jpeg 552w, https://miro.medium.com/max/1280/1*krGqonOLMzSE_PWqH_LvQA.jpeg 640w, https://miro.medium.com/max/1400/1*krGqonOLMzSE_PWqH_LvQA.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><a href="https://www.youtube.com/watch?v=xVwsr9p3irA" class="dy if" target="_blank" rel="noopener ugc nofollow">Source</a></figcaption></figure><p id="4c68" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Since we only need one guess to be right, the initial training will be more stable if we start with diverse guesses that are common for real-life objects.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div class="fd fe mn"><div class="jc s ap jk"><div class="mo je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*CGWTPTY0sfvQoxsS0X6VFg.jpeg?q=20" width="691" height="498" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="691" height="498" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1382/1*CGWTPTY0sfvQoxsS0X6VFg.jpeg" width="691" height="498" srcSet="https://miro.medium.com/max/552/1*CGWTPTY0sfvQoxsS0X6VFg.jpeg 276w, https://miro.medium.com/max/1104/1*CGWTPTY0sfvQoxsS0X6VFg.jpeg 552w, https://miro.medium.com/max/1280/1*CGWTPTY0sfvQoxsS0X6VFg.jpeg 640w, https://miro.medium.com/max/1382/1*CGWTPTY0sfvQoxsS0X6VFg.jpeg 691w" sizes="691px" role="presentation"/></noscript></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">More diverse predictions</figcaption></figure><p id="3040" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">For example, we can create 5 <strong class="hn fw">anchor</strong> boxes with the following shapes.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div class="fd fe mr"><div class="jc s ap jk"><div class="ms je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*8Q8r9ixjTiKLi1mrF36xCw.jpeg?q=20" width="504" height="227" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="504" height="227" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1008/1*8Q8r9ixjTiKLi1mrF36xCw.jpeg" width="504" height="227" srcSet="https://miro.medium.com/max/552/1*8Q8r9ixjTiKLi1mrF36xCw.jpeg 276w, https://miro.medium.com/max/1008/1*8Q8r9ixjTiKLi1mrF36xCw.jpeg 504w" sizes="504px" role="presentation"/></noscript></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">5 anchor boxes</figcaption></figure><p id="8507" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Instead of predicting 5 arbitrary boundary boxes, we predict offsets to each of the anchor boxes above. If we <strong class="hn fw">constrain</strong> the offset values, we can maintain the diversity of the predictions and have each prediction focuses on a specific shape. So the initial training will be more stable.</p><blockquote class="mt mu mv"><p id="2366" class="hl hm kg hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">In the paper, anchors are also called <strong class="hn fw">priors</strong>.</p></blockquote><p id="4b0b" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Here are the changes we make to the network:</p><ul class=""><li id="dd5b" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie jw jx jy gs">Remove the fully connected layers responsible for predicting the boundary box.</li></ul><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe mw"><div class="jc s ap jk"><div class="mx je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*tavjieD0Bum_uX-svYUTKA.jpeg?q=20" width="700" height="297" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="297" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*tavjieD0Bum_uX-svYUTKA.jpeg" width="700" height="297" srcSet="https://miro.medium.com/max/552/1*tavjieD0Bum_uX-svYUTKA.jpeg 276w, https://miro.medium.com/max/1104/1*tavjieD0Bum_uX-svYUTKA.jpeg 552w, https://miro.medium.com/max/1280/1*tavjieD0Bum_uX-svYUTKA.jpeg 640w, https://miro.medium.com/max/1400/1*tavjieD0Bum_uX-svYUTKA.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><ul class=""><li id="4874" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie jw jx jy gs">We move the class prediction from the cell level to the boundary box level. Now, each prediction includes 4 parameters for the boundary box, 1 box confidence score (objectness) and 20 class probabilities. i.e. 5 boundary boxes with 25 parameters: 125 parameters per grid cell. Same as YOLO, the objectness prediction still predicts the IOU of the ground truth and the proposed box.</li></ul><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe my"><div class="jc s ap jk"><div class="mz je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*UsqjfoW3sLkmyXKQ0Hyo8A.png?q=20" width="700" height="319" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="319" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*UsqjfoW3sLkmyXKQ0Hyo8A.png" width="700" height="319" srcSet="https://miro.medium.com/max/552/1*UsqjfoW3sLkmyXKQ0Hyo8A.png 276w, https://miro.medium.com/max/1104/1*UsqjfoW3sLkmyXKQ0Hyo8A.png 552w, https://miro.medium.com/max/1280/1*UsqjfoW3sLkmyXKQ0Hyo8A.png 640w, https://miro.medium.com/max/1400/1*UsqjfoW3sLkmyXKQ0Hyo8A.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><ul class=""><li id="0298" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie jw jx jy gs">To generate predictions with a shape of 7 × 7 × 125, we replace the last convolution layer with three 3 × 3 convolutional layers each outputting 1024 output channels. Then we apply a final 1 × 1 convolutional layer to convert the 7 × 7 × 1024 output into 7 × 7 × 125. (See the section on DarkNet for the details.)</li></ul><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe na"><div class="jc s ap jk"><div class="nb je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*KiSd2CBfcs5af6oliHCqzw.jpeg?q=20" width="700" height="419" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="419" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*KiSd2CBfcs5af6oliHCqzw.jpeg" width="700" height="419" srcSet="https://miro.medium.com/max/552/1*KiSd2CBfcs5af6oliHCqzw.jpeg 276w, https://miro.medium.com/max/1104/1*KiSd2CBfcs5af6oliHCqzw.jpeg 552w, https://miro.medium.com/max/1280/1*KiSd2CBfcs5af6oliHCqzw.jpeg 640w, https://miro.medium.com/max/1400/1*KiSd2CBfcs5af6oliHCqzw.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">Using convolution filters to make predictions.</figcaption></figure><ul class=""><li id="ef2c" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie jw jx jy gs">Change the input image size from 448 × 448 to 416 × 416. This creates an odd number spatial dimension (7×7 v.s. 8×8 grid cell). The center of a picture is often occupied by a large object. With an odd number grid cell, it is more certain on where the object belongs.</li></ul><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe nc"><div class="jc s ap jk"><div class="nd je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*89qezEeLKJLpD8_fM_H4qQ.jpeg?q=20" width="700" height="323" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="323" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*89qezEeLKJLpD8_fM_H4qQ.jpeg" width="700" height="323" srcSet="https://miro.medium.com/max/552/1*89qezEeLKJLpD8_fM_H4qQ.jpeg 276w, https://miro.medium.com/max/1104/1*89qezEeLKJLpD8_fM_H4qQ.jpeg 552w, https://miro.medium.com/max/1280/1*89qezEeLKJLpD8_fM_H4qQ.jpeg 640w, https://miro.medium.com/max/1400/1*89qezEeLKJLpD8_fM_H4qQ.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><ul class=""><li id="7155" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie jw jx jy gs">Remove one pooling layer to make the spatial output of the network to <strong class="hn fw">13×13 </strong>(instead of 7×7).</li></ul><p id="9de4" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Anchor boxes decrease mAP slightly from 69.5 to 69.2 but the recall improves from 81% to 88%. i.e. even the accuracy is slightly decreased but it increases the chances of detecting all the ground truth objects.</p><p id="6c07" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Dimension Clusters</strong></p><p id="ea00" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">In many problem domains, the boundary boxes have strong patterns. For example, in the autonomous driving, the 2 most common boundary boxes will be cars and pedestrians at different distances. To identify the top-K boundary boxes that have the best coverage for the training data, we run K-means clustering on the training data to locate the centroids of the top-K clusters.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div class="fd fe ne"><div class="jc s ap jk"><div class="nf je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*esSmI0UaMr-GrqkUGMg0hA.jpeg?q=20" width="388" height="298" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="388" height="298" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/776/1*esSmI0UaMr-GrqkUGMg0hA.jpeg" width="388" height="298" srcSet="https://miro.medium.com/max/552/1*esSmI0UaMr-GrqkUGMg0hA.jpeg 276w, https://miro.medium.com/max/776/1*esSmI0UaMr-GrqkUGMg0hA.jpeg 388w" sizes="388px" role="presentation"/></noscript></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">(Image modified form <a href="https://mapr.com/blog/monitoring-real-time-uber-data-using-spark-machine-learning-streaming-and-kafka-api-part-1/" class="dy if" target="_blank" rel="noopener ugc nofollow">a k-means cluster</a>)</figcaption></figure><p id="0b47" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Since we are dealing with boundary boxes rather than points, we cannot use the regular spatial distance to measure datapoint distances. No surprise, we use IoU.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe ng"><div class="jc s ap jk"><div class="nh je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*l5wvrPjLlFp6Whgy0MqbKQ.png?q=20" width="700" height="371" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="371" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*l5wvrPjLlFp6Whgy0MqbKQ.png" width="700" height="371" srcSet="https://miro.medium.com/max/552/1*l5wvrPjLlFp6Whgy0MqbKQ.png 276w, https://miro.medium.com/max/1104/1*l5wvrPjLlFp6Whgy0MqbKQ.png 552w, https://miro.medium.com/max/1280/1*l5wvrPjLlFp6Whgy0MqbKQ.png 640w, https://miro.medium.com/max/1400/1*l5wvrPjLlFp6Whgy0MqbKQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><a href="https://arxiv.org/pdf/1612.08242.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">Source</a></figcaption></figure><p id="a0d7" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">On the left, we plot the average IoU between the anchors and the ground truth boxes using different numbers of clusters (anchors). As the number of anchors increases, the accuracy improvement plateaus. For the best return, YOLO settles down with 5 anchors. On the right, it displays the 5 anchors’ shapes. The purplish-blue rectangles are selected from the COCO dataset while the black border rectangles are selected from the VOC2007. In both cases, we have more thin and tall anchors indicating that real-life boundary boxes are not arbitrary.</p><blockquote class="mt mu mv"><p id="ae5e" class="hl hm kg hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Unless we are comparing YOLO and YOLOv2, we will reference YOLOv2 as YOLO for now.</p></blockquote><p id="0780" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Direct location prediction</strong></p><p id="d58d" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">We make predictions on the offsets to the anchors. Nevertheless, if it is unconstrained, our guesses will be randomized again. YOLO predicts 5 parameters (t<em class="kg">x</em>, t<em class="kg">y</em>, t<em class="kg">w</em>, t<em class="kg">h</em>, and t<em class="kg">o</em>) and applies the sigma function to constraint its possible offset range.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe ni"><div class="jc s ap jk"><div class="nj je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*38-Tdx-wQA7c3TX5hdnwpw.jpeg?q=20" width="700" height="465" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="465" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*38-Tdx-wQA7c3TX5hdnwpw.jpeg" width="700" height="465" srcSet="https://miro.medium.com/max/552/1*38-Tdx-wQA7c3TX5hdnwpw.jpeg 276w, https://miro.medium.com/max/1104/1*38-Tdx-wQA7c3TX5hdnwpw.jpeg 552w, https://miro.medium.com/max/1280/1*38-Tdx-wQA7c3TX5hdnwpw.jpeg 640w, https://miro.medium.com/max/1400/1*38-Tdx-wQA7c3TX5hdnwpw.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="60ab" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Here is the visualization. The blue box below is the predicted boundary box and the dotted rectangle is the anchor.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe nk"><div class="jc s ap jk"><div class="nl je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*gyOSRA_FDz4Pf5njoUb4KQ.jpeg?q=20" width="700" height="530" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="530" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*gyOSRA_FDz4Pf5njoUb4KQ.jpeg" width="700" height="530" srcSet="https://miro.medium.com/max/552/1*gyOSRA_FDz4Pf5njoUb4KQ.jpeg 276w, https://miro.medium.com/max/1104/1*gyOSRA_FDz4Pf5njoUb4KQ.jpeg 552w, https://miro.medium.com/max/1280/1*gyOSRA_FDz4Pf5njoUb4KQ.jpeg 640w, https://miro.medium.com/max/1400/1*gyOSRA_FDz4Pf5njoUb4KQ.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">Modified from the <a href="https://arxiv.org/pdf/1612.08242.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">paper</a>.</figcaption></figure><p id="91a9" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">With the use of k-means clustering (dimension clusters) and the improvement mentioned in this section, mAP increases 5%.</p><p id="5583" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Fine-Grained Features</strong></p><p id="2313" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Convolution layers decrease the spatial dimension gradually. As the corresponding resolution decreases, it is harder to detect small objects. Other object detectors like SSD locate objects from different layers of feature maps. So each layer specializes at a different scale. YOLO adopts a different approach called passthrough. It reshapes the 26 × 26 × 512 layer to 13 × 13 × 2048. Then it concatenates with the original 13 × 13 ×1024 output layer. Now we apply convolution filters on the new 13 × 13 × 3072 layer to make predictions.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe nm"><div class="jc s ap jk"><div class="nn je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*RuW-SCIML8SHc5_PrIE9-g.jpeg?q=20" width="700" height="151" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="151" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*RuW-SCIML8SHc5_PrIE9-g.jpeg" width="700" height="151" srcSet="https://miro.medium.com/max/552/1*RuW-SCIML8SHc5_PrIE9-g.jpeg 276w, https://miro.medium.com/max/1104/1*RuW-SCIML8SHc5_PrIE9-g.jpeg 552w, https://miro.medium.com/max/1280/1*RuW-SCIML8SHc5_PrIE9-g.jpeg 640w, https://miro.medium.com/max/1400/1*RuW-SCIML8SHc5_PrIE9-g.jpeg 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="32a7" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Multi-Scale Training</strong></p><p id="3c3d" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">After removing the fully connected layers, YOLO can take images of different sizes. If the width and height are doubled, we are just making 4x output grid cells and therefore 4x predictions. Since the YOLO network downsamples the input by 32, we just need to make sure the width and height is a multiple of 32. During training, YOLO takes images of size 320×320, 352×352, … and 608×608 (with a step of 32). For every 10 batches, YOLOv2 randomly selects another image size to train the model. This acts as data augmentation and forces the network to predict well for different input image dimension and scale. In additional, we can use lower resolution images for object detection at the cost of accuracy. This can be a good tradeoff for speed on low GPU power devices. At 288 × 288 YOLO runs at more than 90 FPS with mAP almost as good as Fast R-CNN. At high-resolution YOLO achieves 78.6 mAP on VOC 2007.</p><h1 id="8d9e" class="kp kq fv bb da kr ks hp kt ku kv hs kw kx ky kz la lb lc ld le lf lg lh li lj gs"><strong class="ce">Accuracy</strong></h1><p id="97e0" class="hl hm fv hn b ho lv hp hq hr lw hs ht hu lx hv hw hx ly hy hz ia lz ib ic ie dn gs">Here is the accuracy improvements after applying the techniques discussed so far:</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe no"><div class="jc s ap jk"><div class="np je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*3IdCKSzR5R0lIE1LSmN4Bg.png?q=20" width="700" height="290" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="290" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*3IdCKSzR5R0lIE1LSmN4Bg.png" width="700" height="290" srcSet="https://miro.medium.com/max/552/1*3IdCKSzR5R0lIE1LSmN4Bg.png 276w, https://miro.medium.com/max/1104/1*3IdCKSzR5R0lIE1LSmN4Bg.png 552w, https://miro.medium.com/max/1280/1*3IdCKSzR5R0lIE1LSmN4Bg.png 640w, https://miro.medium.com/max/1400/1*3IdCKSzR5R0lIE1LSmN4Bg.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><a href="https://arxiv.org/pdf/1612.08242.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">Source</a></figcaption></figure><p id="1739" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Accuracy comparison for different detectors:</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div class="fd fe nq"><div class="jc s ap jk"><div class="nr je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*NJj17Z6FgffYaA4WH2WIjw.png?q=20" width="462" height="325" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="462" height="325" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/924/1*NJj17Z6FgffYaA4WH2WIjw.png" width="462" height="325" srcSet="https://miro.medium.com/max/552/1*NJj17Z6FgffYaA4WH2WIjw.png 276w, https://miro.medium.com/max/924/1*NJj17Z6FgffYaA4WH2WIjw.png 462w" sizes="462px" role="presentation"/></noscript></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><a href="https://arxiv.org/pdf/1612.08242.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">Source</a></figcaption></figure><h1 id="0d9a" class="kp kq fv bb da kr ks hp kt ku kv hs kw kx ky kz la lb lc ld le lf lg lh li lj gs">Speed improvement</h1><p id="89bb" class="hl hm fv hn b ho lv hp hq hr lw hs ht hu lx hv hw hx ly hy hz ia lz ib ic ie dn gs"><strong class="hn fw">GoogLeNet</strong></p><p id="34c3" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">VGG16 requires 30.69 billion floating point operations for a single pass over a 224 × 224 image versus 8.52 billion operations for a customized GoogLeNet. We can replace the VGG16 with the customized GoogLeNet. However, YOLO pays a price on the top-5 accuracy for ImageNet: accuracy drops from 90.0% to 88.0%.</p><p id="4eea" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">DarkNet</strong></p><p id="e314" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">We can further simplify the backbone CNN used. Darknet requires 5.58 billion operations only. With DarkNet, YOLO achieves 72.9% top-1 accuracy and 91.2% top-5 accuracy on ImageNet. Darknet uses mostly 3 × 3 filters to extract features and 1 × 1 filters to reduce output channels. It also uses global average pooling to make predictions. Here is the detail network description:</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div class="fd fe ns"><div class="jc s ap jk"><div class="nt je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/46/1*8FiQUakp9i4MneU4VXk4Ww.png?q=20" width="464" height="598" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="464" height="598" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/928/1*8FiQUakp9i4MneU4VXk4Ww.png" width="464" height="598" srcSet="https://miro.medium.com/max/552/1*8FiQUakp9i4MneU4VXk4Ww.png 276w, https://miro.medium.com/max/928/1*8FiQUakp9i4MneU4VXk4Ww.png 464w" sizes="464px" role="presentation"/></noscript></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">Modified from <a href="https://arxiv.org/pdf/1612.08242.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">source</a></figcaption></figure><p id="d0d9" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">We replace the last convolution layer (the cross-out section) with three 3 × 3 convolutional layers each outputting 1024 output channels. Then we apply a final 1 × 1 convolutional layer to convert the 7 × 7 × 1024 output into 7 × 7 × 125. (5 boundary boxes each with 4 parameters for the box, 1 objectness score and 20 conditional class probabilities)</p></div></div><div class="im"><div class="n p"><div class="lk ll lm ln lo lp at lq au lr aw w"><figure class="ih ii ij ik il im lt lu paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe nu"><img alt="" class="w ir is" src="https://miro.medium.com/max/2000/1*NBnDpz8fitkhcdnkgF2bvg.png" width="1000" height="118" role="presentation"/></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">YOLO with DarkNet</figcaption></figure></div></div></div><div class="n p"><div class="aq ar as at au ft aw w"><h1 id="7726" class="kp kq fv bb da kr ks hp kt ku kv hs kw kx ky kz la lb lc ld le lf lg lh li lj gs">Training</h1><p id="9a43" class="hl hm fv hn b ho lv hp hq hr lw hs ht hu lx hv hw hx ly hy hz ia lz ib ic ie dn gs">YOLO is trained with the ImageNet 1000 class classification dataset in 160 epochs: using stochastic gradient descent with a starting learning rate of 0.1, polynomial rate decay with a power of 4, weight decay of 0.0005 and momentum of 0.9. In the initial training, YOLO uses 224 × 224 images, and then retune it with 448× 448 images for 10 epochs at a 10−3 learning rate. After the training, the classifier achieves a top-1 accuracy of 76.5% and a top-5 accuracy of 93.3%.</p><p id="406c" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Then the fully connected layers and the last convolution layer is removed for a detector. YOLO adds three 3 × 3 convolutional layers with 1024 filters each followed by a final 1 × 1 convolutional layer with 125 output channels. (5 box predictions each with 25 parameters) YOLO also add a passthrough layer. YOLO trains the network for 160 epochs with a starting learning rate of 10−3 , dividing it by 10 at 60 and 90 epochs. YOLO uses a weight decay of 0.0005 and momentum of 0.9.</p><h1 id="c14b" class="kp kq fv bb da kr ks hp kt ku kv hs kw kx ky kz la lb lc ld le lf lg lh li lj gs"><strong class="ce">Classification</strong></h1><p id="9aa2" class="hl hm fv hn b ho lv hp hq hr lw hs ht hu lx hv hw hx ly hy hz ia lz ib ic ie dn gs">Datasets for object detection have far fewer class categories than those for classification. To expand the classes that YOLO can detect, YOLO proposes a method to mix images from both detection and classification datasets during training. It trains the end-to-end network with the object detection samples while backpropagates the classification loss from the classification samples to train the classifier path. This approach encounters a few challenges:</p><ul class=""><li id="a98e" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie jw jx jy gs">How do we merge class labels from different datasets? In particular, object detection datasets and different classification datasets uses different labels.</li><li id="0d6b" class="hl hm fv hn b ho jz hp hq hr ka hs ht hu kb hv hw hx kc hy hz ia kd ib ic ie jw jx jy gs">Any merged labels may not be mutually exclusive, for example, <em class="kg">Norfolk terrier</em> in ImageNet and <em class="kg">dog </em>in COCO. Since it is not mutually exclusive, we can not use softmax to compute the probability.</li></ul><p id="ed9e" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Hierarchical classification</strong></p><p id="92f9" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Without going into details, YOLO combines labels in different datasets to form a tree-like structure <strong class="hn fw">WordTree</strong>. The children form an is-a relationship with its parent like biplane is a plane. But the merged labels are now not mutually exclusive.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe nv"><div class="jc s ap jk"><div class="nw je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/58/1*QKwSclDLcT8eJxyj3CVL0w.png?q=20" width="700" height="721" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="721" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*QKwSclDLcT8eJxyj3CVL0w.png" width="700" height="721" srcSet="https://miro.medium.com/max/552/1*QKwSclDLcT8eJxyj3CVL0w.png 276w, https://miro.medium.com/max/1104/1*QKwSclDLcT8eJxyj3CVL0w.png 552w, https://miro.medium.com/max/1280/1*QKwSclDLcT8eJxyj3CVL0w.png 640w, https://miro.medium.com/max/1400/1*QKwSclDLcT8eJxyj3CVL0w.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz">Combining COCO and ImageNet labels to a hierarchical WordTree (<a href="https://arxiv.org/pdf/1612.08242.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">source</a>)</figcaption></figure><p id="3bec" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Let’s simplify the discussion using the 1000 class ImageNet. Instead of predicting 1000 labels in a flat structure, we create the corresponding WordTree which has 1000 leave nodes for the original labels and 369 nodes for their parent classes. Originally, YOLO predicts the class score for the biplane. But with the WordTree, it now predicts the score for the biplane given it is an airplane.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe nx"><div class="jc s ap jk"><div class="ny je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*TALo1-LuWbF80RTCM7bszw.png?q=20" width="700" height="27" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="27" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*TALo1-LuWbF80RTCM7bszw.png" width="700" height="27" srcSet="https://miro.medium.com/max/552/1*TALo1-LuWbF80RTCM7bszw.png 276w, https://miro.medium.com/max/1104/1*TALo1-LuWbF80RTCM7bszw.png 552w, https://miro.medium.com/max/1280/1*TALo1-LuWbF80RTCM7bszw.png 640w, https://miro.medium.com/max/1400/1*TALo1-LuWbF80RTCM7bszw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="2381" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Since</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe nz"><div class="jc s ap jk"><div class="oa je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*0Oyd6FWXo7OJzzuXuXSoDQ.png?q=20" width="700" height="41" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="41" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*0Oyd6FWXo7OJzzuXuXSoDQ.png" width="700" height="41" srcSet="https://miro.medium.com/max/552/1*0Oyd6FWXo7OJzzuXuXSoDQ.png 276w, https://miro.medium.com/max/1104/1*0Oyd6FWXo7OJzzuXuXSoDQ.png 552w, https://miro.medium.com/max/1280/1*0Oyd6FWXo7OJzzuXuXSoDQ.png 640w, https://miro.medium.com/max/1400/1*0Oyd6FWXo7OJzzuXuXSoDQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="132d" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">we can apply a softmax function to compute the probability</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe ob"><div class="jc s ap jk"><div class="oc je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*_gX42bmgvyib4lyR9GQrAg.png?q=20" width="700" height="32" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="32" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*_gX42bmgvyib4lyR9GQrAg.png" width="700" height="32" srcSet="https://miro.medium.com/max/552/1*_gX42bmgvyib4lyR9GQrAg.png 276w, https://miro.medium.com/max/1104/1*_gX42bmgvyib4lyR9GQrAg.png 552w, https://miro.medium.com/max/1280/1*_gX42bmgvyib4lyR9GQrAg.png 640w, https://miro.medium.com/max/1400/1*_gX42bmgvyib4lyR9GQrAg.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="83de" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">from the scores of its own and the siblings. The difference is, instead of one softmax operations, YOLO performs multiple softmax operations for each parent’s children.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div class="fd fe od"><div class="jc s ap jk"><div class="oe je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*_0ijX5oRZdQCNlSk-gXj3Q.jpeg?q=20" width="386" height="329" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="386" height="329" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/772/1*_0ijX5oRZdQCNlSk-gXj3Q.jpeg" width="386" height="329" srcSet="https://miro.medium.com/max/552/1*_0ijX5oRZdQCNlSk-gXj3Q.jpeg 276w, https://miro.medium.com/max/772/1*_0ijX5oRZdQCNlSk-gXj3Q.jpeg 386w" sizes="386px" role="presentation"/></noscript></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><a href="https://arxiv.org/pdf/1612.08242.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">source</a></figcaption></figure><p id="c923" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">The class probability is then computed from the YOLO predictions by going up the WordTree.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe of"><div class="jc s ap jk"><div class="og je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*0XHKAh0GxpW22y1DlL4hYA.png?q=20" width="700" height="106" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="106" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*0XHKAh0GxpW22y1DlL4hYA.png" width="700" height="106" srcSet="https://miro.medium.com/max/552/1*0XHKAh0GxpW22y1DlL4hYA.png 276w, https://miro.medium.com/max/1104/1*0XHKAh0GxpW22y1DlL4hYA.png 552w, https://miro.medium.com/max/1280/1*0XHKAh0GxpW22y1DlL4hYA.png 640w, https://miro.medium.com/max/1400/1*0XHKAh0GxpW22y1DlL4hYA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="bc6c" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">For classification, we assume an object is already detected and therefore Pr(physical object)=1.</p><p id="5c22" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">One benefit of the hierarchy classification is that when YOLO cannot distinguish the type of airplane, it gives a high score to the airplane instead of forcing it into one of the sub-categories.</p><p id="b4cc" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">When YOLO sees a classification image, it only backpropagates classification loss to train the classifier. YOLO finds the bounding box that predicts the highest probability for that class and it computes the classification loss as well as those from the parents. (If an object is labeled as a biplane, it is also considered to be labeled as airplane, air, vehicle… ) This encourages the model to extract features common to them. So even we have never trained a specific class of objects for object detection, we can still make such predictions by generalizing predictions from related objects.</p><p id="0011" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">In object detection, we set Pr(physical object) equals to the box confidence score which measures whether the box has an object. YOLO traverses down the tree, taking the highest confidence path at every split until it reaches some threshold and YOLO predicts that object class.</p><p id="9465" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">YOLO9000</strong></p><p id="c79d" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">YOLO9000 extends YOLO to detect objects over 9000 classes using hierarchical classification with a 9418 node WordTree. It combines samples from COCO and the top 9000 classes from the ImageNet. YOLO samples four ImageNet data for every COCO data. It learns to find objects using the detection data in COCO and to classify these objects with ImageNet samples.</p><p id="4b01" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">During the evaluation, YOLO test images on categories that it knows how to classify but not trained directly to locate them, i.e. categories that do not exist in COCO. YOLO9000 evaluates its result from the ImageNet object detection dataset which has 200 categories. It shares about 44 categories with COCO. Therefore, the dataset contains 156 categories that have never been trained directly on how to locate them. YOLO extracts similar features for related object types. Hence, we can detect those 156 categories by simply from the feature values.</p><p id="69d2" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">YOLO9000 gets 19.7 mAP overall with 16.0 mAP on those 156 categories. YOLO9000 performs well with new species of animals not found in COCO because their shapes can be generalized easily from their parent classes. However, COCO does not have bounding box labels for any type of clothing so the test struggles with categories like “sunglasses”.</p><h1 id="b8ad" class="kp kq fv bb da kr ks hp kt ku kv hs kw kx ky kz la lb lc ld le lf lg lh li lj gs">YOLOv3</h1><figure class="ih ii ij ik il im"><div class="jc s ap"><div class="jd je s"></div></div></figure><p id="9fad" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">A quote from the YOLO web site on YOLOv3:</p><blockquote class="mt mu mv"><p id="974f" class="hl hm kg hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev.</p></blockquote><p id="667f" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Class Prediction</strong></p><p id="7f2d" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">Most classifiers assume output labels are mutually exclusive. It is true if the output are mutually exclusive object classes. Therefore, YOLO applies a softmax function to convert scores into probabilities that sum up to one. YOLOv3 uses multi-label classification. For example, the output labels may be “pedestrian” and “child” which are not non-exclusive. (the sum of output can be greater than 1 now.) YOLOv3 replaces the softmax function with independent logistic classifiers to calculate the likeliness of the input belongs to a specific label. Instead of using mean square error in calculating the classification loss, YOLOv3 uses binary cross-entropy loss for each label. This also reduces the computation complexity by avoiding the softmax function.</p><p id="c898" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Bounding box prediction &amp; cost function calculation</strong></p><p id="373a" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">YOLOv3 predicts an objectness score for each bounding box using logistic regression. YOLOv3 changes the way in calculating the cost function. If the bounding box prior (anchor) overlaps a ground truth object more than others, the corresponding objectness score should be 1. For other priors with overlap greater than a predefined threshold (default 0.5), they incur no cost. Each ground truth object is associated with one boundary box prior only. If a bounding box prior is not assigned, it incurs no classification and localization lost, just confidence loss on objectness. We use tx and ty (instead of bx and by) to compute the loss.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe oh"><div class="jc s ap jk"><div class="oi je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*UuIx6sH39tefFMxgWAaaNQ.png?q=20" width="700" height="122" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="700" height="122" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/1400/1*UuIx6sH39tefFMxgWAaaNQ.png" width="700" height="122" srcSet="https://miro.medium.com/max/552/1*UuIx6sH39tefFMxgWAaaNQ.png 276w, https://miro.medium.com/max/1104/1*UuIx6sH39tefFMxgWAaaNQ.png 552w, https://miro.medium.com/max/1280/1*UuIx6sH39tefFMxgWAaaNQ.png 640w, https://miro.medium.com/max/1400/1*UuIx6sH39tefFMxgWAaaNQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="6b02" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Feature Pyramid Networks (FPN) like Feature Pyramid</strong></p><p id="b55f" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">YOLOv3 makes 3 predictions per location. Each prediction composes of a boundary box, a objectness and 80 class scores, i.e. N × N × [3 × (4 + 1 + 80) ] predictions.</p><p id="c2ae" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">YOLOv3 makes predictions at 3 different scales (similar to the FPN):</p><ol class=""><li id="403a" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie mk jx jy gs">In the last feature map layer.</li><li id="6ecf" class="hl hm fv hn b ho jz hp hq hr ka hs ht hu kb hv hw hx kc hy hz ia kd ib ic ie mk jx jy gs">Then it goes back 2 layers back and upsamples it by 2. YOLOv3 then takes a feature map with higher resolution and merge it with the upsampled feature map using element-wise addition. YOLOv3 apply convolutional filters on the merged map to make the second set of predictions.</li><li id="7b3e" class="hl hm fv hn b ho jz hp hq hr ka hs ht hu kb hv hw hx kc hy hz ia kd ib ic ie mk jx jy gs">Repeat 2 again so the resulted feature map layer has good high-level structure (semantic) information and good resolution spatial information on object locations.</li></ol><p id="ecae" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">To determine the priors, YOLOv3 applies k-means cluster. Then it pre-select 9 clusters. For COCO, the width and height of the anchors are (10×13),(16×30),(33×23),(30×61),(62×45),(59× 119),(116 × 90),(156 × 198),(373 × 326). These 9 priors are grouped into 3 different groups according to their scale. Each group is assigned to a specific feature map above in detecting objects.</p><p id="4161" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">Feature extractor</strong></p><p id="f03d" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">A new 53-layer Darknet-53 is used to replace the Darknet-19 as the feature extractor. Darknet-53 mainly compose of 3 × 3 and 1× 1 filters with skip connections like the residual network in ResNet. Darknet-53 has less BFLOP (billion floating point operations) than ResNet-152, but achieves the same classification accuracy at 2x faster.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div class="fd fe oj"><div class="jc s ap jk"><div class="ok je s"><div class="ep jg ef es eo ex w jh ji jj"><img alt="" class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/44/1*biRYJyCSv-UTbTQTa4Afqg.png?q=20" width="334" height="462" role="presentation"/></div><img alt="" class="ep jg ef es eo ex w c" width="334" height="462" role="presentation"/><noscript><img alt="" class="ef es eo ex w" src="https://miro.medium.com/max/668/1*biRYJyCSv-UTbTQTa4Afqg.png" width="334" height="462" srcSet="https://miro.medium.com/max/552/1*biRYJyCSv-UTbTQTa4Afqg.png 276w, https://miro.medium.com/max/668/1*biRYJyCSv-UTbTQTa4Afqg.png 334w" sizes="334px" role="presentation"/></noscript></div></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">Darknet-53</a></figcaption></figure><p id="def0" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><strong class="hn fw">YOLOv3 performance</strong></p><p id="20af" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">YOLOv3&#x27;s COCO AP metric is on par with SSD but 3x faster. But YOLOv3’s AP is still behind RetinaNet. In particular, AP@IoU=.75 drops significantly comparing with RetinaNet which suggests YOLOv3 has higher localization error. YOLOv3 also shows significant improvement in detecting small objects.</p></div></div><div class="im"><div class="n p"><div class="lk ll lm ln lo lp at lq au lr aw w"><figure class="ih ii ij ik il im lt lu paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe av"><img alt="" class="w ir is" src="https://miro.medium.com/max/2000/1*bFMwN__ZgfvVRYuo4o8EEg.png" width="1000" height="283" role="presentation"/></div></div></figure></div></div></div><div class="n p"><div class="aq ar as at au ft aw w"><p id="148d" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs">YOLOv3 performs very well in the fast detector category when speed is important.</p><figure class="ih ii ij ik il im fd fe paragraph-image"><div role="button" tabindex="0" class="in io ap ip w iq"><div class="fd fe ol"><img alt="" class="w ir is" src="https://miro.medium.com/max/1400/1*RFpjH8D6TStBaYuZYehe_g.png" width="700" height="437" role="presentation"/></div></div><figcaption class="it iu ff fd fe iv iw bb b bc bd bz"><a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">Source</a></figcaption></figure><h1 id="3054" class="kp kq fv bb da kr ks hp kt ku kv hs kw kx ky kz la lb lc ld le lf lg lh li lj gs">Resources</h1><p id="e094" class="hl hm fv hn b ho lv hp hq hr lw hs ht hu lx hv hw hx ly hy hz ia lz ib ic ie dn gs"><a href="https://arxiv.org/pdf/1506.02640.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">The original paper on YOLO.</a></p><p id="7b35" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><a href="https://arxiv.org/pdf/1612.08242.pdf" class="dy if" target="_blank" rel="noopener ugc nofollow">Paper on YOLOv2 and YOLO9000.</a></p><p id="fac5" class="hl hm fv hn b ho ix hp hq hr iy hs ht hu iz hv hw hx ja hy hz ia jb ib ic ie dn gs"><a href="https://github.com/pjreddie/darknet" class="dy if" target="_blank" rel="noopener ugc nofollow">DarkNet implementation.</a></p></div></div></section></div></article><div class="ep fp eq ot w ou es or ov" data-test-id="post-sidebar"><div class="n p"><div class="aq ar as at au av aw w"><div class="ow n ak"><div class="fp"><div><div class="ox s"><div class="lt s"><a class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed" rel="noopener follow" href="/?source=post_sidebar--------------------------post_sidebar-----------"><h2 class="bb da dt bd dl gs dn">Jonathan Hui</h2></a></div><div class="oy s"><p class="bb b bc bd bz">Deep Learning</p></div><div class="oz n"><span><button class="bb b bc bd pa bf pb pc bi pd bl bm bn bo pe br bs bt bu bv bw bx">Follow</button></span><div class="gz s"><div><div><div class="bw" role="tooltip" aria-hidden="false"><div class="s"><span><a href="https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff3f2f25141ae&amp;operation=register&amp;redirect=https%3A%2F%2Fjonathan-hui.medium.com%2Freal-time-object-detection-with-yolo-yolov2-28b1b93e2088&amp;newsletterV3=bd51f1a63813&amp;newsletterV3Id=f3f2f25141ae&amp;user=Jonathan%20Hui&amp;userId=bd51f1a63813&amp;source=post_sidebar-----28b1b93e2088---------------------subscribe_user-----------" class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed" rel="noopener follow"><button class="bb b bc bd pi cf pj pk pl pm pn po pp pq bn bo pe br bs bt bu bv bw bx" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="pf pg ph"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25" stroke-width="0.5"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)" stroke-width="0.5"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5" stroke-linecap="round"></path><path d="M11.5 14.5L19 20l4-3" stroke-linecap="round"></path></svg></button></a></span></div></div></div></div></div></div></div><div class="pr ps w n o ay pt"><div class="pu n"><div class="n o ay"><div class="s ap pv pw px py pz"><span><a href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F28b1b93e2088&amp;operation=register&amp;redirect=https%3A%2F%2Fjonathan-hui.medium.com%2Freal-time-object-detection-with-yolo-yolov2-28b1b93e2088&amp;user=Jonathan%20Hui&amp;userId=bd51f1a63813&amp;source=post_sidebar-----28b1b93e2088---------------------clap_sidebar-----------" class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed" rel="noopener follow"><div class="cf qa qb qc ek qd qe qf r qg qh"><svg width="29" height="29" aria-label="clap"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></span></div><div class="s qi qj qk ql qm qn qo"><div class="qp"><p class="bb b bc bd bz"><button class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed">8.7K<!-- --> </button></p></div></div></div></div><div class="qq pu s"><div class="n"><button class="ek qb cf"><div class="n o ay"><div class="n o"><svg width="25" height="25" aria-label="responses" class="qr qs ek qh"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg><p class="bb b bc bd bz"><span class="qt qs">77</span></p></div></div></button></div></div><div class="qu s"></div></div></div></div></div></div></div></div><div class="ep fp om eq on oo op oq or os"></div><div><div class="qv im n ak p"><div class="n p"><div class="aq ar as at au ft aw w"><div class="n cu"></div><div class="n o cu"></div><div class="qw qx s"><div class="n ck hj"><div class="n o ay"><div class="qy s"><span class="s qz ra rb e d"><div class="n o ay"><div class="s ap pv pw px py pz"><span><a href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F28b1b93e2088&amp;operation=register&amp;redirect=https%3A%2F%2Fjonathan-hui.medium.com%2Freal-time-object-detection-with-yolo-yolov2-28b1b93e2088&amp;user=Jonathan%20Hui&amp;userId=bd51f1a63813&amp;source=post_actions_footer-----28b1b93e2088---------------------clap_footer-----------" class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed" rel="noopener follow"><div class="cf qa qb qc ek qd qe qf r qg qh"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></span></div><div class="s qi qj qk ql rc rd re"><div class="ap rf qp"><p class="bb b bc bd gs"><button class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed">8.7K<span class="s h g f rg rh"> </span></button><span class="s h g f rg rh"></span></p></div></div></div></span><span class="s h g f rg rh"><div class="n o ay"><div class="s ap pv pw px py pz"><span><a href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fp%2F28b1b93e2088&amp;operation=register&amp;redirect=https%3A%2F%2Fjonathan-hui.medium.com%2Freal-time-object-detection-with-yolo-yolov2-28b1b93e2088&amp;user=Jonathan%20Hui&amp;userId=bd51f1a63813&amp;source=post_actions_footer-----28b1b93e2088---------------------clap_footer-----------" class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed" rel="noopener follow"><div class="cf qa qb qc ek qd qe qf r qg qh"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></div></a></span></div><div class="s qi qj qk ql rc rd re"><div class="qp"><p class="bb b bc bd gs"><button class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed">8.7K<!-- --> </button></p></div></div></div></span></div><div class="ri n"><div class="n"><button class="ek qb cf"><div class="n o ay"><div class="n o"><svg width="29" height="29" aria-label="responses" class="qr qs ek qh rj"><path d="M21.27 20.06a9.04 9.04 0 0 0 2.75-6.68C24.02 8.21 19.67 4 14.1 4S4 8.21 4 13.38c0 5.18 4.53 9.39 10.1 9.39 1 0 2-.14 2.95-.41.28.25.6.49.92.7a7.46 7.46 0 0 0 4.19 1.3c.27 0 .5-.13.6-.35a.63.63 0 0 0-.05-.65 8.08 8.08 0 0 1-1.29-2.58 5.42 5.42 0 0 1-.15-.75zm-3.85 1.32l-.08-.28-.4.12a9.72 9.72 0 0 1-2.84.43c-4.96 0-9-3.71-9-8.27 0-4.55 4.04-8.26 9-8.26 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32a5.59 5.59 0 0 0 .21 1.29c.19.7.49 1.4.89 2.08a6.43 6.43 0 0 1-2.67-1.06c-.34-.22-.88-.48-1.16-.74z"></path></svg><p class="bb b bc bd gs"><span class="rk rl qs">77</span></p></div></div></button></div></div></div><div class="n o"><div class="bw" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="bw" role="tooltip" aria-hidden="false"><button class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div><div class="hk s ab"></div></div></div></div><div class="rm qx s"><ul class="cf cg"><li class="bw rn ro rp"><a href="https://medium.com/tag/machine-learning" class="bb b rq rr bz rs rt bx s pk">Machine Learning</a></li><li class="bw rn ro rp"><a href="https://medium.com/tag/deep-learning" class="bb b rq rr bz rs rt bx s pk">Deep Learning</a></li><li class="bw rn ro rp"><a href="https://medium.com/tag/computer-vision" class="bb b rq rr bz rs rt bx s pk">Computer Vision</a></li><li class="bw rn ro rp"><a href="https://medium.com/tag/artificial-intelligence" class="bb b rq rr bz rs rt bx s pk">Artificial Intelligence</a></li></ul></div><div class="ru s"></div></div></div><div><div class="n p"><div class="aq ar as at au ft aw w"></div></div><div class="s hj"><div class="rv pr s rw"><div class="n p"><div class="aq ar as at au ft aw w"><div class="n o ck"><h2 class="bb da rx ry rz kt sa sb sc kw sd se sf la sg sh si le sj sk sl li jh sm sn so sp sq gs"><a class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed" rel="noopener follow" href="/?source=follow_footer-----28b1b93e2088--------------------------------">More from Jonathan Hui</a></h2><div class="gz n"><span><button class="bb b bc bd pa bf pb pc bi pd bl bm bn bo pe br bs bt bu bv bw bx">Follow</button></span><div class="gz s"><div><div><div class="bw" role="tooltip" aria-hidden="false"><div class="s"><span><a href="https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2Ff3f2f25141ae&amp;operation=register&amp;redirect=https%3A%2F%2Fjonathan-hui.medium.com%2Freal-time-object-detection-with-yolo-yolov2-28b1b93e2088&amp;newsletterV3=bd51f1a63813&amp;newsletterV3Id=f3f2f25141ae&amp;user=Jonathan%20Hui&amp;userId=bd51f1a63813&amp;source=follow_footer-----28b1b93e2088---------------------subscribe_user-----------" class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed" rel="noopener follow"><button class="bb b bc bd pi cf pj pk pl pm pn po pp pq bn bo pe br bs bt bu bv bw bx" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="pf pg ph"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25" stroke-width="0.5"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)" stroke-width="0.5"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5" stroke-linecap="round"></path><path d="M11.5 14.5L19 20l4-3" stroke-linecap="round"></path></svg></button></a></span></div></div></div></div></div></div></div><div class="lu s"><p class="bb b bc bd bz">Deep Learning</p></div></div></div></div></div><div class="sr s rw hj"><div class="n p"><div class="lk lm lo ss st ir aw w"></div></div></div><div class="s fg hj"><div class="n p"><div class="aq ar as at au av aw w"><div class="su id s"><div class="sv ao sw id s sx sy"><h2 class="bb da sz ta kt tb tc kw td te la tf tg le th ti li gs">More From Medium</h2></div><div class="cq n ay cu tj tk tl tm tn to tp tq tr ts tt tu tv tw tx"><div class="ty tz ua ll ub uc ud ln ue uf ug uh ui uj uk ul um un uo up uq"><div class="ur us s"><div class="w ex"><div class="n ck"><div class="s ut qj ql uu"><div class="uv s"><h2 class="bb da rx ry kt sa sb kw uw ux la uy uz le va vb li gs"><a href="https://dileepkumar1422002.medium.com/k-means-clustering-d3778244ed98?source=post_internal_links---------0----------------------------" rel="noopener follow">K-Means Clustering</a></h2></div><div class="o n"><div></div><div class="w s"><div class="n"><div style="flex:1"><span class="bb b bc bd gs"><div class="cr n o vc"><span class="bb b rq bd gs"><a href="https://dileepkumar1422002.medium.com/?source=post_internal_links---------0----------------------------" class="dy dz ca cb cc cd ce cf cg bm vd ch ec ed" rel="noopener follow">Dileepkumar</a></span></div></span></div></div></div></div></div><div class="ee ro s ve vf"><a href="https://dileepkumar1422002.medium.com/k-means-clustering-d3778244ed98?source=post_internal_links---------0----------------------------" class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed s" rel="noopener follow"><div class="jc s ap jk"><div class="vg je s"><div class="ep jg ef es eo ex w jh ji jj"><img class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/0*TX_MTqiTYkxmCmGt?q=20" width="70" height="70" role="presentation"/></div><img class="ep jg vh vi vj vk vl vm vn vo vp vq c" width="70" height="70" role="presentation"/><noscript><img class="vh vi vj vk vl vm vn vo vp vq" src="https://miro.medium.com/fit/c/140/140/0*TX_MTqiTYkxmCmGt" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/0*TX_MTqiTYkxmCmGt 48w, https://miro.medium.com/fit/c/140/140/0*TX_MTqiTYkxmCmGt 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="ty tz ua ll ub uc ud ln ue uf ug uh ui uj uk ul um un uo up uq"><div class="ur us s"><div class="w ex"><div class="n ck"><div class="s ut qj ql uu"><div class="uv s"><h2 class="bb da rx ry kt sa sb kw uw ux la uy uz le va vb li gs"><a href="https://kargarisaac.medium.com/paper-review-learning-hand-eye-coordination-for-robotic-grasping-with-large-scale-data-collection-a858d69d83b8?source=post_internal_links---------1----------------------------" rel="noopener follow">Paper Review: Learning Hand-Eye Coordination for Robotic Grasping with Large-Scale Data Collection</a></h2></div><div class="o n"><div></div><div class="w s"><div class="n"><div style="flex:1"><span class="bb b bc bd gs"><div class="cr n o vc"><span class="bb b rq bd gs"><a href="https://kargarisaac.medium.com/?source=post_internal_links---------1----------------------------" class="dy dz ca cb cc cd ce cf cg bm vd ch ec ed" rel="noopener follow">Isaac Kargar</a></span></div></span></div></div></div></div></div><div class="ee ro s ve vf"><a href="https://kargarisaac.medium.com/paper-review-learning-hand-eye-coordination-for-robotic-grasping-with-large-scale-data-collection-a858d69d83b8?source=post_internal_links---------1----------------------------" class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed s" rel="noopener follow"><div class="jc s ap jk"><div class="vg je s"><div class="ep jg ef es eo ex w jh ji jj"><img class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*WngUhHTS8MD96VLwNKrPSw.png?q=20" width="70" height="70" role="presentation"/></div><img class="ep jg vh vi vj vk vl vm vn vo vp vq c" width="70" height="70" role="presentation"/><noscript><img class="vh vi vj vk vl vm vn vo vp vq" src="https://miro.medium.com/fit/c/140/140/1*WngUhHTS8MD96VLwNKrPSw.png" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/1*WngUhHTS8MD96VLwNKrPSw.png 48w, https://miro.medium.com/fit/c/140/140/1*WngUhHTS8MD96VLwNKrPSw.png 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="ty tz ua ll ub uc ud ln ue uf ug uh ui uj uk ul um un uo up uq"><div class="ur us s"><div class="w ex"><div class="n ck"><div class="s ut qj ql uu"><div class="uv s"><h2 class="bb da rx ry kt sa sb kw uw ux la uy uz le va vb li gs"><a href="https://becominghuman.ai/classifying-traffic-signs-728744d3deac?source=post_internal_links---------2----------------------------" rel="noopener follow">Classifying Traffic Signs</a></h2></div><div class="o n"><div></div><div class="w s"><div class="n"><div style="flex:1"><span class="bb b bc bd gs"><div class="cr n o vc"><span class="bb b rq bd gs"><a href="https://davideclark314.medium.com/?source=post_internal_links---------2----------------------------" class="dy dz ca cb cc cd ce cf cg bm vd ch ec ed" rel="noopener follow">David Clark</a><span> <!-- -->in<!-- --> <a href="https://becominghuman.ai/?source=post_internal_links---------2----------------------------" class="dy dz ca cb cc cd ce cf cg bm vd ch ec ed" rel="noopener follow">Becoming Human: Artificial Intelligence Magazine</a></span></span></div></span></div></div></div></div></div><div class="ee ro s ve vf"><a href="https://becominghuman.ai/classifying-traffic-signs-728744d3deac?source=post_internal_links---------2----------------------------" class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed s" rel="noopener follow"><div class="jc s ap jk"><div class="vg je s"><div class="ep jg ef es eo ex w jh ji jj"><img class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*jFxVEbTPq2NSGl0QKBszdQ.png?q=20" width="70" height="70" role="presentation"/></div><img class="ep jg vh vi vj vk vl vm vn vo vp vq c" width="70" height="70" role="presentation"/><noscript><img class="vh vi vj vk vl vm vn vo vp vq" src="https://miro.medium.com/fit/c/140/140/1*jFxVEbTPq2NSGl0QKBszdQ.png" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/1*jFxVEbTPq2NSGl0QKBszdQ.png 48w, https://miro.medium.com/fit/c/140/140/1*jFxVEbTPq2NSGl0QKBszdQ.png 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="ty tz ua ll ub uc ud ln ue uf ug uh ui uj uk ul um un uo up uq"><div class="ur us s"><div class="w ex"><div class="n ck"><div class="s ut qj ql uu"><div class="uv s"><h2 class="bb da rx ry kt sa sb kw uw ux la uy uz le va vb li gs"><a rel="noopener follow" href="/tensorflow-sequence-to-sequence-model-examples-232bf6acd15f?source=post_internal_links---------3----------------------------">TensorFlow Sequence to Sequence Model Examples</a></h2></div><div class="o n"><div></div><div class="w s"><div class="n"><div style="flex:1"><span class="bb b bc bd gs"><div class="cr n o vc"><span class="bb b rq bd gs"><a class="dy dz ca cb cc cd ce cf cg bm vd ch ec ed" rel="noopener follow" href="/?source=post_internal_links---------3----------------------------">Jonathan Hui</a></span></div></span></div></div></div></div></div><div class="ee ro s ve vf"><a class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed s" rel="noopener follow" href="/tensorflow-sequence-to-sequence-model-examples-232bf6acd15f?source=post_internal_links---------3----------------------------"><div class="jc s ap jk"><div class="vg je s"><div class="ep jg ef es eo ex w jh ji jj"><img class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*Pw1Eeiu1bDCX0u-aBJ7U_Q.jpeg?q=20" width="70" height="70" role="presentation"/></div><img class="ep jg vh vi vj vk vl vm vn vo vp vq c" width="70" height="70" role="presentation"/><noscript><img class="vh vi vj vk vl vm vn vo vp vq" src="https://miro.medium.com/fit/c/140/140/1*Pw1Eeiu1bDCX0u-aBJ7U_Q.jpeg" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/1*Pw1Eeiu1bDCX0u-aBJ7U_Q.jpeg 48w, https://miro.medium.com/fit/c/140/140/1*Pw1Eeiu1bDCX0u-aBJ7U_Q.jpeg 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="ty tz ua ll ub uc ud ln ue uf ug uh ui uj uk ul um un uo up uq"><div class="ur us s"><div class="w ex"><div class="n ck"><div class="s ut qj ql uu"><div class="uv s"><h2 class="bb da rx ry kt sa sb kw uw ux la uy uz le va vb li gs"><a rel="noopener follow" href="/tensorflow-keras-53f190a37704?source=post_internal_links---------4----------------------------">TensorFlow &amp; Keras</a></h2></div><div class="o n"><div></div><div class="w s"><div class="n"><div style="flex:1"><span class="bb b bc bd gs"><div class="cr n o vc"><span class="bb b rq bd gs"><a class="dy dz ca cb cc cd ce cf cg bm vd ch ec ed" rel="noopener follow" href="/?source=post_internal_links---------4----------------------------">Jonathan Hui</a></span></div></span></div></div></div></div></div><div class="ee ro s ve vf"><a class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed s" rel="noopener follow" href="/tensorflow-keras-53f190a37704?source=post_internal_links---------4----------------------------"><div class="jc s ap jk"><div class="vg je s"><div class="ep jg ef es eo ex w jh ji jj"><img class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*6k7tbgyc1Qb2yCtqCzKCAg.jpeg?q=20" width="70" height="70" role="presentation"/></div><img class="ep jg vh vi vj vk vl vm vn vo vp vq c" width="70" height="70" role="presentation"/><noscript><img class="vh vi vj vk vl vm vn vo vp vq" src="https://miro.medium.com/fit/c/140/140/1*6k7tbgyc1Qb2yCtqCzKCAg.jpeg" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/1*6k7tbgyc1Qb2yCtqCzKCAg.jpeg 48w, https://miro.medium.com/fit/c/140/140/1*6k7tbgyc1Qb2yCtqCzKCAg.jpeg 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="ty tz ua ll ub uc ud ln ue uf ug uh ui uj uk ul um un uo up uq"><div class="ur us s"><div class="w ex"><div class="n ck"><div class="s ut qj ql uu"><div class="uv s"><h2 class="bb da rx ry kt sa sb kw uw ux la uy uz le va vb li gs"><a href="https://medium.com/clipme/clipme-meme-clip-generation-a9d94cbdc83d?source=post_internal_links---------5----------------------------" rel="noopener follow">ClipMe: Meme Clip Generation</a></h2></div><div class="o n"><div></div><div class="w s"><div class="n"><div style="flex:1"><span class="bb b bc bd gs"><div class="cr n o vc"><span class="bb b rq bd gs"><a href="https://shivam-sardana.medium.com/?source=post_internal_links---------5----------------------------" class="dy dz ca cb cc cd ce cf cg bm vd ch ec ed" rel="noopener follow">Shivam Sardana</a><span> <!-- -->in<!-- --> <a href="https://medium.com/clipme?source=post_internal_links---------5----------------------------" class="dy dz ca cb cc cd ce cf cg bm vd ch ec ed" rel="noopener follow">ClipMe</a></span></span></div></span></div></div></div></div></div><div class="ee ro s ve vf"><a href="https://medium.com/clipme/clipme-meme-clip-generation-a9d94cbdc83d?source=post_internal_links---------5----------------------------" class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed s" rel="noopener follow"><div class="jc s ap jk"><div class="vg je s"><div class="ep jg ef es eo ex w jh ji jj"><img class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*z-rSfscdqNtsKOTpnGXCWA.png?q=20" width="70" height="70" role="presentation"/></div><img class="ep jg vh vi vj vk vl vm vn vo vp vq c" width="70" height="70" role="presentation"/><noscript><img class="vh vi vj vk vl vm vn vo vp vq" src="https://miro.medium.com/fit/c/140/140/1*z-rSfscdqNtsKOTpnGXCWA.png" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/1*z-rSfscdqNtsKOTpnGXCWA.png 48w, https://miro.medium.com/fit/c/140/140/1*z-rSfscdqNtsKOTpnGXCWA.png 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="ty tz ua ll ub uc ud ln ue uf ug uh ui uj uk ul um un uo up uq"><div class="ur us s"><div class="w ex"><div class="n ck"><div class="s ut qj ql uu"><div class="uv s"><h2 class="bb da rx ry kt sa sb kw uw ux la uy uz le va vb li gs"><a href="https://noufelefgoum.medium.com/why-neural-network-work-with-nonlinear-problem-f81262b49895?source=post_internal_links---------6----------------------------" rel="noopener follow">why neural network work with nonlinear problem</a></h2></div><div class="o n"><div></div><div class="w s"><div class="n"><div style="flex:1"><span class="bb b bc bd gs"><div class="cr n o vc"><span class="bb b rq bd gs"><a href="https://noufelefgoum.medium.com/?source=post_internal_links---------6----------------------------" class="dy dz ca cb cc cd ce cf cg bm vd ch ec ed" rel="noopener follow">raian lefgoum</a></span></div></span></div></div></div></div></div><div class="ee ro s ve vf"><a href="https://noufelefgoum.medium.com/why-neural-network-work-with-nonlinear-problem-f81262b49895?source=post_internal_links---------6----------------------------" class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed s" rel="noopener follow"><div class="jc s ap jk"><div class="vg je s"><div class="ep jg ef es eo ex w jh ji jj"><img class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/1*ZcKvYSlLGNfKSYDXr8dJyA.png?q=20" width="70" height="70" role="presentation"/></div><img class="ep jg vh vi vj vk vl vm vn vo vp vq c" width="70" height="70" role="presentation"/><noscript><img class="vh vi vj vk vl vm vn vo vp vq" src="https://miro.medium.com/fit/c/140/140/1*ZcKvYSlLGNfKSYDXr8dJyA.png" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/1*ZcKvYSlLGNfKSYDXr8dJyA.png 48w, https://miro.medium.com/fit/c/140/140/1*ZcKvYSlLGNfKSYDXr8dJyA.png 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div><div class="ty tz ua ll ub uc ud ln ue uf ug uh ui uj uk ul um un uo up uq"><div class="ur us s"><div class="w ex"><div class="n ck"><div class="s ut qj ql uu"><div class="uv s"><h2 class="bb da rx ry kt sa sb kw uw ux la uy uz le va vb li gs"><a href="https://towardsdatascience.com/data-augmentation-library-for-text-9661736b13ff?source=post_internal_links---------7----------------------------" rel="noopener follow">Data Augmentation library for text</a></h2></div><div class="o n"><div></div><div class="w s"><div class="n"><div style="flex:1"><span class="bb b bc bd gs"><div class="cr n o vc"><span class="bb b rq bd gs"><a href="https://medium.com/@makcedward?source=post_internal_links---------7----------------------------" class="dy dz ca cb cc cd ce cf cg bm vd ch ec ed" rel="noopener follow">Edward Ma</a><span> <!-- -->in<!-- --> <a href="https://towardsdatascience.com/?source=post_internal_links---------7----------------------------" class="dy dz ca cb cc cd ce cf cg bm vd ch ec ed" rel="noopener follow">Towards Data Science</a></span></span></div></span></div></div></div></div></div><div class="ee ro s ve vf"><a href="https://towardsdatascience.com/data-augmentation-library-for-text-9661736b13ff?source=post_internal_links---------7----------------------------" class="dy dz ca cb cc cd ce cf cg bm ea eb ch ec ed s" rel="noopener follow"><div class="jc s ap jk"><div class="vg je s"><div class="ep jg ef es eo ex w jh ji jj"><img class="ef es eo ex w jm jn jo" src="https://miro.medium.com/max/60/0*bB6Grog7PMawSL5_?q=20" width="70" height="70" role="presentation"/></div><img class="ep jg vh vi vj vk vl vm vn vo vp vq c" width="70" height="70" role="presentation"/><noscript><img class="vh vi vj vk vl vm vn vo vp vq" src="https://miro.medium.com/fit/c/140/140/0*bB6Grog7PMawSL5_" width="70" height="70" srcSet="https://miro.medium.com/fit/c/96/140/0*bB6Grog7PMawSL5_ 48w, https://miro.medium.com/fit/c/140/140/0*bB6Grog7PMawSL5_ 70w" sizes="70px" role="presentation"/></noscript></div></div></a></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20211008-230528-819d08c9ad"</script><script>window.__GRAPHQL_URI__ = "https://jonathan-hui.medium.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"auroraPage":{"isAuroraPageEnabled":true},"bookReader":{"assets":{},"reader":{"currentAsset":null,"currentGFI":null,"settingsPanelIsOpen":false,"settings":{"fontFamily":"CHARTER","fontScale":"M","publisherStyling":false,"textAlignment":"start","theme":"White","lineSpacing":0,"wordSpacing":0,"letterSpacing":0},"internalNavCounter":0,"currentSelection":null}},"cache":{"experimentGroupSet":true,"reason":"","group":"enabled","tags":["group-edgeCachePosts","post-28b1b93e2088","user-bd51f1a63813"],"serverVariantState":"7ad34225edb022d8b2352fe7f82583d32c70747f3c06b60decb5efad6eacf0bf","middlewareEnabled":true,"cacheStatus":"DYNAMIC","vary":[]},"client":{"hydrated":false,"isUs":false,"countryCode":"","isNativeMedium":false,"isSafariMobile":true,"isSafari":true,"routingEntity":{"type":"USER","id":"bd51f1a63813","explicit":true}},"debug":{"requestId":"e14954a7-ed5e-43fc-821a-00593ac55e9c","hybridDevServices":[],"showBookReaderDebugger":false,"originalSpanCarrier":{"ot-tracer-spanid":"659e46d658a6f729","ot-tracer-traceid":"6df03ffa78c08cdf","ot-tracer-sampled":"true"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Fjonathan-hui.medium.com\u002Freal-time-object-detection-with-yolo-yolov2-28b1b93e2088","host":"jonathan-hui.medium.com","hostname":"jonathan-hui.medium.com","referrer":"","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false,"queryString":"","currentHash":""},"tracing":{},"config":{"nodeEnv":"production","version":"main-20211008-230528-819d08c9ad","isTaggedVersion":false,"isMediumDotApp":false,"isMediumDotAppVariant":false,"target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"lightstep.medium.systems","token":"ce5be895bef60919541332990ac9fef2","appVersion":"main-20211008-230528-819d08c9ad","disableClientReporting":false},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20211008-230528-819d08c9ad","commit":"819d08c9adbfcd4adee2a787c7aaad514fa9b920"}},"datacenter":"us"},"googleAnalyticsCode":"UA-24232453-2","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium"},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl"},"session":{"xsrf":""}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","meterPost({\"postId\":\"28b1b93e2088\",\"postMeteringOptions\":{\"referrer\":\"\",\"sk\":null,\"source\":null}})":{"__ref":"MeteringInfo:{}"},"postResult({\"id\":\"28b1b93e2088\"})":{"__ref":"Post:28b1b93e2088"}},"MeteringInfo:{}":{"__typename":"MeteringInfo","postIds":[],"maxUnlockCount":3,"unlocksRemaining":0},"User:bd51f1a63813":{"id":"bd51f1a63813","__typename":"User","name":"Jonathan Hui","username":"jonathan-hui","newsletterV3":{"__ref":"NewsletterV3:f3f2f25141ae"},"customStyleSheet":null,"isSuspended":false,"bio":"Deep Learning","imageId":"1*c3Z3aOPBooxEX4tx4RkzLw.jpeg","hasCompletedProfile":false,"isAuroraVisible":true,"mediumMemberAt":0,"socialStats":{"__typename":"SocialStats","followerCount":22161,"followingCount":27,"collectionFollowingCount":9},"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"jonathan-hui.medium.com","status":"ACTIVE","isSubdomain":true}},"hasSubdomain":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:bd51f1a63813-viewerId:lo_3c97a71a5cb9"},"bookAuthor":null,"isPartnerProgramEnrolled":false,"viewerIsUser":false,"homepagePostsConnection({\"paging\":{\"limit\":1}})":{"__typename":"PostConnection","posts":[{"__ref":"Post:19821810a14"}]},"postSubscribeMembershipUpsellShownAt":0,"allowNotes":false,"replyToEmailBannerShownCount":0,"twitterScreenName":"jonathan_hui","followedCollections":10,"atsQualifiedAt":1612205429124},"UserViewerEdge:userId:bd51f1a63813-viewerId:lo_3c97a71a5cb9":{"id":"userId:bd51f1a63813-viewerId:lo_3c97a71a5cb9","__typename":"UserViewerEdge","createdAt":0,"lastPostCreatedAt":0,"isFollowing":false,"isUser":false},"NewsletterV3:f3f2f25141ae":{"id":"f3f2f25141ae","__typename":"NewsletterV3","type":"NEWSLETTER_TYPE_AUTHOR","slug":"bd51f1a63813","name":"bd51f1a63813","collection":null,"user":{"__ref":"User:bd51f1a63813"},"description":"","promoHeadline":"","promoBody":"","replyToEmail":"","showPromo":false,"subscribersCount":62},"Post:19821810a14":{"id":"19821810a14","__typename":"Post"},"Paragraph:4d50ee2de52f_0":{"id":"4d50ee2de52f_0","__typename":"Paragraph","name":"79c8","text":"Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_1":{"id":"4d50ee2de52f_1","__typename":"Paragraph","name":"1749","text":"You only look once (YOLO) is an object detection system targeted for real-time processing. We will introduce YOLO, YOLOv2 and YOLO9000 in this article. For those only interested in YOLOv3, please forward to the bottom of the article. Here is the accuracy and speed comparison provided by the YOLO web site.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":210,"end":232,"type":"A","href":"https:\u002F\u002Fmedium.com\u002F@jonathan_hui\u002Freal-time-object-detection-with-yolo-yolov2-28b1b93e2088#b8ad","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":25,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_2":{"id":"4d50ee2de52f_2","__typename":"Paragraph","name":"86c5","text":"Source","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*ju1oaoIkVUkaIPAdpehlzA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":6,"type":"A","href":"https:\u002F\u002Fpjreddie.com\u002Fdarknet\u002Fyolo\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_3":{"id":"4d50ee2de52f_3","__typename":"Paragraph","name":"e01c","text":"A demonstration from the YOLOv2.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_4":{"id":"4d50ee2de52f_4","__typename":"Paragraph","name":"da74","text":"Object detection in real-time","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:b901faec3876bb5da6091d9410fd9c72"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_5":{"id":"4d50ee2de52f_5","__typename":"Paragraph","name":"ff6b","text":"Let’s start with our own testing image below.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_6":{"id":"4d50ee2de52f_6","__typename":"Paragraph","name":"c0e3","text":"Testing image","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*EYFejGUjvjPcc4PZTwoufw.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":13,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_7":{"id":"4d50ee2de52f_7","__typename":"Paragraph","name":"acb9","text":"The objects detected by YOLO:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_8":{"id":"4d50ee2de52f_8","__typename":"Paragraph","name":"6b18","text":"Objects detected by YOLO.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*QOGcvHbrDZiCqTG6THIQ_w.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_9":{"id":"4d50ee2de52f_9","__typename":"Paragraph","name":"4bc0","text":"Grid cell","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":9,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_10":{"id":"4d50ee2de52f_10","__typename":"Paragraph","name":"65e3","text":"For our discussion, we crop our original photo. YOLO divides the input image into an S×S grid. Each grid cell predicts only one object. For example, the yellow grid cell below tries to predict the “person” object whose center (the blue dot) falls inside the grid cell.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":85,"end":86,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":87,"end":88,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":124,"end":127,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_11":{"id":"4d50ee2de52f_11","__typename":"Paragraph","name":"f32c","text":"Each grid cell detects only one object.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*6qZXYCDUkC5Bc8nRolT0Mw.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_12":{"id":"4d50ee2de52f_12","__typename":"Paragraph","name":"2220","text":"Each grid cell predicts a fixed number of boundary boxes. In this example, the yellow grid cell makes two boundary box predictions (blue boxes) to locate where the person is.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_13":{"id":"4d50ee2de52f_13","__typename":"Paragraph","name":"e9d1","text":"Each grid cell make a fixed number of boundary box guesses for the object.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*4Y1PaY3ZgxKt5w84_0pNxw.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_14":{"id":"4d50ee2de52f_14","__typename":"Paragraph","name":"779b","text":"However, the one-object rule limits how close detected objects can be. For that, YOLO does have some limitations on how close objects can be. For the picture below, there are 9 Santas in the lower left corner but YOLO can detect 5 only.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_15":{"id":"4d50ee2de52f_15","__typename":"Paragraph","name":"45e6","text":"YOLO may miss objects that are too close.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*j4PnWfxP3yoVPOFyI27tww.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_16":{"id":"4d50ee2de52f_16","__typename":"Paragraph","name":"d9cf","text":"For each grid cell,","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_17":{"id":"4d50ee2de52f_17","__typename":"Paragraph","name":"f040","text":"it predicts B boundary boxes and each box has one box confidence score,","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":12,"end":13,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":50,"end":70,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_18":{"id":"4d50ee2de52f_18","__typename":"Paragraph","name":"23b9","text":"it detects one object only regardless of the number of boxes B,","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":11,"end":14,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_19":{"id":"4d50ee2de52f_19","__typename":"Paragraph","name":"4e9c","text":"it predicts C conditional class probabilities (one per class for the likeliness of the object class).","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":12,"end":13,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":14,"end":45,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_20":{"id":"4d50ee2de52f_20","__typename":"Paragraph","name":"be90","text":"To evaluate PASCAL VOC, YOLO uses 7×7 grids (S×S), 2 boundary boxes (B) and 20 classes (C).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_21":{"id":"4d50ee2de52f_21","__typename":"Paragraph","name":"ce26","text":"YOLO makes SxS predictions with B boundary boxes.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*OuMJUWo2rXYA-GYU63NUGw.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_22":{"id":"4d50ee2de52f_22","__typename":"Paragraph","name":"5ef3","text":"Let’s get into more details. Each boundary box contains 5 elements: (x, y, w, h) and a box confidence score. The confidence score reflects how likely the box contains an object (objectness) and how accurate is the boundary box. We normalize the bounding box width w and height h by the image width and height. x and y are offsets to the corresponding cell. Hence, x, y, w and h are all between 0 and 1. Each cell has 20 conditional class probabilities. The conditional class probability is the probability that the detected object belongs to a particular class (one probability per category for each cell). So, YOLO’s prediction has a shape of (S, S, B×5 + C) = (7, 7, 2×5 + 20) = (7, 7, 30).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":87,"end":107,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":178,"end":188,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":457,"end":486,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":69,"end":79,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":264,"end":265,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":277,"end":278,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":310,"end":311,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":316,"end":317,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":364,"end":371,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":376,"end":377,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_23":{"id":"4d50ee2de52f_23","__typename":"Paragraph","name":"8ad8","text":"Source","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*jtnrhMFNwGxiQmkY6LkdCQ.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":6,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.02640.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_24":{"id":"4d50ee2de52f_24","__typename":"Paragraph","name":"03d4","text":"The major concept of YOLO is to build a CNN network to predict a (7, 7, 30) tensor. It uses a CNN network to reduce the spatial dimension to 7×7 with 1024 output channels at each location. YOLO performs a linear regression using two fully connected layers to make 7×7×2 boundary box predictions (the middle picture below). To make a final prediction, we keep those with high box confidence scores (greater than 0.25) as our final predictions (the right picture).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_25":{"id":"4d50ee2de52f_25","__typename":"Paragraph","name":"dfc8","text":"Source","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*e0VY6U1_WMF2KBoKQNZvkQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":6,"type":"A","href":"https:\u002F\u002Fpjreddie.com\u002Fdarknet\u002Fyolo\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_26":{"id":"4d50ee2de52f_26","__typename":"Paragraph","name":"4733","text":"The class confidence score for each prediction box is computed as:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":4,"end":27,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_27":{"id":"4d50ee2de52f_27","__typename":"Paragraph","name":"d818","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*qVL77IZyEnra4DvENayXUA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_28":{"id":"4d50ee2de52f_28","__typename":"Paragraph","name":"7161","text":"It measures the confidence on both the classification and the localization (where an object is located).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":62,"end":74,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_29":{"id":"4d50ee2de52f_29","__typename":"Paragraph","name":"8084","text":"We may mix up those scoring and probability terms easily. Here are the mathematical definitions for your future reference.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_30":{"id":"4d50ee2de52f_30","__typename":"Paragraph","name":"38d0","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*0IPktA65WxOBfP_ULQWcmw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_31":{"id":"4d50ee2de52f_31","__typename":"Paragraph","name":"5fe5","text":"Network design","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":14,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_32":{"id":"4d50ee2de52f_32","__typename":"Paragraph","name":"c3a9","text":"Source","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*9ER4GVUtQGVA2Y0skC9OQQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":6,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.02640.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_33":{"id":"4d50ee2de52f_33","__typename":"Paragraph","name":"91ea","text":"YOLO has 24 convolutional layers followed by 2 fully connected layers (FC). Some convolution layers use 1 × 1 reduction layers alternatively to reduce the depth of the features maps. For the last convolution layer, it outputs a tensor with shape (7, 7, 1024). The tensor is then flattened. Using 2 fully connected layers as a form of linear regression, it outputs 7×7×30 parameters and then reshapes to (7, 7, 30), i.e. 2 boundary box predictions per location.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_34":{"id":"4d50ee2de52f_34","__typename":"Paragraph","name":"6672","text":"A faster but less accurate version of YOLO, called Fast YOLO, uses only 9 convolutional layers with shallower feature maps.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_35":{"id":"4d50ee2de52f_35","__typename":"Paragraph","name":"7bde","text":"Loss function","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":13,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_36":{"id":"4d50ee2de52f_36","__typename":"Paragraph","name":"5db5","text":"YOLO predicts multiple bounding boxes per grid cell. To compute the loss for the true positive, we only want one of them to be responsible for the object. For this purpose, we select the one with the highest IoU (intersection over union) with the ground truth. This strategy leads to specialization among the bounding box predictions. Each prediction gets better at predicting certain sizes and aspect ratios.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":127,"end":138,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_37":{"id":"4d50ee2de52f_37","__typename":"Paragraph","name":"8503","text":"YOLO uses sum-squared error between the predictions and the ground truth to calculate loss. The loss function composes of:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_38":{"id":"4d50ee2de52f_38","__typename":"Paragraph","name":"d975","text":"the classification loss.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":4,"end":23,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_39":{"id":"4d50ee2de52f_39","__typename":"Paragraph","name":"ef04","text":"the localization loss (errors between the predicted boundary box and the ground truth).","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":4,"end":21,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_40":{"id":"4d50ee2de52f_40","__typename":"Paragraph","name":"718c","text":"the confidence loss (the objectness of the box).","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":4,"end":19,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_41":{"id":"4d50ee2de52f_41","__typename":"Paragraph","name":"b1d2","text":"Classification loss","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":19,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_42":{"id":"4d50ee2de52f_42","__typename":"Paragraph","name":"4291","text":"If an object is detected, the classification loss at each cell is the squared error of the class conditional probabilities for each class:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":3,"end":24,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_43":{"id":"4d50ee2de52f_43","__typename":"Paragraph","name":"b3b1","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*lF6SCAVj5jMwLxs39SCogw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_44":{"id":"4d50ee2de52f_44","__typename":"Paragraph","name":"7476","text":"Localization loss","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":17,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_45":{"id":"4d50ee2de52f_45","__typename":"Paragraph","name":"e7fa","text":"The localization loss measures the errors in the predicted boundary box locations and sizes. We only count the box responsible for detecting the object.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_46":{"id":"4d50ee2de52f_46","__typename":"Paragraph","name":"3877","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*BwhGMvffFfqtND9413oiwA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_47":{"id":"4d50ee2de52f_47","__typename":"Paragraph","name":"890d","text":"We do not want to weight absolute errors in large boxes and small boxes equally. i.e. a 2-pixel error in a large box is the same for a small box. To partially address this, YOLO predicts the square root of the bounding box width and height instead of the width and height. In addition, to put more emphasis on the boundary box accuracy, we multiply the loss by λcoord (default: 5).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":362,"end":367,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_48":{"id":"4d50ee2de52f_48","__typename":"Paragraph","name":"34aa","text":"Confidence loss","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":15,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_49":{"id":"4d50ee2de52f_49","__typename":"Paragraph","name":"d347","text":"If an object is detected in the box, the confidence loss (measuring the objectness of the box) is:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":3,"end":35,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_50":{"id":"4d50ee2de52f_50","__typename":"Paragraph","name":"1563","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*QT7mwEbyLJYIxTYtOWClFQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_51":{"id":"4d50ee2de52f_51","__typename":"Paragraph","name":"6918","text":"If an object is not detected in the box, the confidence loss is:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":3,"end":39,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_52":{"id":"4d50ee2de52f_52","__typename":"Paragraph","name":"74ef","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*Yc_OJIXOoV2WaGQ6PqhTXA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_53":{"id":"4d50ee2de52f_53","__typename":"Paragraph","name":"ca3e","text":"Most boxes do not contain any objects. This causes a class imbalance problem, i.e. we train the model to detect background more frequently than detecting objects. To remedy this, we weight this loss down by a factor λnoobj (default: 0.5).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":217,"end":222,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_54":{"id":"4d50ee2de52f_54","__typename":"Paragraph","name":"e037","text":"Loss","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":4,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_55":{"id":"4d50ee2de52f_55","__typename":"Paragraph","name":"a20c","text":"The final loss adds localization, confidence and classification losses together.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_56":{"id":"4d50ee2de52f_56","__typename":"Paragraph","name":"08f1","text":"Source","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*aW6htqx4Q7APLrSQg2eWDw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":6,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.02640.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_57":{"id":"4d50ee2de52f_57","__typename":"Paragraph","name":"7519","text":"Inference: Non-maximal suppression","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_58":{"id":"4d50ee2de52f_58","__typename":"Paragraph","name":"9c17","text":"YOLO can make duplicate detections for the same object. To fix this, YOLO applies non-maximal suppression to remove duplications with lower confidence. Non-maximal suppression adds 2- 3% in mAP.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_59":{"id":"4d50ee2de52f_59","__typename":"Paragraph","name":"a146","text":"Here is one of the possible non-maximal suppression implementation:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_60":{"id":"4d50ee2de52f_60","__typename":"Paragraph","name":"9fab","text":"Sort the predictions by the confidence scores.","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_61":{"id":"4d50ee2de52f_61","__typename":"Paragraph","name":"7185","text":"Start from the top scores, ignore any current prediction if we find any previous predictions that have the same class and IoU \u003E 0.5 with the current prediction.","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_62":{"id":"4d50ee2de52f_62","__typename":"Paragraph","name":"691a","text":"Repeat step 2 until all predictions are checked.","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_63":{"id":"4d50ee2de52f_63","__typename":"Paragraph","name":"73bf","text":"Benefits of YOLO","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":16,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_64":{"id":"4d50ee2de52f_64","__typename":"Paragraph","name":"49ce","text":"Fast. Good for real-time processing.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_65":{"id":"4d50ee2de52f_65","__typename":"Paragraph","name":"0c9b","text":"Predictions (object locations and classes) are made from one single network. Can be trained end-to-end to improve accuracy.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_66":{"id":"4d50ee2de52f_66","__typename":"Paragraph","name":"4722","text":"YOLO is more generalized. It outperforms other methods when generalizing from natural images to other domains like artwork.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_67":{"id":"4d50ee2de52f_67","__typename":"Paragraph","name":"382f","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*XbOnbcZmc50hyhhTwhD5QA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_68":{"id":"4d50ee2de52f_68","__typename":"Paragraph","name":"d34b","text":"Region proposal methods limit the classifier to the specific region. YOLO accesses to the whole image in predicting boundaries. With the additional context, YOLO demonstrates fewer false positives in background areas.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_69":{"id":"4d50ee2de52f_69","__typename":"Paragraph","name":"fb3f","text":"YOLO detects one object per grid cell. It enforces spatial diversity in making predictions.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_70":{"id":"4d50ee2de52f_70","__typename":"Paragraph","name":"77ea","text":"YOLOv2","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_71":{"id":"4d50ee2de52f_71","__typename":"Paragraph","name":"2df6","text":"SSD is a strong competitor for YOLO which at one point demonstrates higher accuracy for real-time processing. Comparing with region based detectors, YOLO has higher localization errors and the recall (measure how good to locate all objects) is lower. YOLOv2 is the second version of the YOLO with the objective of improving the accuracy significantly while making it faster.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_72":{"id":"4d50ee2de52f_72","__typename":"Paragraph","name":"12a7","text":"Accuracy improvements","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_73":{"id":"4d50ee2de52f_73","__typename":"Paragraph","name":"63fc","text":"Batch normalization","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":19,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_74":{"id":"4d50ee2de52f_74","__typename":"Paragraph","name":"34ba","text":"Add batch normalization in convolution layers. This removes the need for dropouts and pushes mAP up 2%.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_75":{"id":"4d50ee2de52f_75","__typename":"Paragraph","name":"f021","text":"High-resolution classifier","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":26,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_76":{"id":"4d50ee2de52f_76","__typename":"Paragraph","name":"6a6d","text":"The YOLO training composes of 2 phases. First, we train a classifier network like VGG16. Then we replace the fully connected layers with a convolution layer and retrain it end-to-end for the object detection. YOLO trains the classifier with 224 × 224 pictures followed by 448 × 448 pictures for the object detection. YOLOv2 starts with 224 × 224 pictures for the classifier training but then retune the classifier again with 448 × 448 pictures using much fewer epochs. This makes the detector training easier and moves mAP up by 4%.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_77":{"id":"4d50ee2de52f_77","__typename":"Paragraph","name":"afa6","text":"Convolutional with Anchor Boxes","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":31,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_78":{"id":"4d50ee2de52f_78","__typename":"Paragraph","name":"77f0","text":"As indicated in the YOLO paper, the early training is susceptible to unstable gradients. Initially, YOLO makes arbitrary guesses on the boundary boxes. These guesses may work well for some objects but badly for others resulting in steep gradient changes. In early training, predictions are fighting with each other on what shapes to specialize on.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":20,"end":30,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.02640.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_79":{"id":"4d50ee2de52f_79","__typename":"Paragraph","name":"5762","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*7iwTsezrn-tSndx96twprA.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_80":{"id":"4d50ee2de52f_80","__typename":"Paragraph","name":"4704","text":"In the real-life domain, the boundary boxes are not arbitrary. Cars have very similar shapes and pedestrians have an approximate aspect ratio of 0.41.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_81":{"id":"4d50ee2de52f_81","__typename":"Paragraph","name":"c23c","text":"Source","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*krGqonOLMzSE_PWqH_LvQA.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":6,"type":"A","href":"https:\u002F\u002Fwww.youtube.com\u002Fwatch?v=xVwsr9p3irA","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_82":{"id":"4d50ee2de52f_82","__typename":"Paragraph","name":"4c68","text":"Since we only need one guess to be right, the initial training will be more stable if we start with diverse guesses that are common for real-life objects.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_83":{"id":"4d50ee2de52f_83","__typename":"Paragraph","name":"763e","text":"More diverse predictions","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*CGWTPTY0sfvQoxsS0X6VFg.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_84":{"id":"4d50ee2de52f_84","__typename":"Paragraph","name":"3040","text":"For example, we can create 5 anchor boxes with the following shapes.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":29,"end":35,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_85":{"id":"4d50ee2de52f_85","__typename":"Paragraph","name":"db1e","text":"5 anchor boxes","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*8Q8r9ixjTiKLi1mrF36xCw.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_86":{"id":"4d50ee2de52f_86","__typename":"Paragraph","name":"8507","text":"Instead of predicting 5 arbitrary boundary boxes, we predict offsets to each of the anchor boxes above. If we constrain the offset values, we can maintain the diversity of the predictions and have each prediction focuses on a specific shape. So the initial training will be more stable.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":110,"end":119,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_87":{"id":"4d50ee2de52f_87","__typename":"Paragraph","name":"2366","text":"In the paper, anchors are also called priors.","type":"BQ","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":38,"end":44,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_88":{"id":"4d50ee2de52f_88","__typename":"Paragraph","name":"4b0b","text":"Here are the changes we make to the network:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_89":{"id":"4d50ee2de52f_89","__typename":"Paragraph","name":"dd5b","text":"Remove the fully connected layers responsible for predicting the boundary box.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_90":{"id":"4d50ee2de52f_90","__typename":"Paragraph","name":"12f2","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*tavjieD0Bum_uX-svYUTKA.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_91":{"id":"4d50ee2de52f_91","__typename":"Paragraph","name":"4874","text":"We move the class prediction from the cell level to the boundary box level. Now, each prediction includes 4 parameters for the boundary box, 1 box confidence score (objectness) and 20 class probabilities. i.e. 5 boundary boxes with 25 parameters: 125 parameters per grid cell. Same as YOLO, the objectness prediction still predicts the IOU of the ground truth and the proposed box.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_92":{"id":"4d50ee2de52f_92","__typename":"Paragraph","name":"7c2e","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*UsqjfoW3sLkmyXKQ0Hyo8A.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_93":{"id":"4d50ee2de52f_93","__typename":"Paragraph","name":"0298","text":"To generate predictions with a shape of 7 × 7 × 125, we replace the last convolution layer with three 3 × 3 convolutional layers each outputting 1024 output channels. Then we apply a final 1 × 1 convolutional layer to convert the 7 × 7 × 1024 output into 7 × 7 × 125. (See the section on DarkNet for the details.)","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_94":{"id":"4d50ee2de52f_94","__typename":"Paragraph","name":"b734","text":"Using convolution filters to make predictions.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*KiSd2CBfcs5af6oliHCqzw.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_95":{"id":"4d50ee2de52f_95","__typename":"Paragraph","name":"ef2c","text":"Change the input image size from 448 × 448 to 416 × 416. This creates an odd number spatial dimension (7×7 v.s. 8×8 grid cell). The center of a picture is often occupied by a large object. With an odd number grid cell, it is more certain on where the object belongs.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_96":{"id":"4d50ee2de52f_96","__typename":"Paragraph","name":"a764","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*89qezEeLKJLpD8_fM_H4qQ.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_97":{"id":"4d50ee2de52f_97","__typename":"Paragraph","name":"7155","text":"Remove one pooling layer to make the spatial output of the network to 13×13 (instead of 7×7).","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":70,"end":76,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_98":{"id":"4d50ee2de52f_98","__typename":"Paragraph","name":"9de4","text":"Anchor boxes decrease mAP slightly from 69.5 to 69.2 but the recall improves from 81% to 88%. i.e. even the accuracy is slightly decreased but it increases the chances of detecting all the ground truth objects.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_99":{"id":"4d50ee2de52f_99","__typename":"Paragraph","name":"6c07","text":"Dimension Clusters","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":18,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_100":{"id":"4d50ee2de52f_100","__typename":"Paragraph","name":"ea00","text":"In many problem domains, the boundary boxes have strong patterns. For example, in the autonomous driving, the 2 most common boundary boxes will be cars and pedestrians at different distances. To identify the top-K boundary boxes that have the best coverage for the training data, we run K-means clustering on the training data to locate the centroids of the top-K clusters.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_101":{"id":"4d50ee2de52f_101","__typename":"Paragraph","name":"d8a6","text":"(Image modified form a k-means cluster)","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*esSmI0UaMr-GrqkUGMg0hA.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":21,"end":38,"type":"A","href":"https:\u002F\u002Fmapr.com\u002Fblog\u002Fmonitoring-real-time-uber-data-using-spark-machine-learning-streaming-and-kafka-api-part-1\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_102":{"id":"4d50ee2de52f_102","__typename":"Paragraph","name":"0b47","text":"Since we are dealing with boundary boxes rather than points, we cannot use the regular spatial distance to measure datapoint distances. No surprise, we use IoU.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_103":{"id":"4d50ee2de52f_103","__typename":"Paragraph","name":"b048","text":"Source","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*l5wvrPjLlFp6Whgy0MqbKQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":6,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.08242.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_104":{"id":"4d50ee2de52f_104","__typename":"Paragraph","name":"a0d7","text":"On the left, we plot the average IoU between the anchors and the ground truth boxes using different numbers of clusters (anchors). As the number of anchors increases, the accuracy improvement plateaus. For the best return, YOLO settles down with 5 anchors. On the right, it displays the 5 anchors’ shapes. The purplish-blue rectangles are selected from the COCO dataset while the black border rectangles are selected from the VOC2007. In both cases, we have more thin and tall anchors indicating that real-life boundary boxes are not arbitrary.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_105":{"id":"4d50ee2de52f_105","__typename":"Paragraph","name":"ae5e","text":"Unless we are comparing YOLO and YOLOv2, we will reference YOLOv2 as YOLO for now.","type":"BQ","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_106":{"id":"4d50ee2de52f_106","__typename":"Paragraph","name":"0780","text":"Direct location prediction","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":26,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_107":{"id":"4d50ee2de52f_107","__typename":"Paragraph","name":"d58d","text":"We make predictions on the offsets to the anchors. Nevertheless, if it is unconstrained, our guesses will be randomized again. YOLO predicts 5 parameters (tx, ty, tw, th, and to) and applies the sigma function to constraint its possible offset range.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":156,"end":157,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":160,"end":161,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":164,"end":165,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":168,"end":169,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":176,"end":177,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_108":{"id":"4d50ee2de52f_108","__typename":"Paragraph","name":"06b8","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*38-Tdx-wQA7c3TX5hdnwpw.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_109":{"id":"4d50ee2de52f_109","__typename":"Paragraph","name":"60ab","text":"Here is the visualization. The blue box below is the predicted boundary box and the dotted rectangle is the anchor.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_110":{"id":"4d50ee2de52f_110","__typename":"Paragraph","name":"7034","text":"Modified from the paper.","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*gyOSRA_FDz4Pf5njoUb4KQ.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":18,"end":23,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.08242.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_111":{"id":"4d50ee2de52f_111","__typename":"Paragraph","name":"91a9","text":"With the use of k-means clustering (dimension clusters) and the improvement mentioned in this section, mAP increases 5%.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_112":{"id":"4d50ee2de52f_112","__typename":"Paragraph","name":"5583","text":"Fine-Grained Features","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":21,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_113":{"id":"4d50ee2de52f_113","__typename":"Paragraph","name":"2313","text":"Convolution layers decrease the spatial dimension gradually. As the corresponding resolution decreases, it is harder to detect small objects. Other object detectors like SSD locate objects from different layers of feature maps. So each layer specializes at a different scale. YOLO adopts a different approach called passthrough. It reshapes the 26 × 26 × 512 layer to 13 × 13 × 2048. Then it concatenates with the original 13 × 13 ×1024 output layer. Now we apply convolution filters on the new 13 × 13 × 3072 layer to make predictions.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_114":{"id":"4d50ee2de52f_114","__typename":"Paragraph","name":"12f8","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*RuW-SCIML8SHc5_PrIE9-g.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_115":{"id":"4d50ee2de52f_115","__typename":"Paragraph","name":"32a7","text":"Multi-Scale Training","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":20,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_116":{"id":"4d50ee2de52f_116","__typename":"Paragraph","name":"3c3d","text":"After removing the fully connected layers, YOLO can take images of different sizes. If the width and height are doubled, we are just making 4x output grid cells and therefore 4x predictions. Since the YOLO network downsamples the input by 32, we just need to make sure the width and height is a multiple of 32. During training, YOLO takes images of size 320×320, 352×352, … and 608×608 (with a step of 32). For every 10 batches, YOLOv2 randomly selects another image size to train the model. This acts as data augmentation and forces the network to predict well for different input image dimension and scale. In additional, we can use lower resolution images for object detection at the cost of accuracy. This can be a good tradeoff for speed on low GPU power devices. At 288 × 288 YOLO runs at more than 90 FPS with mAP almost as good as Fast R-CNN. At high-resolution YOLO achieves 78.6 mAP on VOC 2007.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_117":{"id":"4d50ee2de52f_117","__typename":"Paragraph","name":"8d9e","text":"Accuracy","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":8,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_118":{"id":"4d50ee2de52f_118","__typename":"Paragraph","name":"97e0","text":"Here is the accuracy improvements after applying the techniques discussed so far:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_119":{"id":"4d50ee2de52f_119","__typename":"Paragraph","name":"d6c3","text":"Source","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*3IdCKSzR5R0lIE1LSmN4Bg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":6,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.08242.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_120":{"id":"4d50ee2de52f_120","__typename":"Paragraph","name":"1739","text":"Accuracy comparison for different detectors:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_121":{"id":"4d50ee2de52f_121","__typename":"Paragraph","name":"5ec1","text":"Source","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*NJj17Z6FgffYaA4WH2WIjw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":6,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.08242.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_122":{"id":"4d50ee2de52f_122","__typename":"Paragraph","name":"0d9a","text":"Speed improvement","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_123":{"id":"4d50ee2de52f_123","__typename":"Paragraph","name":"89bb","text":"GoogLeNet","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":9,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_124":{"id":"4d50ee2de52f_124","__typename":"Paragraph","name":"34c3","text":"VGG16 requires 30.69 billion floating point operations for a single pass over a 224 × 224 image versus 8.52 billion operations for a customized GoogLeNet. We can replace the VGG16 with the customized GoogLeNet. However, YOLO pays a price on the top-5 accuracy for ImageNet: accuracy drops from 90.0% to 88.0%.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_125":{"id":"4d50ee2de52f_125","__typename":"Paragraph","name":"4eea","text":"DarkNet","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":7,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_126":{"id":"4d50ee2de52f_126","__typename":"Paragraph","name":"e314","text":"We can further simplify the backbone CNN used. Darknet requires 5.58 billion operations only. With DarkNet, YOLO achieves 72.9% top-1 accuracy and 91.2% top-5 accuracy on ImageNet. Darknet uses mostly 3 × 3 filters to extract features and 1 × 1 filters to reduce output channels. It also uses global average pooling to make predictions. Here is the detail network description:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_127":{"id":"4d50ee2de52f_127","__typename":"Paragraph","name":"3e5c","text":"Modified from source","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*8FiQUakp9i4MneU4VXk4Ww.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":14,"end":20,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.08242.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_128":{"id":"4d50ee2de52f_128","__typename":"Paragraph","name":"d0d9","text":"We replace the last convolution layer (the cross-out section) with three 3 × 3 convolutional layers each outputting 1024 output channels. Then we apply a final 1 × 1 convolutional layer to convert the 7 × 7 × 1024 output into 7 × 7 × 125. (5 boundary boxes each with 4 parameters for the box, 1 objectness score and 20 conditional class probabilities)","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_129":{"id":"4d50ee2de52f_129","__typename":"Paragraph","name":"970d","text":"YOLO with DarkNet","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*NBnDpz8fitkhcdnkgF2bvg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_130":{"id":"4d50ee2de52f_130","__typename":"Paragraph","name":"7726","text":"Training","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_131":{"id":"4d50ee2de52f_131","__typename":"Paragraph","name":"9a43","text":"YOLO is trained with the ImageNet 1000 class classification dataset in 160 epochs: using stochastic gradient descent with a starting learning rate of 0.1, polynomial rate decay with a power of 4, weight decay of 0.0005 and momentum of 0.9. In the initial training, YOLO uses 224 × 224 images, and then retune it with 448× 448 images for 10 epochs at a 10−3 learning rate. After the training, the classifier achieves a top-1 accuracy of 76.5% and a top-5 accuracy of 93.3%.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_132":{"id":"4d50ee2de52f_132","__typename":"Paragraph","name":"406c","text":"Then the fully connected layers and the last convolution layer is removed for a detector. YOLO adds three 3 × 3 convolutional layers with 1024 filters each followed by a final 1 × 1 convolutional layer with 125 output channels. (5 box predictions each with 25 parameters) YOLO also add a passthrough layer. YOLO trains the network for 160 epochs with a starting learning rate of 10−3 , dividing it by 10 at 60 and 90 epochs. YOLO uses a weight decay of 0.0005 and momentum of 0.9.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_133":{"id":"4d50ee2de52f_133","__typename":"Paragraph","name":"c14b","text":"Classification","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":14,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_134":{"id":"4d50ee2de52f_134","__typename":"Paragraph","name":"9aa2","text":"Datasets for object detection have far fewer class categories than those for classification. To expand the classes that YOLO can detect, YOLO proposes a method to mix images from both detection and classification datasets during training. It trains the end-to-end network with the object detection samples while backpropagates the classification loss from the classification samples to train the classifier path. This approach encounters a few challenges:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_135":{"id":"4d50ee2de52f_135","__typename":"Paragraph","name":"a98e","text":"How do we merge class labels from different datasets? In particular, object detection datasets and different classification datasets uses different labels.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_136":{"id":"4d50ee2de52f_136","__typename":"Paragraph","name":"0d6b","text":"Any merged labels may not be mutually exclusive, for example, Norfolk terrier in ImageNet and dog in COCO. Since it is not mutually exclusive, we can not use softmax to compute the probability.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":62,"end":77,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":94,"end":98,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_137":{"id":"4d50ee2de52f_137","__typename":"Paragraph","name":"ed9e","text":"Hierarchical classification","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":27,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_138":{"id":"4d50ee2de52f_138","__typename":"Paragraph","name":"92f9","text":"Without going into details, YOLO combines labels in different datasets to form a tree-like structure WordTree. The children form an is-a relationship with its parent like biplane is a plane. But the merged labels are now not mutually exclusive.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":101,"end":109,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_139":{"id":"4d50ee2de52f_139","__typename":"Paragraph","name":"3704","text":"Combining COCO and ImageNet labels to a hierarchical WordTree (source)","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*QKwSclDLcT8eJxyj3CVL0w.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":63,"end":69,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.08242.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_140":{"id":"4d50ee2de52f_140","__typename":"Paragraph","name":"3bec","text":"Let’s simplify the discussion using the 1000 class ImageNet. Instead of predicting 1000 labels in a flat structure, we create the corresponding WordTree which has 1000 leave nodes for the original labels and 369 nodes for their parent classes. Originally, YOLO predicts the class score for the biplane. But with the WordTree, it now predicts the score for the biplane given it is an airplane.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_141":{"id":"4d50ee2de52f_141","__typename":"Paragraph","name":"e2b5","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*TALo1-LuWbF80RTCM7bszw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_142":{"id":"4d50ee2de52f_142","__typename":"Paragraph","name":"2381","text":"Since","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_143":{"id":"4d50ee2de52f_143","__typename":"Paragraph","name":"e1bf","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*0Oyd6FWXo7OJzzuXuXSoDQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_144":{"id":"4d50ee2de52f_144","__typename":"Paragraph","name":"132d","text":"we can apply a softmax function to compute the probability","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_145":{"id":"4d50ee2de52f_145","__typename":"Paragraph","name":"4a97","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*_gX42bmgvyib4lyR9GQrAg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_146":{"id":"4d50ee2de52f_146","__typename":"Paragraph","name":"83de","text":"from the scores of its own and the siblings. The difference is, instead of one softmax operations, YOLO performs multiple softmax operations for each parent’s children.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_147":{"id":"4d50ee2de52f_147","__typename":"Paragraph","name":"7b7d","text":"source","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*_0ijX5oRZdQCNlSk-gXj3Q.jpeg"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":6,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.08242.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_148":{"id":"4d50ee2de52f_148","__typename":"Paragraph","name":"c923","text":"The class probability is then computed from the YOLO predictions by going up the WordTree.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_149":{"id":"4d50ee2de52f_149","__typename":"Paragraph","name":"0e48","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*0XHKAh0GxpW22y1DlL4hYA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_150":{"id":"4d50ee2de52f_150","__typename":"Paragraph","name":"bc6c","text":"For classification, we assume an object is already detected and therefore Pr(physical object)=1.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_151":{"id":"4d50ee2de52f_151","__typename":"Paragraph","name":"5c22","text":"One benefit of the hierarchy classification is that when YOLO cannot distinguish the type of airplane, it gives a high score to the airplane instead of forcing it into one of the sub-categories.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_152":{"id":"4d50ee2de52f_152","__typename":"Paragraph","name":"b4cc","text":"When YOLO sees a classification image, it only backpropagates classification loss to train the classifier. YOLO finds the bounding box that predicts the highest probability for that class and it computes the classification loss as well as those from the parents. (If an object is labeled as a biplane, it is also considered to be labeled as airplane, air, vehicle… ) This encourages the model to extract features common to them. So even we have never trained a specific class of objects for object detection, we can still make such predictions by generalizing predictions from related objects.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_153":{"id":"4d50ee2de52f_153","__typename":"Paragraph","name":"0011","text":"In object detection, we set Pr(physical object) equals to the box confidence score which measures whether the box has an object. YOLO traverses down the tree, taking the highest confidence path at every split until it reaches some threshold and YOLO predicts that object class.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_154":{"id":"4d50ee2de52f_154","__typename":"Paragraph","name":"9465","text":"YOLO9000","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":8,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_155":{"id":"4d50ee2de52f_155","__typename":"Paragraph","name":"c79d","text":"YOLO9000 extends YOLO to detect objects over 9000 classes using hierarchical classification with a 9418 node WordTree. It combines samples from COCO and the top 9000 classes from the ImageNet. YOLO samples four ImageNet data for every COCO data. It learns to find objects using the detection data in COCO and to classify these objects with ImageNet samples.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_156":{"id":"4d50ee2de52f_156","__typename":"Paragraph","name":"4b01","text":"During the evaluation, YOLO test images on categories that it knows how to classify but not trained directly to locate them, i.e. categories that do not exist in COCO. YOLO9000 evaluates its result from the ImageNet object detection dataset which has 200 categories. It shares about 44 categories with COCO. Therefore, the dataset contains 156 categories that have never been trained directly on how to locate them. YOLO extracts similar features for related object types. Hence, we can detect those 156 categories by simply from the feature values.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_157":{"id":"4d50ee2de52f_157","__typename":"Paragraph","name":"69d2","text":"YOLO9000 gets 19.7 mAP overall with 16.0 mAP on those 156 categories. YOLO9000 performs well with new species of animals not found in COCO because their shapes can be generalized easily from their parent classes. However, COCO does not have bounding box labels for any type of clothing so the test struggles with categories like “sunglasses”.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_158":{"id":"4d50ee2de52f_158","__typename":"Paragraph","name":"b8ad","text":"YOLOv3","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_159":{"id":"4d50ee2de52f_159","__typename":"Paragraph","name":"fd5c","text":"","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"hasDropCap":null,"iframe":{"__typename":"Iframe","mediaResource":{"__ref":"MediaResource:28431402a61e5d8c00d4e6827e656b8b"}},"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_160":{"id":"4d50ee2de52f_160","__typename":"Paragraph","name":"9fad","text":"A quote from the YOLO web site on YOLOv3:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_161":{"id":"4d50ee2de52f_161","__typename":"Paragraph","name":"974f","text":"On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev.","type":"BQ","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_162":{"id":"4d50ee2de52f_162","__typename":"Paragraph","name":"667f","text":"Class Prediction","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":16,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_163":{"id":"4d50ee2de52f_163","__typename":"Paragraph","name":"7f2d","text":"Most classifiers assume output labels are mutually exclusive. It is true if the output are mutually exclusive object classes. Therefore, YOLO applies a softmax function to convert scores into probabilities that sum up to one. YOLOv3 uses multi-label classification. For example, the output labels may be “pedestrian” and “child” which are not non-exclusive. (the sum of output can be greater than 1 now.) YOLOv3 replaces the softmax function with independent logistic classifiers to calculate the likeliness of the input belongs to a specific label. Instead of using mean square error in calculating the classification loss, YOLOv3 uses binary cross-entropy loss for each label. This also reduces the computation complexity by avoiding the softmax function.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_164":{"id":"4d50ee2de52f_164","__typename":"Paragraph","name":"c898","text":"Bounding box prediction & cost function calculation","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":51,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_165":{"id":"4d50ee2de52f_165","__typename":"Paragraph","name":"373a","text":"YOLOv3 predicts an objectness score for each bounding box using logistic regression. YOLOv3 changes the way in calculating the cost function. If the bounding box prior (anchor) overlaps a ground truth object more than others, the corresponding objectness score should be 1. For other priors with overlap greater than a predefined threshold (default 0.5), they incur no cost. Each ground truth object is associated with one boundary box prior only. If a bounding box prior is not assigned, it incurs no classification and localization lost, just confidence loss on objectness. We use tx and ty (instead of bx and by) to compute the loss.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_166":{"id":"4d50ee2de52f_166","__typename":"Paragraph","name":"a75a","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*UuIx6sH39tefFMxgWAaaNQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_167":{"id":"4d50ee2de52f_167","__typename":"Paragraph","name":"6b02","text":"Feature Pyramid Networks (FPN) like Feature Pyramid","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":51,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_168":{"id":"4d50ee2de52f_168","__typename":"Paragraph","name":"b55f","text":"YOLOv3 makes 3 predictions per location. Each prediction composes of a boundary box, a objectness and 80 class scores, i.e. N × N × [3 × (4 + 1 + 80) ] predictions.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_169":{"id":"4d50ee2de52f_169","__typename":"Paragraph","name":"c2ae","text":"YOLOv3 makes predictions at 3 different scales (similar to the FPN):","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_170":{"id":"4d50ee2de52f_170","__typename":"Paragraph","name":"403a","text":"In the last feature map layer.","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_171":{"id":"4d50ee2de52f_171","__typename":"Paragraph","name":"6ecf","text":"Then it goes back 2 layers back and upsamples it by 2. YOLOv3 then takes a feature map with higher resolution and merge it with the upsampled feature map using element-wise addition. YOLOv3 apply convolutional filters on the merged map to make the second set of predictions.","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_172":{"id":"4d50ee2de52f_172","__typename":"Paragraph","name":"7b3e","text":"Repeat 2 again so the resulted feature map layer has good high-level structure (semantic) information and good resolution spatial information on object locations.","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_173":{"id":"4d50ee2de52f_173","__typename":"Paragraph","name":"ecae","text":"To determine the priors, YOLOv3 applies k-means cluster. Then it pre-select 9 clusters. For COCO, the width and height of the anchors are (10×13),(16×30),(33×23),(30×61),(62×45),(59× 119),(116 × 90),(156 × 198),(373 × 326). These 9 priors are grouped into 3 different groups according to their scale. Each group is assigned to a specific feature map above in detecting objects.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_174":{"id":"4d50ee2de52f_174","__typename":"Paragraph","name":"4161","text":"Feature extractor","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":17,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_175":{"id":"4d50ee2de52f_175","__typename":"Paragraph","name":"f03d","text":"A new 53-layer Darknet-53 is used to replace the Darknet-19 as the feature extractor. Darknet-53 mainly compose of 3 × 3 and 1× 1 filters with skip connections like the residual network in ResNet. Darknet-53 has less BFLOP (billion floating point operations) than ResNet-152, but achieves the same classification accuracy at 2x faster.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_176":{"id":"4d50ee2de52f_176","__typename":"Paragraph","name":"9603","text":"Darknet-53","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*biRYJyCSv-UTbTQTa4Afqg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":10,"type":"A","href":"https:\u002F\u002Fpjreddie.com\u002Fmedia\u002Ffiles\u002Fpapers\u002FYOLOv3.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_177":{"id":"4d50ee2de52f_177","__typename":"Paragraph","name":"def0","text":"YOLOv3 performance","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":18,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_178":{"id":"4d50ee2de52f_178","__typename":"Paragraph","name":"20af","text":"YOLOv3's COCO AP metric is on par with SSD but 3x faster. But YOLOv3’s AP is still behind RetinaNet. In particular, AP@IoU=.75 drops significantly comparing with RetinaNet which suggests YOLOv3 has higher localization error. YOLOv3 also shows significant improvement in detecting small objects.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_179":{"id":"4d50ee2de52f_179","__typename":"Paragraph","name":"5cda","text":"","type":"IMG","href":null,"layout":"OUTSET_CENTER","metadata":{"__ref":"ImageMetadata:1*bFMwN__ZgfvVRYuo4o8EEg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_180":{"id":"4d50ee2de52f_180","__typename":"Paragraph","name":"148d","text":"YOLOv3 performs very well in the fast detector category when speed is important.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_181":{"id":"4d50ee2de52f_181","__typename":"Paragraph","name":"27e3","text":"Source","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*RFpjH8D6TStBaYuZYehe_g.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":6,"type":"A","href":"https:\u002F\u002Fpjreddie.com\u002Fmedia\u002Ffiles\u002Fpapers\u002FYOLOv3.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_182":{"id":"4d50ee2de52f_182","__typename":"Paragraph","name":"3054","text":"Resources","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:4d50ee2de52f_183":{"id":"4d50ee2de52f_183","__typename":"Paragraph","name":"e094","text":"The original paper on YOLO.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":27,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1506.02640.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_184":{"id":"4d50ee2de52f_184","__typename":"Paragraph","name":"7b35","text":"Paper on YOLOv2 and YOLO9000.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":29,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fpdf\u002F1612.08242.pdf","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:4d50ee2de52f_185":{"id":"4d50ee2de52f_185","__typename":"Paragraph","name":"fac5","text":"DarkNet implementation.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":23,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fpjreddie\u002Fdarknet","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"ImageMetadata:1*ju1oaoIkVUkaIPAdpehlzA.png":{"id":"1*ju1oaoIkVUkaIPAdpehlzA.png","__typename":"ImageMetadata","originalHeight":960,"originalWidth":1091,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:b901faec3876bb5da6091d9410fd9c72":{"id":"b901faec3876bb5da6091d9410fd9c72","__typename":"MediaResource","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FVOC3huqHrss%3Ffeature%3Doembed&display_name=YouTube&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DVOC3huqHrss&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FVOC3huqHrss%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=youtube","iframeHeight":480,"iframeWidth":854,"title":"YOLO v2"},"ImageMetadata:1*EYFejGUjvjPcc4PZTwoufw.jpeg":{"id":"1*EYFejGUjvjPcc4PZTwoufw.jpeg","__typename":"ImageMetadata","originalHeight":586,"originalWidth":872,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*QOGcvHbrDZiCqTG6THIQ_w.png":{"id":"1*QOGcvHbrDZiCqTG6THIQ_w.png","__typename":"ImageMetadata","originalHeight":586,"originalWidth":872,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*6qZXYCDUkC5Bc8nRolT0Mw.jpeg":{"id":"1*6qZXYCDUkC5Bc8nRolT0Mw.jpeg","__typename":"ImageMetadata","originalHeight":669,"originalWidth":1139,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*4Y1PaY3ZgxKt5w84_0pNxw.jpeg":{"id":"1*4Y1PaY3ZgxKt5w84_0pNxw.jpeg","__typename":"ImageMetadata","originalHeight":669,"originalWidth":667,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*j4PnWfxP3yoVPOFyI27tww.jpeg":{"id":"1*j4PnWfxP3yoVPOFyI27tww.jpeg","__typename":"ImageMetadata","originalHeight":1855,"originalWidth":3036,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*OuMJUWo2rXYA-GYU63NUGw.jpeg":{"id":"1*OuMJUWo2rXYA-GYU63NUGw.jpeg","__typename":"ImageMetadata","originalHeight":1354,"originalWidth":2113,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*jtnrhMFNwGxiQmkY6LkdCQ.jpeg":{"id":"1*jtnrhMFNwGxiQmkY6LkdCQ.jpeg","__typename":"ImageMetadata","originalHeight":238,"originalWidth":804,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*e0VY6U1_WMF2KBoKQNZvkQ.png":{"id":"1*e0VY6U1_WMF2KBoKQNZvkQ.png","__typename":"ImageMetadata","originalHeight":469,"originalWidth":1715,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*qVL77IZyEnra4DvENayXUA.png":{"id":"1*qVL77IZyEnra4DvENayXUA.png","__typename":"ImageMetadata","originalHeight":90,"originalWidth":1528,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*0IPktA65WxOBfP_ULQWcmw.png":{"id":"1*0IPktA65WxOBfP_ULQWcmw.png","__typename":"ImageMetadata","originalHeight":566,"originalWidth":1624,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*9ER4GVUtQGVA2Y0skC9OQQ.png":{"id":"1*9ER4GVUtQGVA2Y0skC9OQQ.png","__typename":"ImageMetadata","originalHeight":806,"originalWidth":1920,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*lF6SCAVj5jMwLxs39SCogw.png":{"id":"1*lF6SCAVj5jMwLxs39SCogw.png","__typename":"ImageMetadata","originalHeight":434,"originalWidth":1454,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*BwhGMvffFfqtND9413oiwA.png":{"id":"1*BwhGMvffFfqtND9413oiwA.png","__typename":"ImageMetadata","originalHeight":610,"originalWidth":1598,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*QT7mwEbyLJYIxTYtOWClFQ.png":{"id":"1*QT7mwEbyLJYIxTYtOWClFQ.png","__typename":"ImageMetadata","originalHeight":464,"originalWidth":1658,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*Yc_OJIXOoV2WaGQ6PqhTXA.png":{"id":"1*Yc_OJIXOoV2WaGQ6PqhTXA.png","__typename":"ImageMetadata","originalHeight":550,"originalWidth":1526,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*aW6htqx4Q7APLrSQg2eWDw.png":{"id":"1*aW6htqx4Q7APLrSQg2eWDw.png","__typename":"ImageMetadata","originalHeight":640,"originalWidth":1372,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*XbOnbcZmc50hyhhTwhD5QA.png":{"id":"1*XbOnbcZmc50hyhhTwhD5QA.png","__typename":"ImageMetadata","originalHeight":762,"originalWidth":2140,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*7iwTsezrn-tSndx96twprA.jpeg":{"id":"1*7iwTsezrn-tSndx96twprA.jpeg","__typename":"ImageMetadata","originalHeight":498,"originalWidth":691,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*krGqonOLMzSE_PWqH_LvQA.jpeg":{"id":"1*krGqonOLMzSE_PWqH_LvQA.jpeg","__typename":"ImageMetadata","originalHeight":1428,"originalWidth":2688,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*CGWTPTY0sfvQoxsS0X6VFg.jpeg":{"id":"1*CGWTPTY0sfvQoxsS0X6VFg.jpeg","__typename":"ImageMetadata","originalHeight":498,"originalWidth":691,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*8Q8r9ixjTiKLi1mrF36xCw.jpeg":{"id":"1*8Q8r9ixjTiKLi1mrF36xCw.jpeg","__typename":"ImageMetadata","originalHeight":227,"originalWidth":504,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*tavjieD0Bum_uX-svYUTKA.jpeg":{"id":"1*tavjieD0Bum_uX-svYUTKA.jpeg","__typename":"ImageMetadata","originalHeight":806,"originalWidth":1904,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*UsqjfoW3sLkmyXKQ0Hyo8A.png":{"id":"1*UsqjfoW3sLkmyXKQ0Hyo8A.png","__typename":"ImageMetadata","originalHeight":476,"originalWidth":1047,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*KiSd2CBfcs5af6oliHCqzw.jpeg":{"id":"1*KiSd2CBfcs5af6oliHCqzw.jpeg","__typename":"ImageMetadata","originalHeight":943,"originalWidth":1579,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*89qezEeLKJLpD8_fM_H4qQ.jpeg":{"id":"1*89qezEeLKJLpD8_fM_H4qQ.jpeg","__typename":"ImageMetadata","originalHeight":379,"originalWidth":822,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*esSmI0UaMr-GrqkUGMg0hA.jpeg":{"id":"1*esSmI0UaMr-GrqkUGMg0hA.jpeg","__typename":"ImageMetadata","originalHeight":298,"originalWidth":388,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*l5wvrPjLlFp6Whgy0MqbKQ.png":{"id":"1*l5wvrPjLlFp6Whgy0MqbKQ.png","__typename":"ImageMetadata","originalHeight":590,"originalWidth":1114,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*38-Tdx-wQA7c3TX5hdnwpw.jpeg":{"id":"1*38-Tdx-wQA7c3TX5hdnwpw.jpeg","__typename":"ImageMetadata","originalHeight":1020,"originalWidth":1536,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*gyOSRA_FDz4Pf5njoUb4KQ.jpeg":{"id":"1*gyOSRA_FDz4Pf5njoUb4KQ.jpeg","__typename":"ImageMetadata","originalHeight":918,"originalWidth":1214,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*RuW-SCIML8SHc5_PrIE9-g.jpeg":{"id":"1*RuW-SCIML8SHc5_PrIE9-g.jpeg","__typename":"ImageMetadata","originalHeight":345,"originalWidth":1600,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*3IdCKSzR5R0lIE1LSmN4Bg.png":{"id":"1*3IdCKSzR5R0lIE1LSmN4Bg.png","__typename":"ImageMetadata","originalHeight":648,"originalWidth":1568,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*NJj17Z6FgffYaA4WH2WIjw.png":{"id":"1*NJj17Z6FgffYaA4WH2WIjw.png","__typename":"ImageMetadata","originalHeight":325,"originalWidth":462,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*8FiQUakp9i4MneU4VXk4Ww.png":{"id":"1*8FiQUakp9i4MneU4VXk4Ww.png","__typename":"ImageMetadata","originalHeight":598,"originalWidth":464,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*NBnDpz8fitkhcdnkgF2bvg.png":{"id":"1*NBnDpz8fitkhcdnkgF2bvg.png","__typename":"ImageMetadata","originalHeight":211,"originalWidth":1794,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*QKwSclDLcT8eJxyj3CVL0w.png":{"id":"1*QKwSclDLcT8eJxyj3CVL0w.png","__typename":"ImageMetadata","originalHeight":1482,"originalWidth":1440,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*TALo1-LuWbF80RTCM7bszw.png":{"id":"1*TALo1-LuWbF80RTCM7bszw.png","__typename":"ImageMetadata","originalHeight":60,"originalWidth":1562,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*0Oyd6FWXo7OJzzuXuXSoDQ.png":{"id":"1*0Oyd6FWXo7OJzzuXuXSoDQ.png","__typename":"ImageMetadata","originalHeight":90,"originalWidth":1554,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*_gX42bmgvyib4lyR9GQrAg.png":{"id":"1*_gX42bmgvyib4lyR9GQrAg.png","__typename":"ImageMetadata","originalHeight":70,"originalWidth":1576,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*_0ijX5oRZdQCNlSk-gXj3Q.jpeg":{"id":"1*_0ijX5oRZdQCNlSk-gXj3Q.jpeg","__typename":"ImageMetadata","originalHeight":329,"originalWidth":386,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*0XHKAh0GxpW22y1DlL4hYA.png":{"id":"1*0XHKAh0GxpW22y1DlL4hYA.png","__typename":"ImageMetadata","originalHeight":238,"originalWidth":1578,"focusPercentX":null,"focusPercentY":null,"alt":null},"MediaResource:28431402a61e5d8c00d4e6827e656b8b":{"id":"28431402a61e5d8c00d4e6827e656b8b","__typename":"MediaResource","iframeSrc":"https:\u002F\u002Fcdn.embedly.com\u002Fwidgets\u002Fmedia.html?src=https%3A%2F%2Fwww.youtube.com%2Fembed%2FMPU2HistivI%3Ffeature%3Doembed&url=http%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DMPU2HistivI&image=https%3A%2F%2Fi.ytimg.com%2Fvi%2FMPU2HistivI%2Fhqdefault.jpg&key=a19fcc184b9711e1b4764040d3dc5c07&type=text%2Fhtml&schema=youtube","iframeHeight":480,"iframeWidth":854,"title":"YOLOv3"},"ImageMetadata:1*UuIx6sH39tefFMxgWAaaNQ.png":{"id":"1*UuIx6sH39tefFMxgWAaaNQ.png","__typename":"ImageMetadata","originalHeight":180,"originalWidth":1040,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*biRYJyCSv-UTbTQTa4Afqg.png":{"id":"1*biRYJyCSv-UTbTQTa4Afqg.png","__typename":"ImageMetadata","originalHeight":462,"originalWidth":334,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*bFMwN__ZgfvVRYuo4o8EEg.png":{"id":"1*bFMwN__ZgfvVRYuo4o8EEg.png","__typename":"ImageMetadata","originalHeight":290,"originalWidth":1028,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*RFpjH8D6TStBaYuZYehe_g.png":{"id":"1*RFpjH8D6TStBaYuZYehe_g.png","__typename":"ImageMetadata","originalHeight":828,"originalWidth":1328,"focusPercentX":null,"focusPercentY":null,"alt":null},"Tag:machine-learning":{"id":"machine-learning","__typename":"Tag","displayTitle":"Machine Learning","normalizedTagSlug":"machine-learning"},"Tag:deep-learning":{"id":"deep-learning","__typename":"Tag","displayTitle":"Deep Learning","normalizedTagSlug":"deep-learning"},"Tag:computer-vision":{"id":"computer-vision","__typename":"Tag","displayTitle":"Computer Vision","normalizedTagSlug":"computer-vision"},"Tag:artificial-intelligence":{"id":"artificial-intelligence","__typename":"Tag","displayTitle":"Artificial Intelligence","normalizedTagSlug":"artificial-intelligence"},"ImageMetadata:0*TX_MTqiTYkxmCmGt":{"id":"0*TX_MTqiTYkxmCmGt","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"UserViewerEdge:userId:7cfe5abac1ac-viewerId:lo_3c97a71a5cb9":{"id":"userId:7cfe5abac1ac-viewerId:lo_3c97a71a5cb9","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"User:7cfe5abac1ac":{"id":"7cfe5abac1ac","__typename":"User","name":"Dileepkumar","username":"dileepkumar1422002","bio":"Passionate learner || Data Science Enthusiast || Django || Competitive Programmer","imageId":"1*3qIEScAGIpVG5Q2Fgpt3tg.jpeg","mediumMemberAt":0,"isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:7cfe5abac1ac-viewerId:lo_3c97a71a5cb9"},"viewerIsUser":false,"newsletterV3":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"dileepkumar1422002.medium.com"}},"hasSubdomain":true,"postSubscribeMembershipUpsellShownAt":0},"Post:d3778244ed98":{"id":"d3778244ed98","__typename":"Post","title":"K-Means Clustering","mediumUrl":"https:\u002F\u002Fdileepkumar1422002.medium.com\u002Fk-means-clustering-d3778244ed98","previewImage":{"__ref":"ImageMetadata:0*TX_MTqiTYkxmCmGt"},"isPublished":true,"firstPublishedAt":1629646406653,"readingTime":3.2150943396226417,"statusForCollection":null,"isLocked":false,"visibility":"PUBLIC","collection":null,"creator":{"__ref":"User:7cfe5abac1ac"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*WngUhHTS8MD96VLwNKrPSw.png":{"id":"1*WngUhHTS8MD96VLwNKrPSw.png","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"UserViewerEdge:userId:bf5ea8e11f80-viewerId:lo_3c97a71a5cb9":{"id":"userId:bf5ea8e11f80-viewerId:lo_3c97a71a5cb9","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"User:bf5ea8e11f80":{"id":"bf5ea8e11f80","__typename":"User","name":"Isaac Kargar","username":"kargarisaac","bio":"I’m a Ph.D. student at the Intelligent Robotics Group at Aalto University working on self-driving cars, reinforcement learning, and machine learning.","imageId":"0*V0vgkiDJhhNI0dN7.jpg","mediumMemberAt":0,"isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:bf5ea8e11f80-viewerId:lo_3c97a71a5cb9"},"viewerIsUser":false,"newsletterV3":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"kargarisaac.medium.com"}},"hasSubdomain":true,"postSubscribeMembershipUpsellShownAt":0},"Post:a858d69d83b8":{"id":"a858d69d83b8","__typename":"Post","title":"Paper Review: Learning Hand-Eye Coordination for Robotic Grasping with Large-Scale Data Collection","mediumUrl":"https:\u002F\u002Fkargarisaac.medium.com\u002Fpaper-review-learning-hand-eye-coordination-for-robotic-grasping-with-large-scale-data-collection-a858d69d83b8","previewImage":{"__ref":"ImageMetadata:1*WngUhHTS8MD96VLwNKrPSw.png"},"isPublished":true,"firstPublishedAt":1555253693436,"readingTime":3.112264150943396,"statusForCollection":null,"isLocked":false,"visibility":"PUBLIC","collection":null,"creator":{"__ref":"User:bf5ea8e11f80"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*jFxVEbTPq2NSGl0QKBszdQ.png":{"id":"1*jFxVEbTPq2NSGl0QKBszdQ.png","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"CollectionViewerEdge:collectionId:5e5bef33608a-viewerId:lo_3c97a71a5cb9":{"id":"collectionId:5e5bef33608a-viewerId:lo_3c97a71a5cb9","__typename":"CollectionViewerEdge","isEditor":false},"Collection:5e5bef33608a":{"id":"5e5bef33608a","__typename":"Collection","name":"Becoming Human: Artificial Intelligence Magazine","description":"Latest News, Info and Tutorials on Artificial Intelligence, Machine Learning, Deep Learning, Big Data and what it means for Humanity.","tagline":"Latest News, Info and Tutorials on Artificial Intelligence…","domain":"becominghuman.ai","slug":"becoming-human","isAuroraEligible":true,"isAuroraVisible":false,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:5e5bef33608a-viewerId:lo_3c97a71a5cb9"},"canToggleEmail":false},"UserViewerEdge:userId:c74e54b2ea24-viewerId:lo_3c97a71a5cb9":{"id":"userId:c74e54b2ea24-viewerId:lo_3c97a71a5cb9","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"User:c74e54b2ea24":{"id":"c74e54b2ea24","__typename":"User","name":"David Clark","username":"davideclark314","bio":"","imageId":"1*zI0lKHVqHlt4ZUC2jy2wVw.png","mediumMemberAt":0,"isPartnerProgramEnrolled":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:c74e54b2ea24-viewerId:lo_3c97a71a5cb9"},"viewerIsUser":false,"newsletterV3":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"davideclark314.medium.com"}},"hasSubdomain":true,"postSubscribeMembershipUpsellShownAt":0},"Post:728744d3deac":{"id":"728744d3deac","__typename":"Post","title":"Classifying Traffic Signs","mediumUrl":"https:\u002F\u002Fbecominghuman.ai\u002Fclassifying-traffic-signs-728744d3deac","previewImage":{"__ref":"ImageMetadata:1*jFxVEbTPq2NSGl0QKBszdQ.png"},"isPublished":true,"firstPublishedAt":1491262524670,"readingTime":3.2952830188679245,"statusForCollection":"APPROVED","isLocked":false,"visibility":"PUBLIC","collection":{"__ref":"Collection:5e5bef33608a"},"creator":{"__ref":"User:c74e54b2ea24"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*Pw1Eeiu1bDCX0u-aBJ7U_Q.jpeg":{"id":"1*Pw1Eeiu1bDCX0u-aBJ7U_Q.jpeg","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"Post:232bf6acd15f":{"id":"232bf6acd15f","__typename":"Post","title":"TensorFlow Sequence to Sequence Model Examples","mediumUrl":"https:\u002F\u002Fjonathan-hui.medium.com\u002Ftensorflow-sequence-to-sequence-model-examples-232bf6acd15f","previewImage":{"__ref":"ImageMetadata:1*Pw1Eeiu1bDCX0u-aBJ7U_Q.jpeg"},"isPublished":true,"firstPublishedAt":1612202959922,"readingTime":9.435849056603773,"statusForCollection":null,"isLocked":false,"visibility":"PUBLIC","collection":null,"creator":{"__ref":"User:bd51f1a63813"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*6k7tbgyc1Qb2yCtqCzKCAg.jpeg":{"id":"1*6k7tbgyc1Qb2yCtqCzKCAg.jpeg","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"Post:53f190a37704":{"id":"53f190a37704","__typename":"Post","title":"TensorFlow & Keras","mediumUrl":"https:\u002F\u002Fjonathan-hui.medium.com\u002Ftensorflow-keras-53f190a37704","previewImage":{"__ref":"ImageMetadata:1*6k7tbgyc1Qb2yCtqCzKCAg.jpeg"},"isPublished":true,"firstPublishedAt":1610565819425,"readingTime":13.358490566037737,"statusForCollection":null,"isLocked":false,"visibility":"PUBLIC","collection":null,"creator":{"__ref":"User:bd51f1a63813"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*z-rSfscdqNtsKOTpnGXCWA.png":{"id":"1*z-rSfscdqNtsKOTpnGXCWA.png","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"CollectionViewerEdge:collectionId:de4e70eeb7f-viewerId:lo_3c97a71a5cb9":{"id":"collectionId:de4e70eeb7f-viewerId:lo_3c97a71a5cb9","__typename":"CollectionViewerEdge","isEditor":false},"Collection:de4e70eeb7f":{"id":"de4e70eeb7f","__typename":"Collection","name":"ClipMe","description":"CSCI566 Project","tagline":"ClipMe: Meme Clip Generation","domain":null,"slug":"clipme","isAuroraEligible":false,"isAuroraVisible":false,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:de4e70eeb7f-viewerId:lo_3c97a71a5cb9"},"canToggleEmail":false},"UserViewerEdge:userId:1a9a4c139464-viewerId:lo_3c97a71a5cb9":{"id":"userId:1a9a4c139464-viewerId:lo_3c97a71a5cb9","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"User:1a9a4c139464":{"id":"1a9a4c139464","__typename":"User","name":"Shivam Sardana","username":"shivam-sardana","bio":"","imageId":"0*NqNmVmLPsLCw5ROH.jpg","mediumMemberAt":0,"isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:1a9a4c139464-viewerId:lo_3c97a71a5cb9"},"viewerIsUser":false,"newsletterV3":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"shivam-sardana.medium.com"}},"hasSubdomain":true,"postSubscribeMembershipUpsellShownAt":0},"Post:a9d94cbdc83d":{"id":"a9d94cbdc83d","__typename":"Post","title":"ClipMe: Meme Clip Generation","mediumUrl":"https:\u002F\u002Fmedium.com\u002Fclipme\u002Fclipme-meme-clip-generation-a9d94cbdc83d","previewImage":{"__ref":"ImageMetadata:1*z-rSfscdqNtsKOTpnGXCWA.png"},"isPublished":true,"firstPublishedAt":1606709520041,"readingTime":7.126415094339623,"statusForCollection":"APPROVED","isLocked":false,"visibility":"PUBLIC","collection":{"__ref":"Collection:de4e70eeb7f"},"creator":{"__ref":"User:1a9a4c139464"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:1*ZcKvYSlLGNfKSYDXr8dJyA.png":{"id":"1*ZcKvYSlLGNfKSYDXr8dJyA.png","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"UserViewerEdge:userId:9922d25d6c50-viewerId:lo_3c97a71a5cb9":{"id":"userId:9922d25d6c50-viewerId:lo_3c97a71a5cb9","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"User:9922d25d6c50":{"id":"9922d25d6c50","__typename":"User","name":"raian lefgoum","username":"noufelefgoum","bio":"Computer Science Engineer and Mathematics passionate, interested in optimization and machine learning.","imageId":"1*JXexDL8nFl93ILTaiY1HnQ.jpeg","mediumMemberAt":0,"isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:9922d25d6c50-viewerId:lo_3c97a71a5cb9"},"viewerIsUser":false,"newsletterV3":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"noufelefgoum.medium.com"}},"hasSubdomain":true,"postSubscribeMembershipUpsellShownAt":0},"Post:f81262b49895":{"id":"f81262b49895","__typename":"Post","title":"why neural network work with nonlinear problem","mediumUrl":"https:\u002F\u002Fnoufelefgoum.medium.com\u002Fwhy-neural-network-work-with-nonlinear-problem-f81262b49895","previewImage":{"__ref":"ImageMetadata:1*ZcKvYSlLGNfKSYDXr8dJyA.png"},"isPublished":true,"firstPublishedAt":1560540862113,"readingTime":8.693396226415095,"statusForCollection":null,"isLocked":false,"visibility":"PUBLIC","collection":null,"creator":{"__ref":"User:9922d25d6c50"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"ImageMetadata:0*bB6Grog7PMawSL5_":{"id":"0*bB6Grog7PMawSL5_","__typename":"ImageMetadata","focusPercentX":null,"focusPercentY":null},"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:lo_3c97a71a5cb9":{"id":"collectionId:7f60cf5620c9-viewerId:lo_3c97a71a5cb9","__typename":"CollectionViewerEdge","isEditor":false},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","__typename":"Collection","name":"Towards Data Science","description":"Your home for data science. A Medium publication sharing concepts, ideas and codes.","tagline":"A Medium publication sharing concepts, ideas and codes.","domain":"towardsdatascience.com","slug":"towards-data-science","isAuroraEligible":true,"isAuroraVisible":true,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:lo_3c97a71a5cb9"},"canToggleEmail":true},"UserViewerEdge:userId:ba547bff904f-viewerId:lo_3c97a71a5cb9":{"id":"userId:ba547bff904f-viewerId:lo_3c97a71a5cb9","__typename":"UserViewerEdge","isFollowing":false,"isUser":false},"NewsletterV3:de3db5912a7c":{"id":"de3db5912a7c","__typename":"NewsletterV3","type":"NEWSLETTER_TYPE_AUTHOR","slug":"ba547bff904f","name":"ba547bff904f","collection":null,"user":{"__ref":"User:ba547bff904f"}},"User:ba547bff904f":{"id":"ba547bff904f","__typename":"User","name":"Edward Ma","username":"makcedward","newsletterV3":{"__ref":"NewsletterV3:de3db5912a7c"},"bio":"Focus in Natural Language Processing, Data Science Platform Architecture. https:\u002F\u002Fmakcedward.github.io\u002F","imageId":"1*Mi-KJV6LzO0xKjZQyQtMgQ.jpeg","mediumMemberAt":0,"isPartnerProgramEnrolled":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:ba547bff904f-viewerId:lo_3c97a71a5cb9"},"viewerIsUser":false,"customDomainState":null,"hasSubdomain":false,"postSubscribeMembershipUpsellShownAt":0},"Post:9661736b13ff":{"id":"9661736b13ff","__typename":"Post","title":"Data Augmentation library for text","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-augmentation-library-for-text-9661736b13ff","previewImage":{"__ref":"ImageMetadata:0*bB6Grog7PMawSL5_"},"isPublished":true,"firstPublishedAt":1555797033945,"readingTime":6.1716981132075475,"statusForCollection":"APPROVED","isLocked":false,"visibility":"PUBLIC","collection":{"__ref":"Collection:7f60cf5620c9"},"creator":{"__ref":"User:ba547bff904f"},"previewContent":{"__typename":"PreviewContent","isFullContent":false}},"PostViewerEdge:postId:28b1b93e2088-viewerId:lo_3c97a71a5cb9":{"id":"postId:28b1b93e2088-viewerId:lo_3c97a71a5cb9","__typename":"PostViewerEdge","catalogsConnection":null},"Post:28b1b93e2088":{"id":"28b1b93e2088","__typename":"Post","creator":{"__ref":"User:bd51f1a63813"},"canonicalUrl":"","collection":null,"content({\"postMeteringOptions\":{\"referrer\":\"\",\"sk\":null,\"source\":null}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"validatedShareKey":"","bodyModel":{"__typename":"RichText","paragraphs":[{"__ref":"Paragraph:4d50ee2de52f_0"},{"__ref":"Paragraph:4d50ee2de52f_1"},{"__ref":"Paragraph:4d50ee2de52f_2"},{"__ref":"Paragraph:4d50ee2de52f_3"},{"__ref":"Paragraph:4d50ee2de52f_4"},{"__ref":"Paragraph:4d50ee2de52f_5"},{"__ref":"Paragraph:4d50ee2de52f_6"},{"__ref":"Paragraph:4d50ee2de52f_7"},{"__ref":"Paragraph:4d50ee2de52f_8"},{"__ref":"Paragraph:4d50ee2de52f_9"},{"__ref":"Paragraph:4d50ee2de52f_10"},{"__ref":"Paragraph:4d50ee2de52f_11"},{"__ref":"Paragraph:4d50ee2de52f_12"},{"__ref":"Paragraph:4d50ee2de52f_13"},{"__ref":"Paragraph:4d50ee2de52f_14"},{"__ref":"Paragraph:4d50ee2de52f_15"},{"__ref":"Paragraph:4d50ee2de52f_16"},{"__ref":"Paragraph:4d50ee2de52f_17"},{"__ref":"Paragraph:4d50ee2de52f_18"},{"__ref":"Paragraph:4d50ee2de52f_19"},{"__ref":"Paragraph:4d50ee2de52f_20"},{"__ref":"Paragraph:4d50ee2de52f_21"},{"__ref":"Paragraph:4d50ee2de52f_22"},{"__ref":"Paragraph:4d50ee2de52f_23"},{"__ref":"Paragraph:4d50ee2de52f_24"},{"__ref":"Paragraph:4d50ee2de52f_25"},{"__ref":"Paragraph:4d50ee2de52f_26"},{"__ref":"Paragraph:4d50ee2de52f_27"},{"__ref":"Paragraph:4d50ee2de52f_28"},{"__ref":"Paragraph:4d50ee2de52f_29"},{"__ref":"Paragraph:4d50ee2de52f_30"},{"__ref":"Paragraph:4d50ee2de52f_31"},{"__ref":"Paragraph:4d50ee2de52f_32"},{"__ref":"Paragraph:4d50ee2de52f_33"},{"__ref":"Paragraph:4d50ee2de52f_34"},{"__ref":"Paragraph:4d50ee2de52f_35"},{"__ref":"Paragraph:4d50ee2de52f_36"},{"__ref":"Paragraph:4d50ee2de52f_37"},{"__ref":"Paragraph:4d50ee2de52f_38"},{"__ref":"Paragraph:4d50ee2de52f_39"},{"__ref":"Paragraph:4d50ee2de52f_40"},{"__ref":"Paragraph:4d50ee2de52f_41"},{"__ref":"Paragraph:4d50ee2de52f_42"},{"__ref":"Paragraph:4d50ee2de52f_43"},{"__ref":"Paragraph:4d50ee2de52f_44"},{"__ref":"Paragraph:4d50ee2de52f_45"},{"__ref":"Paragraph:4d50ee2de52f_46"},{"__ref":"Paragraph:4d50ee2de52f_47"},{"__ref":"Paragraph:4d50ee2de52f_48"},{"__ref":"Paragraph:4d50ee2de52f_49"},{"__ref":"Paragraph:4d50ee2de52f_50"},{"__ref":"Paragraph:4d50ee2de52f_51"},{"__ref":"Paragraph:4d50ee2de52f_52"},{"__ref":"Paragraph:4d50ee2de52f_53"},{"__ref":"Paragraph:4d50ee2de52f_54"},{"__ref":"Paragraph:4d50ee2de52f_55"},{"__ref":"Paragraph:4d50ee2de52f_56"},{"__ref":"Paragraph:4d50ee2de52f_57"},{"__ref":"Paragraph:4d50ee2de52f_58"},{"__ref":"Paragraph:4d50ee2de52f_59"},{"__ref":"Paragraph:4d50ee2de52f_60"},{"__ref":"Paragraph:4d50ee2de52f_61"},{"__ref":"Paragraph:4d50ee2de52f_62"},{"__ref":"Paragraph:4d50ee2de52f_63"},{"__ref":"Paragraph:4d50ee2de52f_64"},{"__ref":"Paragraph:4d50ee2de52f_65"},{"__ref":"Paragraph:4d50ee2de52f_66"},{"__ref":"Paragraph:4d50ee2de52f_67"},{"__ref":"Paragraph:4d50ee2de52f_68"},{"__ref":"Paragraph:4d50ee2de52f_69"},{"__ref":"Paragraph:4d50ee2de52f_70"},{"__ref":"Paragraph:4d50ee2de52f_71"},{"__ref":"Paragraph:4d50ee2de52f_72"},{"__ref":"Paragraph:4d50ee2de52f_73"},{"__ref":"Paragraph:4d50ee2de52f_74"},{"__ref":"Paragraph:4d50ee2de52f_75"},{"__ref":"Paragraph:4d50ee2de52f_76"},{"__ref":"Paragraph:4d50ee2de52f_77"},{"__ref":"Paragraph:4d50ee2de52f_78"},{"__ref":"Paragraph:4d50ee2de52f_79"},{"__ref":"Paragraph:4d50ee2de52f_80"},{"__ref":"Paragraph:4d50ee2de52f_81"},{"__ref":"Paragraph:4d50ee2de52f_82"},{"__ref":"Paragraph:4d50ee2de52f_83"},{"__ref":"Paragraph:4d50ee2de52f_84"},{"__ref":"Paragraph:4d50ee2de52f_85"},{"__ref":"Paragraph:4d50ee2de52f_86"},{"__ref":"Paragraph:4d50ee2de52f_87"},{"__ref":"Paragraph:4d50ee2de52f_88"},{"__ref":"Paragraph:4d50ee2de52f_89"},{"__ref":"Paragraph:4d50ee2de52f_90"},{"__ref":"Paragraph:4d50ee2de52f_91"},{"__ref":"Paragraph:4d50ee2de52f_92"},{"__ref":"Paragraph:4d50ee2de52f_93"},{"__ref":"Paragraph:4d50ee2de52f_94"},{"__ref":"Paragraph:4d50ee2de52f_95"},{"__ref":"Paragraph:4d50ee2de52f_96"},{"__ref":"Paragraph:4d50ee2de52f_97"},{"__ref":"Paragraph:4d50ee2de52f_98"},{"__ref":"Paragraph:4d50ee2de52f_99"},{"__ref":"Paragraph:4d50ee2de52f_100"},{"__ref":"Paragraph:4d50ee2de52f_101"},{"__ref":"Paragraph:4d50ee2de52f_102"},{"__ref":"Paragraph:4d50ee2de52f_103"},{"__ref":"Paragraph:4d50ee2de52f_104"},{"__ref":"Paragraph:4d50ee2de52f_105"},{"__ref":"Paragraph:4d50ee2de52f_106"},{"__ref":"Paragraph:4d50ee2de52f_107"},{"__ref":"Paragraph:4d50ee2de52f_108"},{"__ref":"Paragraph:4d50ee2de52f_109"},{"__ref":"Paragraph:4d50ee2de52f_110"},{"__ref":"Paragraph:4d50ee2de52f_111"},{"__ref":"Paragraph:4d50ee2de52f_112"},{"__ref":"Paragraph:4d50ee2de52f_113"},{"__ref":"Paragraph:4d50ee2de52f_114"},{"__ref":"Paragraph:4d50ee2de52f_115"},{"__ref":"Paragraph:4d50ee2de52f_116"},{"__ref":"Paragraph:4d50ee2de52f_117"},{"__ref":"Paragraph:4d50ee2de52f_118"},{"__ref":"Paragraph:4d50ee2de52f_119"},{"__ref":"Paragraph:4d50ee2de52f_120"},{"__ref":"Paragraph:4d50ee2de52f_121"},{"__ref":"Paragraph:4d50ee2de52f_122"},{"__ref":"Paragraph:4d50ee2de52f_123"},{"__ref":"Paragraph:4d50ee2de52f_124"},{"__ref":"Paragraph:4d50ee2de52f_125"},{"__ref":"Paragraph:4d50ee2de52f_126"},{"__ref":"Paragraph:4d50ee2de52f_127"},{"__ref":"Paragraph:4d50ee2de52f_128"},{"__ref":"Paragraph:4d50ee2de52f_129"},{"__ref":"Paragraph:4d50ee2de52f_130"},{"__ref":"Paragraph:4d50ee2de52f_131"},{"__ref":"Paragraph:4d50ee2de52f_132"},{"__ref":"Paragraph:4d50ee2de52f_133"},{"__ref":"Paragraph:4d50ee2de52f_134"},{"__ref":"Paragraph:4d50ee2de52f_135"},{"__ref":"Paragraph:4d50ee2de52f_136"},{"__ref":"Paragraph:4d50ee2de52f_137"},{"__ref":"Paragraph:4d50ee2de52f_138"},{"__ref":"Paragraph:4d50ee2de52f_139"},{"__ref":"Paragraph:4d50ee2de52f_140"},{"__ref":"Paragraph:4d50ee2de52f_141"},{"__ref":"Paragraph:4d50ee2de52f_142"},{"__ref":"Paragraph:4d50ee2de52f_143"},{"__ref":"Paragraph:4d50ee2de52f_144"},{"__ref":"Paragraph:4d50ee2de52f_145"},{"__ref":"Paragraph:4d50ee2de52f_146"},{"__ref":"Paragraph:4d50ee2de52f_147"},{"__ref":"Paragraph:4d50ee2de52f_148"},{"__ref":"Paragraph:4d50ee2de52f_149"},{"__ref":"Paragraph:4d50ee2de52f_150"},{"__ref":"Paragraph:4d50ee2de52f_151"},{"__ref":"Paragraph:4d50ee2de52f_152"},{"__ref":"Paragraph:4d50ee2de52f_153"},{"__ref":"Paragraph:4d50ee2de52f_154"},{"__ref":"Paragraph:4d50ee2de52f_155"},{"__ref":"Paragraph:4d50ee2de52f_156"},{"__ref":"Paragraph:4d50ee2de52f_157"},{"__ref":"Paragraph:4d50ee2de52f_158"},{"__ref":"Paragraph:4d50ee2de52f_159"},{"__ref":"Paragraph:4d50ee2de52f_160"},{"__ref":"Paragraph:4d50ee2de52f_161"},{"__ref":"Paragraph:4d50ee2de52f_162"},{"__ref":"Paragraph:4d50ee2de52f_163"},{"__ref":"Paragraph:4d50ee2de52f_164"},{"__ref":"Paragraph:4d50ee2de52f_165"},{"__ref":"Paragraph:4d50ee2de52f_166"},{"__ref":"Paragraph:4d50ee2de52f_167"},{"__ref":"Paragraph:4d50ee2de52f_168"},{"__ref":"Paragraph:4d50ee2de52f_169"},{"__ref":"Paragraph:4d50ee2de52f_170"},{"__ref":"Paragraph:4d50ee2de52f_171"},{"__ref":"Paragraph:4d50ee2de52f_172"},{"__ref":"Paragraph:4d50ee2de52f_173"},{"__ref":"Paragraph:4d50ee2de52f_174"},{"__ref":"Paragraph:4d50ee2de52f_175"},{"__ref":"Paragraph:4d50ee2de52f_176"},{"__ref":"Paragraph:4d50ee2de52f_177"},{"__ref":"Paragraph:4d50ee2de52f_178"},{"__ref":"Paragraph:4d50ee2de52f_179"},{"__ref":"Paragraph:4d50ee2de52f_180"},{"__ref":"Paragraph:4d50ee2de52f_181"},{"__ref":"Paragraph:4d50ee2de52f_182"},{"__ref":"Paragraph:4d50ee2de52f_183"},{"__ref":"Paragraph:4d50ee2de52f_184"},{"__ref":"Paragraph:4d50ee2de52f_185"}],"sections":[{"__typename":"Section","name":"b001","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}]}},"customStyleSheet":null,"firstPublishedAt":1521338881687,"isLocked":false,"isPublished":true,"isShortform":false,"layerCake":0,"primaryTopic":null,"title":"Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3","isMarkedPaywallOnly":false,"mediumUrl":"https:\u002F\u002Fjonathan-hui.medium.com\u002Freal-time-object-detection-with-yolo-yolov2-28b1b93e2088","isLimitedState":false,"inResponseToPostResult":null,"inResponseToCatalogResult":null,"visibility":"PUBLIC","license":"ALL_RIGHTS_RESERVED","allowResponses":true,"newsletterId":"","sequence":null,"tags":[{"__ref":"Tag:machine-learning"},{"__ref":"Tag:deep-learning"},{"__ref":"Tag:computer-vision"},{"__ref":"Tag:artificial-intelligence"}],"topics":[{"__typename":"Topic","topicId":"1eca0103fff3","name":"Machine Learning"}],"isNewsletter":false,"isPublishToEmail":false,"socialTitle":"","socialDek":"","noIndex":null,"curationStatus":null,"metaDescription":"","latestPublishedAt":1566914950959,"readingTime":17.72830188679245,"previewContent":{"__typename":"PreviewContent","subtitle":"You only look once (YOLO) is an object detection system targeted for real-time processing. We will introduce YOLO, YOLOv2 and YOLO9000 in…"},"previewImage":{"__ref":"ImageMetadata:1*j4PnWfxP3yoVPOFyI27tww.jpeg"},"clapCount":8741,"postResponses":{"__typename":"PostResponses","count":77},"isSuspended":false,"pendingCollection":null,"statusForCollection":null,"lockedSource":"LOCKED_POST_SOURCE_NONE","pinnedAt":0,"pinnedByCreatorAt":0,"curationEligibleAt":0,"responseDistribution":"NOT_DISTRIBUTED","internalLinks({\"paging\":{\"limit\":8}})":{"__typename":"InternalLinksConnection","items":[{"__ref":"Post:d3778244ed98"},{"__ref":"Post:a858d69d83b8"},{"__ref":"Post:728744d3deac"},{"__ref":"Post:232bf6acd15f"},{"__ref":"Post:53f190a37704"},{"__ref":"Post:a9d94cbdc83d"},{"__ref":"Post:f81262b49895"},{"__ref":"Post:9661736b13ff"}]},"viewerEdge":{"__ref":"PostViewerEdge:postId:28b1b93e2088-viewerId:lo_3c97a71a5cb9"},"collaborators":[],"translationSourcePost":null,"inResponseToMediaResource":null,"audioVersionUrl":"","seoTitle":"","updatedAt":1566914951256,"shortformType":"SHORTFORM_TYPE_LINK","structuredData":"","seoDescription":"","isIndexable":true,"latestPublishedVersion":"4d50ee2de52f","voterCount":1340,"recommenders":[],"content({})":{"__typename":"PostContent","isLockedPreviewOnly":false,"validatedShareKey":"","bodyModel":{"__typename":"RichText","paragraphs":[{"__ref":"Paragraph:4d50ee2de52f_0"},{"__ref":"Paragraph:4d50ee2de52f_1"},{"__ref":"Paragraph:4d50ee2de52f_2"},{"__ref":"Paragraph:4d50ee2de52f_3"},{"__ref":"Paragraph:4d50ee2de52f_4"},{"__ref":"Paragraph:4d50ee2de52f_5"},{"__ref":"Paragraph:4d50ee2de52f_6"},{"__ref":"Paragraph:4d50ee2de52f_7"},{"__ref":"Paragraph:4d50ee2de52f_8"},{"__ref":"Paragraph:4d50ee2de52f_9"},{"__ref":"Paragraph:4d50ee2de52f_10"},{"__ref":"Paragraph:4d50ee2de52f_11"},{"__ref":"Paragraph:4d50ee2de52f_12"},{"__ref":"Paragraph:4d50ee2de52f_13"},{"__ref":"Paragraph:4d50ee2de52f_14"},{"__ref":"Paragraph:4d50ee2de52f_15"},{"__ref":"Paragraph:4d50ee2de52f_16"},{"__ref":"Paragraph:4d50ee2de52f_17"},{"__ref":"Paragraph:4d50ee2de52f_18"},{"__ref":"Paragraph:4d50ee2de52f_19"},{"__ref":"Paragraph:4d50ee2de52f_20"},{"__ref":"Paragraph:4d50ee2de52f_21"},{"__ref":"Paragraph:4d50ee2de52f_22"},{"__ref":"Paragraph:4d50ee2de52f_23"},{"__ref":"Paragraph:4d50ee2de52f_24"},{"__ref":"Paragraph:4d50ee2de52f_25"},{"__ref":"Paragraph:4d50ee2de52f_26"},{"__ref":"Paragraph:4d50ee2de52f_27"},{"__ref":"Paragraph:4d50ee2de52f_28"},{"__ref":"Paragraph:4d50ee2de52f_29"},{"__ref":"Paragraph:4d50ee2de52f_30"},{"__ref":"Paragraph:4d50ee2de52f_31"},{"__ref":"Paragraph:4d50ee2de52f_32"},{"__ref":"Paragraph:4d50ee2de52f_33"},{"__ref":"Paragraph:4d50ee2de52f_34"},{"__ref":"Paragraph:4d50ee2de52f_35"},{"__ref":"Paragraph:4d50ee2de52f_36"},{"__ref":"Paragraph:4d50ee2de52f_37"},{"__ref":"Paragraph:4d50ee2de52f_38"},{"__ref":"Paragraph:4d50ee2de52f_39"},{"__ref":"Paragraph:4d50ee2de52f_40"},{"__ref":"Paragraph:4d50ee2de52f_41"},{"__ref":"Paragraph:4d50ee2de52f_42"},{"__ref":"Paragraph:4d50ee2de52f_43"},{"__ref":"Paragraph:4d50ee2de52f_44"},{"__ref":"Paragraph:4d50ee2de52f_45"},{"__ref":"Paragraph:4d50ee2de52f_46"},{"__ref":"Paragraph:4d50ee2de52f_47"},{"__ref":"Paragraph:4d50ee2de52f_48"},{"__ref":"Paragraph:4d50ee2de52f_49"},{"__ref":"Paragraph:4d50ee2de52f_50"},{"__ref":"Paragraph:4d50ee2de52f_51"},{"__ref":"Paragraph:4d50ee2de52f_52"},{"__ref":"Paragraph:4d50ee2de52f_53"},{"__ref":"Paragraph:4d50ee2de52f_54"},{"__ref":"Paragraph:4d50ee2de52f_55"},{"__ref":"Paragraph:4d50ee2de52f_56"},{"__ref":"Paragraph:4d50ee2de52f_57"},{"__ref":"Paragraph:4d50ee2de52f_58"},{"__ref":"Paragraph:4d50ee2de52f_59"},{"__ref":"Paragraph:4d50ee2de52f_60"},{"__ref":"Paragraph:4d50ee2de52f_61"},{"__ref":"Paragraph:4d50ee2de52f_62"},{"__ref":"Paragraph:4d50ee2de52f_63"},{"__ref":"Paragraph:4d50ee2de52f_64"},{"__ref":"Paragraph:4d50ee2de52f_65"},{"__ref":"Paragraph:4d50ee2de52f_66"},{"__ref":"Paragraph:4d50ee2de52f_67"},{"__ref":"Paragraph:4d50ee2de52f_68"},{"__ref":"Paragraph:4d50ee2de52f_69"},{"__ref":"Paragraph:4d50ee2de52f_70"},{"__ref":"Paragraph:4d50ee2de52f_71"},{"__ref":"Paragraph:4d50ee2de52f_72"},{"__ref":"Paragraph:4d50ee2de52f_73"},{"__ref":"Paragraph:4d50ee2de52f_74"},{"__ref":"Paragraph:4d50ee2de52f_75"},{"__ref":"Paragraph:4d50ee2de52f_76"},{"__ref":"Paragraph:4d50ee2de52f_77"},{"__ref":"Paragraph:4d50ee2de52f_78"},{"__ref":"Paragraph:4d50ee2de52f_79"},{"__ref":"Paragraph:4d50ee2de52f_80"},{"__ref":"Paragraph:4d50ee2de52f_81"},{"__ref":"Paragraph:4d50ee2de52f_82"},{"__ref":"Paragraph:4d50ee2de52f_83"},{"__ref":"Paragraph:4d50ee2de52f_84"},{"__ref":"Paragraph:4d50ee2de52f_85"},{"__ref":"Paragraph:4d50ee2de52f_86"},{"__ref":"Paragraph:4d50ee2de52f_87"},{"__ref":"Paragraph:4d50ee2de52f_88"},{"__ref":"Paragraph:4d50ee2de52f_89"},{"__ref":"Paragraph:4d50ee2de52f_90"},{"__ref":"Paragraph:4d50ee2de52f_91"},{"__ref":"Paragraph:4d50ee2de52f_92"},{"__ref":"Paragraph:4d50ee2de52f_93"},{"__ref":"Paragraph:4d50ee2de52f_94"},{"__ref":"Paragraph:4d50ee2de52f_95"},{"__ref":"Paragraph:4d50ee2de52f_96"},{"__ref":"Paragraph:4d50ee2de52f_97"},{"__ref":"Paragraph:4d50ee2de52f_98"},{"__ref":"Paragraph:4d50ee2de52f_99"},{"__ref":"Paragraph:4d50ee2de52f_100"},{"__ref":"Paragraph:4d50ee2de52f_101"},{"__ref":"Paragraph:4d50ee2de52f_102"},{"__ref":"Paragraph:4d50ee2de52f_103"},{"__ref":"Paragraph:4d50ee2de52f_104"},{"__ref":"Paragraph:4d50ee2de52f_105"},{"__ref":"Paragraph:4d50ee2de52f_106"},{"__ref":"Paragraph:4d50ee2de52f_107"},{"__ref":"Paragraph:4d50ee2de52f_108"},{"__ref":"Paragraph:4d50ee2de52f_109"},{"__ref":"Paragraph:4d50ee2de52f_110"},{"__ref":"Paragraph:4d50ee2de52f_111"},{"__ref":"Paragraph:4d50ee2de52f_112"},{"__ref":"Paragraph:4d50ee2de52f_113"},{"__ref":"Paragraph:4d50ee2de52f_114"},{"__ref":"Paragraph:4d50ee2de52f_115"},{"__ref":"Paragraph:4d50ee2de52f_116"},{"__ref":"Paragraph:4d50ee2de52f_117"},{"__ref":"Paragraph:4d50ee2de52f_118"},{"__ref":"Paragraph:4d50ee2de52f_119"},{"__ref":"Paragraph:4d50ee2de52f_120"},{"__ref":"Paragraph:4d50ee2de52f_121"},{"__ref":"Paragraph:4d50ee2de52f_122"},{"__ref":"Paragraph:4d50ee2de52f_123"},{"__ref":"Paragraph:4d50ee2de52f_124"},{"__ref":"Paragraph:4d50ee2de52f_125"},{"__ref":"Paragraph:4d50ee2de52f_126"},{"__ref":"Paragraph:4d50ee2de52f_127"},{"__ref":"Paragraph:4d50ee2de52f_128"},{"__ref":"Paragraph:4d50ee2de52f_129"},{"__ref":"Paragraph:4d50ee2de52f_130"},{"__ref":"Paragraph:4d50ee2de52f_131"},{"__ref":"Paragraph:4d50ee2de52f_132"},{"__ref":"Paragraph:4d50ee2de52f_133"},{"__ref":"Paragraph:4d50ee2de52f_134"},{"__ref":"Paragraph:4d50ee2de52f_135"},{"__ref":"Paragraph:4d50ee2de52f_136"},{"__ref":"Paragraph:4d50ee2de52f_137"},{"__ref":"Paragraph:4d50ee2de52f_138"},{"__ref":"Paragraph:4d50ee2de52f_139"},{"__ref":"Paragraph:4d50ee2de52f_140"},{"__ref":"Paragraph:4d50ee2de52f_141"},{"__ref":"Paragraph:4d50ee2de52f_142"},{"__ref":"Paragraph:4d50ee2de52f_143"},{"__ref":"Paragraph:4d50ee2de52f_144"},{"__ref":"Paragraph:4d50ee2de52f_145"},{"__ref":"Paragraph:4d50ee2de52f_146"},{"__ref":"Paragraph:4d50ee2de52f_147"},{"__ref":"Paragraph:4d50ee2de52f_148"},{"__ref":"Paragraph:4d50ee2de52f_149"},{"__ref":"Paragraph:4d50ee2de52f_150"},{"__ref":"Paragraph:4d50ee2de52f_151"},{"__ref":"Paragraph:4d50ee2de52f_152"},{"__ref":"Paragraph:4d50ee2de52f_153"},{"__ref":"Paragraph:4d50ee2de52f_154"},{"__ref":"Paragraph:4d50ee2de52f_155"},{"__ref":"Paragraph:4d50ee2de52f_156"},{"__ref":"Paragraph:4d50ee2de52f_157"},{"__ref":"Paragraph:4d50ee2de52f_158"},{"__ref":"Paragraph:4d50ee2de52f_159"},{"__ref":"Paragraph:4d50ee2de52f_160"},{"__ref":"Paragraph:4d50ee2de52f_161"},{"__ref":"Paragraph:4d50ee2de52f_162"},{"__ref":"Paragraph:4d50ee2de52f_163"},{"__ref":"Paragraph:4d50ee2de52f_164"},{"__ref":"Paragraph:4d50ee2de52f_165"},{"__ref":"Paragraph:4d50ee2de52f_166"},{"__ref":"Paragraph:4d50ee2de52f_167"},{"__ref":"Paragraph:4d50ee2de52f_168"},{"__ref":"Paragraph:4d50ee2de52f_169"},{"__ref":"Paragraph:4d50ee2de52f_170"},{"__ref":"Paragraph:4d50ee2de52f_171"},{"__ref":"Paragraph:4d50ee2de52f_172"},{"__ref":"Paragraph:4d50ee2de52f_173"},{"__ref":"Paragraph:4d50ee2de52f_174"},{"__ref":"Paragraph:4d50ee2de52f_175"},{"__ref":"Paragraph:4d50ee2de52f_176"},{"__ref":"Paragraph:4d50ee2de52f_177"},{"__ref":"Paragraph:4d50ee2de52f_178"},{"__ref":"Paragraph:4d50ee2de52f_179"},{"__ref":"Paragraph:4d50ee2de52f_180"},{"__ref":"Paragraph:4d50ee2de52f_181"},{"__ref":"Paragraph:4d50ee2de52f_182"},{"__ref":"Paragraph:4d50ee2de52f_183"},{"__ref":"Paragraph:4d50ee2de52f_184"},{"__ref":"Paragraph:4d50ee2de52f_185"}],"sections":[{"__typename":"Section","name":"b001","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}]}}}}</script><script>window.__MIDDLEWARE_STATE__={"session":{"xsrf":""},"cache":{"cacheStatus":"HIT"}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.bc51a45f.js"></script><script src="https://cdn-client.medium.com/lite/static/js/36804.29338eda.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.d7dd848a.js"></script><script src="https://cdn-client.medium.com/lite/static/js/45573.4354ed57.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/instrumentation.428b7498.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/reporting.0a3746f4.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/81144.fd02afd8.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/11034.d256484f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/90192.75e671b6.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/81645.c8a01874.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/79088.e4863540.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/6970.6307dcb3.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/21356.2a2c8a5d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/50006.f237604f.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/70832.0e08f3eb.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/80685.451a05c0.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/57612.d8a98116.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5055.da1a97c1.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/92397.6387f3c2.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/43730.80d07eb1.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/61781.5265363a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/79851.0c6f9f31.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/33673.de5f47de.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/15140.f892b8c7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/95972.996c4300.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/36851.c71210a1.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/26022.a57d511a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/11366.069ea1f1.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/60519.be2d6721.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/62182.91cdfb4e.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/35285.dc03faaf.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/92265.b76dd4b6.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/74418.96d6f871.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9972.269c800c.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/43642.001e4abe.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/66510.2dedb1f1.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/10733.e8966287.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/Post.7f15b0f2.chunk.js"></script><script>window.main();</script><script defer src="https://static.cloudflareinsights.com/beacon.min.js" data-cf-beacon='{"rayId":"69be7040a94a3537","token":"0b5f665943484354a59c39c6833f7078","version":"2021.9.0","si":100}'></script>
</body></html>